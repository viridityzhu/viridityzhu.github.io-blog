<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reading Neural Capture of Animatable 3D Human from Monocular Video</title>
      <link href="/blog/Reading-Neural-Capture-of-Animatable-3D-Human-from-Monocular-Video/"/>
      <url>/blog/Reading-Neural-Capture-of-Animatable-3D-Human-from-Monocular-Video/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼š</p><p>ä½œè€…ï¼šGusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, and Yan Lu</p><p>å‘è¡¨ï¼š ECCV 2022</p><p>é“¾æ¥ï¼š <a href="https://arxiv.org/abs/2208.08728" class="uri">https://arxiv.org/abs/2208.08728</a></p><hr /><h2 id="why">Whyï¼š</h2><ol type="1"><li>ä¹‹å‰çš„3Däººä½“é‡å»ºå·¥ä½œä¸€èˆ¬éœ€è¦å¤šè§†è§’è§†é¢‘ï¼Œæˆ–è€…é¢å¤–çš„3Då‡ ä½•ä¿¡æ¯ã€‚è¿™ç¯‡å·¥ä½œä»¥å•è§†è§’è§†é¢‘ä¸ºè¾“å…¥</li><li>ä¹‹å‰çš„å·¥ä½œå»ºæ¨¡å‡ºæ¥çš„3Däººä½“å¾ˆéš¾æ³›åŒ–åˆ°æ–°çš„pose</li><li>ä¹‹å‰çš„å·¥ä½œéƒ½åªèƒ½è§£å†³ä¸€éƒ¨åˆ†çš„é—®é¢˜ï¼šåŸºäºå‚æ•°åŒ–äººä½“æ¨¡å‹çš„æ–¹æ³•å¯¹appearanceçš„è¡¨ç¤ºç²¾åº¦æœ‰é™ï¼›åŸºäºNeRFçš„æ–¹æ³•çš„appearanceæ•ˆæœå¥½ï¼Œä½†æ˜¯è¦ä¹ˆåªå…³æ³¨äºNeRFåœºæœ¬èº«çš„æ„å»ºï¼Œè¦ä¹ˆéœ€è¦ç²¾ç¡®çš„3D meshä½œä¸ºå…ˆéªŒã€‚</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>æå‡ºä¸€ä¸ªä»å•è§†è§’è§†é¢‘é‡å»ºanimatableçš„3Däººä½“çš„æ–¹æ³•</li><li>è¡¨ç¤ºæ–¹æ³•æ˜¯æŠŠ dynamic NeRF å’Œä¸€ä¸ª human mesh ï¼ˆSMPLï¼‰ç›¸ç»“åˆã€‚è¿™ä¸ª dynamic NeRF çš„è¾“å…¥æ˜¯ä¸€äº›åµŒå…¥åˆ°mesh é¡¶ç‚¹çš„å±€éƒ¨ä¿¡æ¯ï¼Œè¿™æ ·ï¼Œå½“éœ€è¦è¡¨ç°ä¸€ä¸ªäººä¸åŒçš„å§¿åŠ¿çš„æ—¶å€™ï¼Œæœ¬è´¨ä¸Šæ˜¯å¯¹è¿™ä¸ª canonical space çš„é™æ€NeRFè¿›è¡Œdeformationã€‚<strong>è¿™é‡Œçš„å…³é”®é—®é¢˜æ˜¯å¦‚ä½•è®¾è®¡è¿™ä¸ªå±€éƒ¨ä¿¡æ¯ï¼Œæ¥è®©æŸ¥è¯¢observation spaceä¸­çš„ä»»æ„ä¸€ç‚¹çš„æ—¶å€™ï¼Œéƒ½èƒ½å¤Ÿè‰¯å¥½åœ°deformåˆ°canonical spaceï¼Œä»è€Œæ‰¾åˆ°é™æ€NeRFé‡Œæ­£ç¡®çš„ç‚¹</strong></li><li>åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆå€Ÿç”¨åˆ«çš„å·¥å…·åˆå§‹åŒ–ä¸€ä¸ªmesh poseï¼Œç„¶åé€å¸§åœ°åŒæ—¶ finetune mesh pose å’Œ NeRF</li></ol><h2 id="how">Howï¼š</h2><h3 id="query-embedding-for-nerf">Query embedding for NeRF</h3><figure><img src="https://s2.loli.net/2023/11/24/IFWcdEZDfquSVNJ.png" alt="image-20231124051120340" /><figcaption>image-20231124051120340</figcaption></figure><p>ç²¾é«“å°±åœ¨å›¾é‡Œäº†ï¼š</p><ol type="1"><li><p>äººä½“çš„è¡¨ç°å½¢å¼æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„ï¼šé¦–å…ˆæœ‰ä¸€ä¸ªç”±poseå‚æ•°<span class="math inline">\(\theta\)</span>é©±åŠ¨çš„SMPL meshï¼Œä»¥åŠä¸€ä¸ªmesh-guided NeRFï¼Œåè€…çš„è¾“å…¥æ˜¯å¯¹åº”query rayä¸Šçš„3D pointsçš„embedding</p></li><li><p>Query embeddingçš„å…·ä½“æ„æˆï¼š</p><ol type="1"><li>æœ€ç›´è§‚çš„ Latent Codeï¼šå­˜å‚¨åœ¨æ¯ä¸€ä¸ªmeshé¡¶ç‚¹ä¸Šï¼Œè¡¨ç¤ºçš„æ˜¯appearanceä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªquery pointï¼Œä¼šæ‰¾åˆ°meshé¡¶ç‚¹ä¸­K nearest neighborsæ‰€å¯¹åº”çš„latent codes</li><li>è¢«ç§°ä¸º Deformation Guidanceï¼šå…¶å®æ˜¯åœ¨<strong>canonical space</strong>ä¸­ï¼Œåˆšåˆšç”¨åˆ°çš„é‚£äº›KNNé¡¶ç‚¹çš„åæ ‡ï¼ˆç”¨inverse LBSå¾—åˆ°ï¼‰ï¼Œä»¥åŠquery pointç›¸å¯¹äºmeshè¡¨é¢æŠ•å½±ç‚¹çš„æ–¹å‘ã€‚è¿™ä¸ªä¿¡æ¯èƒ½å¤ŸæŒ‡å¯¼deformation fieldï¼Œæ‰€ä»¥å«guidance</li><li>å¦ä¸€æ–¹é¢è¿˜æœ‰ Deformation Priorsï¼šæ˜¯åœ¨<strong>observation space</strong>ä¸­ï¼Œquery pointç›¸å¯¹äºåˆšåˆšç”¨åˆ°çš„é‚£äº›KNNé¡¶ç‚¹çš„è·ç¦»ã€‚æ–‡ä¸­è¯´è¿™æ˜¯ç”¨æ¥é˜²æ­¢deformation fieldè½å…¥local minimaçš„ï¼Œæ‰€ä»¥å«åšpriors</li></ol></li><li><figure><img src="https://s2.loli.net/2023/11/24/2d4HiJucZTotBxP.png" alt="image-20231124052407094" /><figcaption>image-20231124052407094</figcaption></figure><p>æ–‡ä¸­ç‰¹æ„ç”¨ä¸Šå›¾å¼ºè°ƒäº†è¿™é‡Œéœ€è¦ç”¨åˆ°Kè¿‘é‚»é¡¶ç‚¹ï¼Œè€Œä¸æ˜¯å•ä¸ªæœ€è¿‘çš„é¡¶ç‚¹ã€‚å› ä¸ºå¦‚æœåªç”¨å•ä¸ªæœ€è¿‘çš„é¡¶ç‚¹ï¼ˆå›¾aï¼‰ï¼Œå°±ä¸èƒ½æä¾›ä¸åŒçš„deformation pattern çš„ä¿¡æ¯ï¼›è€Œï¼ˆbï¼‰é‡ŒåŠ ä¸Šäº†K-NN distanceä¹‹åï¼Œå°±èƒ½æœ‰è¿™ä¸ªdeformation patternä¿¡æ¯äº†ã€‚ï¼ˆæˆ‘æœ‰ç‚¹ç–‘æƒ‘ä»€ä¹ˆæ˜¯deformation patternï¼Œå°±æ˜¯è¿™ä¸ªè¡¨é¢çš„å‡¹å‡¸æ€§å—ï¼Ÿï¼‰</p></li></ol><h3 id="è®­ç»ƒè¿‡ç¨‹">è®­ç»ƒè¿‡ç¨‹</h3><ol type="1"><li>é¦–å…ˆå€Ÿç”¨åˆ«çš„å·¥å…·åˆå§‹åŒ–ä¸€ä¸ªmesh poseã€‚ä½†æ˜¯è¿™ä¸ªposeä¸å¤Ÿç²¾å‡†ï¼Œè¿˜éœ€è¦finetuneã€‚æ–‡ä¸­æåˆ°è¿™é‡Œç›´æ¥æ˜¯finetune per-frame pose parameterï¼Œè€Œä¸ç”¨per-vertex offsetï¼Œå› ä¸ºåè€…å¯èƒ½å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°local minimaï¼ˆï¼Ÿæœ‰ç‚¹ç–‘æƒ‘ï¼Œæˆ‘ä»¥ä¸ºç”¨pose parameteræˆ–è®¸æ˜¯æœ‰åˆ©äºåœ¨åç»­ç”¨temporal consistencyä¹‹ç±»çš„ï¼Œä½†æ˜¯å¥½åƒå¹¶æ²¡æœ‰ç”¨åˆ°ï¼›é‚£pose paramç›¸æ¯”ä¹‹ä¸‹å°±æ˜¯ä¸€ä¸ªæ›´ä¸ç²¾å‡†è€Œå·²ï¼Ÿï¼‰</li><li>è®­ç»ƒæ—¶é€å¸§åœ°åŒæ—¶ finetune mesh pose å’Œ NeRFã€‚losså¾ˆç›´è§‚ï¼š<ol type="1"><li>NeRFæ¸²æŸ“å›¾å’ŒåŸè§†é¢‘å¸§çš„L2 loss</li><li>æ­£åˆ™ <span class="math inline">\(\|\theta-\theta^0\|^2_2\)</span>ï¼Œæ˜¯ä¸ºäº†è®©æ¯å¸§çš„pose finetuneä¸è‡³äºå¤ªåç¦»åˆå§‹ä¼°è®¡</li></ol></li></ol><h3 id="å®éªŒ">å®éªŒ</h3><ol type="1"><li>è®­ç»ƒè¦åœ¨v100ä¸Š60å°æ—¶ï¼›æ•°æ®é›†ç”¨åˆ°People-Snapshotã€DoubleFusionã€ZJU-MoCapã€Human3.6Mï¼›æŒ‡æ ‡ç”¨PSNRå’ŒSSIM</li><li>åœ¨2022ï¼Œæ²¡æœ‰ä»€ä¹ˆèƒ½ç›´æ¥å¯¹æ¯”çš„å…¶ä»–å·¥ä½œï¼Œè·Ÿéœ€è¦å¤šè§†è§’è§†é¢‘è¾“å…¥çš„AniNeRFã€åº•å±‚æ¶æ„å¾ˆä¸åŒçš„A-NeRFã€mesh-based çš„æ–¹æ³•VideoAvataræ¯”äº†ä¸‰ä¸‹ï¼Œæ¯”ä»–ä»¬éƒ½å¥½</li></ol><figure><img src="https://s2.loli.net/2023/11/24/r4V68kqj9HBdaxD.png" alt="image-20231124054314933" /><figcaption>image-20231124054314933</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Computer Vision </tag>
            
            <tag> Human Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Handy: Towards a high fidelity 3D hand shape and appearance model</title>
      <link href="/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/"/>
      <url>/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼š<a href="https://rolpotamias.github.io/Handy/" class="uri">https://rolpotamias.github.io/Handy/</a></p><p>ä½œè€…ï¼šRolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou. From Imperial College London and Cosmos.</p><p>å‘è¡¨ï¼š CVPR23</p><p>é“¾æ¥ï¼š <a href="https://github.com/rolpotamias/handy" class="uri">https://github.com/rolpotamias/handy</a></p><hr /><figure><img src="https://github.com/rolpotamias/handy/raw/main/figures/teaser_fig.png" alt="handy" /><figcaption>handy</figcaption></figure><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><p>Qï¼šæˆ‘æ„Ÿè§‰è¿™ä¸ªä»»åŠ¡å¬èµ·æ¥è¿˜æŒºç›´è§‚çš„ï¼Œå°±æ˜¯ç”¨GANå»è®­ç»ƒå¤–è§‚ï¼Œå®šä¹‰ä¸€äº›æ›´å¤šverticesçš„mesh templateï¼Œç”¨è¶…çº§å¤§é‡çš„æ ·æœ¬å»è®­ç»ƒå †æ•ˆæœå˜›ï¼Ÿhand modelçš„å®šä¹‰ä¼šæœ‰ä»€ä¹ˆæ–°æ„å—ï¼Ÿæˆ‘å€’æ˜¯æƒ³ä¸å‡ºæ¥ã€‚</p><p>Aï¼šç¡®å®å¾ˆç›´è§‚ï¼Œhand modelçš„å®šä¹‰æ²¡ä»€ä¹ˆå¤ªå¤§åŒºåˆ«ã€‚è´¡çŒ®ç‚¹ä¸»è¦åœ¨äºï¼š1. å¾ˆå¤§å¾ˆå¥½å¾ˆvariantçš„æ–°æ•°æ®é›†ï¼Œé€ æˆäº†å¾ˆå¥½çš„Handy 2. ç”¨StyleGANæ¥å­¦textureï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„PCAï¼Œå¾—åˆ°çš„textureæ›´é«˜é¢‘ç»†èŠ‚ï¼Œæ›´å¥½ã€‚</p><h2 id="why">Whyï¼š</h2><ol type="1"><li>VR ARå‘å±•ï¼Œå¯¹äººæ‰‹çš„å»ºæ¨¡ã€è¿½è¸ªå’Œé‡å»ºçš„ç ”ç©¶å˜å¾—æµè¡Œï¼Œå› ä¸ºæ‰‹æ˜¯ä¸€ä¸ªé‡è¦çš„æ˜¾ç¤ºäººçš„è¡Œä¸ºçš„ä¸œè¥¿</li><li>å¤§éƒ¨åˆ†å·¥ä½œåŸºäºMANOï¼Œåªæœ‰å¾ˆç²—ç³™çš„low polygon countï¼Œè€Œä¸”åªåŸºäº31ä¸ªæ ·æœ¬æ„å»ºï¼Œdistributionä¸å¤Ÿå®½</li><li>å¤§éƒ¨åˆ†å·¥ä½œéƒ½å¿½ç•¥äº†æè´¨çš„æ„å»º</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>æå‡ºä¸€ä¸ªlarge-scaleçš„hand modelï¼ŒåŒ…å«äº†å½¢çŠ¶å’Œå¤–è§‚ï¼Œç”¨è¶…è¿‡1200ä¸ªäººç±»æ ·æœ¬è®­ç»ƒï¼Œæ ·æœ¬æœ‰large diversity</li><li>æ„å»ºSynthetic datasetï¼Œè®­ç»ƒä¸€ä¸ªhand pose estimationç½‘ç»œï¼Œä»å•å¼ å›¾åƒä¸­é‡å»ºæ‰‹</li><li>æå‡ºä¸€ä¸ªåŸºäºGANçš„æœ‰é«˜é¢‘ç»†èŠ‚çš„æ‰‹çš„å¤–è§‚+å½¢çŠ¶é‡å»ºæ–¹æ³•ï¼Œå³ä½¿æ˜¯in-the-wildçš„å•è§†è§’å›¾åƒä½œä¸ºè¾“å…¥</li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>çœ‹ä¸Šå»ä½œè€…æ˜¯ç”¨NeRFåšäº†ä¸€ä¸ªhigh fidelityçš„hand modelã€‚æˆ‘ä¸å¤ªæ¸…æ¥šæŠ€æœ¯ç»†èŠ‚å¦‚ä½•å®ç°ï¼Œå°¤å…¶æ˜¯nerfå¦‚ä½•è·Ÿparametric modelç»“åˆï¼Œå¦‚æœè®­ç»ƒä¸€ä¸ªnerf layerï¼Œè®©å®ƒå¯ä»¥æ ¹æ®å•å¼ è¾“å…¥å›¾åƒinferä¸€ä¸ªæ–°æ‰‹ã€‚ä¸çŸ¥é“æˆ‘å“ªé‡Œæ¥çš„è¯¯è§£ï¼Œæ€»ä¹‹ä¸æ˜¯ç”¨çš„nerfè¯¶â€¦â€¦</li><li>fig 1 çœ‹ä¸Šå»æ•ˆæœæœ‰ç‚¹å‡â€¦â€¦ä¼¼ä¹æ˜¯çš®è‚¤åå…‰ç‡çš„é—®é¢˜ï¼Œç”¨çš„ä»€ä¹ˆlighting representationå‘¢ï¼Ÿæ²¡ä»€ä¹ˆrepresentationï¼Œçº¯ç²¹ç”¨PCAå»æ‰äº†é˜´å½±æˆåˆ†</li><li>å±…ç„¶è¿çš±çº¹ã€è¡€ç®¡ã€æŒ‡ç”²æ²¹ä¹Ÿèƒ½å‡ºæ¥ï¼Œç¡®å®æ˜¯é«˜é¢‘ç»†èŠ‚äº†ã€‚æœ‰é’ˆå¯¹è¿™äº›ä¸œè¥¿åšç‰¹åˆ«çš„ä¼˜åŒ–å—ï¼Ÿè¿˜æ˜¯å…¨æ˜¯é‚£ä¸ªstyle-based GANçš„åŠŸåŠ³ï¼Œæˆ–è€…å¤§æ ·æœ¬é‡çš„åŠŸåŠ³å‘¢ï¼ŸçœŸæ˜¯å¤§åŠ›å‡ºå¥‡è¿¹å‘€ã€‚è¿˜çœŸå°±æ˜¯GANçš„åŠŸåŠ³â€¦â€¦ï¼Ÿ</li></ol><h2 id="how">Howï¼š</h2><h3 id="æ”¶é›†large-scaleæ•°æ®é›†">1. æ”¶é›†large-scaleæ•°æ®é›†</h3><p>raw scanï¼š3000 vertices meshesã€‚1208ä¸ªäººï¼ŒåŒ…æ‹¬å…³äºä»–ä»¬çš„meta dataï¼Œæ¯”å¦‚æ€§åˆ«ï¼Œå¹´é¾„ï¼Œèº«é«˜ï¼Œç§æ—ç­‰ã€‚è¿™äº›äººçš„diversityæ¯”è¾ƒå¤§</p><h3 id="å½¢çŠ¶é‡å»º">2. å½¢çŠ¶é‡å»º</h3><ol type="1"><li>å¯¹é½3D scans å’Œ mesh template<ol type="1"><li>ç”¨äº†ä¸¤ç»„templateï¼Œä¸€ä¸ªæ˜¯ä½åˆ†è¾¨ç‡çš„MANOï¼Œå®ƒå¯ä»¥ç›´æ¥ç”¨è¿›SMPLäººä½“æ¨¡å‹ä¸­ï¼Œæœ‰778ä¸ªé¡¶ç‚¹ï¼›ä¸€ä¸ªæ˜¯é«˜åˆ†è¾¨çš„templateï¼Œæœ‰8407ä¸ªé¡¶ç‚¹</li><li>è·å¾—ç¨ å¯†çš„correspondenceçš„æ–¹æ³•æ˜¯ï¼š<ol type="1"><li>ä»å¤šè§†è§’æ¸²æŸ“è¿™äº›raw scansï¼Œç”¨MediaPipeæ¥æ£€æµ‹2Då…³é”®ç‚¹</li><li>ç”¨linear triangulationæ¥æŠŠ2Då…³é”®ç‚¹è½¬æ¢åˆ°3Dï¼›åˆ©ç”¨æ‰‹æŒ‡éª¨æ¶åˆ°è¡¨é¢å°–ç«¯çš„æŠ•å½±æ¥æ£€æµ‹æŒ‡å°–ã€‚</li><li>ç”¨3Då…³é”®ç‚¹æ¥æŠŠtemplateå’Œ3D scansçš„è¡¨é¢å¯¹é½</li><li>ç”¨Non-rigid Iterative Closest Point algorithm (NICP)æ¥registrationï¼Œå¯»æ‰¾ç¨ å¯†çš„é¡¶ç‚¹å¯¹åº”å…³ç³»</li></ol></li></ol></li><li>è½¬æ¢æˆè§„èŒƒçš„å¼ å¼€æ‰‹æŒçš„å§¿åŠ¿<ol type="1"><li>ç”¨PCAæ„å»ºä¸€ä¸ªæ‰‹éƒ¨å½¢çŠ¶æ¨¡å‹ã€‚</li><li>å…¬å¼å’ŒMANOå‡ ä¹ä¸€æ ·ï¼Œ<span class="math inline">\(\beta\)</span> <span class="math inline">\(\theta\)</span> ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯å½¢çŠ¶å’Œå§¿åŠ¿å‚æ•°ã€‚</li></ol></li></ol><h3 id="é«˜åˆ†è¾¨ç‡å¤–è§‚æ¨¡å‹">3. é«˜åˆ†è¾¨ç‡å¤–è§‚æ¨¡å‹</h3><ol type="1"><li>å«ä¸€ä¸ªå›¾åƒå­¦è‰ºæœ¯å®¶ï¼ˆğŸ˜³ï¼‰è®¾è®¡äº†ä¸€ä¸ªUV templateï¼ŒæŠŠscansç»™unwrapæˆé‚£æ ·äº†</li><li>å¯¹UV texturesè¿›è¡Œé¢„å¤„ç†ï¼Œå»æ‰é˜´å½±å’Œå…‰ç…§ï¼šç”¨PCAæ¥è¯†åˆ«æè¿°é˜´å½±çš„å› ç´ ï¼Œç„¶åæŠŠè¿™äº›å› ç´ å»æ‰ã€‚ï¼ˆPCAå±…ç„¶è¿™ä¹ˆå¥½ç”¨ï¼Ÿï¼ï¼‰</li><li>ç”¨ä¸€ä¸ªå›¾åƒå¤„ç†æ­¥éª¤ï¼Œå°†æ‰‹éƒ¨çº¹ç†æ˜ å°„åˆ°æ›´è‡ªç„¶çš„é¢œè‰²ï¼ŒåŒ…æ‹¬å¢åŠ äº®åº¦ï¼Œä¼½ç›æ ¡æ­£ï¼Œä»¥åŠè°ƒæ•´è‰²è°ƒã€‚</li><li>è®­ç»ƒè¿‡ç¨‹ï¼šä¸åƒå…¶ä»–æ–¹æ³•é‚£æ ·ç›´æ¥æŠŠå¤–è§‚ç©ºé—´æ˜ å°„åˆ°ä¸€ä¸ªä½é¢‘PCAåŸŸï¼Œè€Œæ˜¯ç”¨GANæ¥å»ºæ¨¡æè´¨ã€‚å­¦ä¹ ç‡è¾ƒå°ï¼Œ0.001ï¼›ä¸€ä¸ªæ­£åˆ™æƒé‡50ä¹Ÿå¾ˆæœ‰æ•ˆã€‚ï¼ˆå•Šï¼Ÿè¿™ä¸ªGANå°±è¿™ä¹ˆä¸€å¥å¸¦è¿‡å—ï¼Ÿç›´æ¥ç”¨çš„StyleGANv3ï¼Ÿï¼‰</li></ol><h3 id="å®éªŒ">å®éªŒ</h3><ol type="1"><li>å’ŒMANOæ¯”hand modelï¼š<ol type="1"><li>æ›´ç´§è‡´ï¼Œ5ä¸ªä¸»æˆåˆ†è¡¨ç°90% varianceï¼Œmanoéœ€è¦9ä¸ªæ‰è¡Œ</li><li>æ³›åŒ–åˆ°æ•°æ®é›†å¤–çš„æ‰‹çš„èƒ½åŠ›æ›´å¼º</li><li>ç‰¹å¼‚æ€§è¯¯å·®ï¼ˆspecificity errorï¼‰ï¼Ÿè¡¡é‡ç”Ÿæˆçš„æ‰‹å’Œground truthçš„è¯¯å·®</li></ol></li><li>é‡å»ºå°å­©çš„æ‰‹ï¼Œæ•ˆæœæ›´å¥½</li><li>ä»å•å¼ å›¾åƒè¿›è¡Œ3Dé‡å»ºï¼š<ol type="1"><li>ç”Ÿæˆæ•°æ®é›†ï¼šç”¨è‡ªå·±è®­çš„GANæ¨¡å‹ç”Ÿæˆ30000å¼ å›¾åƒï¼Œä¸ºäº†æ›´çœŸå®ï¼Œæ¸²æŸ“çš„æ‰‹è·ŸShapeNetä¸­çš„ç‰©ä½“æœ‰äº¤äº’ï¼Œä»¥åŠæ˜¯å’Œç”¨SMPLè¡¨ç¤ºçš„äººæ”¾åœ¨ä¸€èµ·çš„</li><li>æ¨¡å‹ç›´æ¥å‚è€ƒ3ï¼Œ14ï¼Œ16ï¼›åŠ äº†ä¸€ä¸ªé¢„æµ‹æè´¨å‚æ•°çš„åˆ†æ”¯</li><li>lossï¼šL2 between estimated and gt shape parameterï¼Œ pose parameterï¼Œand 3D verticesï¼› L1 between estimated and gt UV mapï¼›L1 between estimated and gt 2D imageï¼›LPIPS loss on two images</li><li>å¦å¤–è®¾è®¡äº†in-the-wildæ•°æ®é›†ï¼Œç”¨é¢„è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹handy å§¿åŠ¿ã€å½¢çŠ¶å’Œæè´¨å‚æ•°ï¼Œç„¶ååªä¼˜åŒ–æè´¨å‚æ•°wæ¥æ‹Ÿåˆæè´¨ã€‚</li><li>ä¼˜åŒ–å‡½æ•°åŒ…æ‹¬L1 and LPIPS loss on two imagesï¼Œä»¥åŠä¸€ä¸ªå¯¹wçš„L2æ­£åˆ™ã€‚å¾—åˆ°äº†æ”¹è¿›çš„æè´¨å‚æ•°wâ€˜ä¹‹åï¼Œfinetuneå›å½’ç½‘ç»œã€‚</li><li>ä¸ºäº†å®šé‡è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„çº¹ç†é‡å»ºï¼Œæˆ‘ä»¬å‘ç½‘ç»œæä¾›æ•°æ®ä¸­ä½¿ç”¨çš„æ‰«æè®¾å¤‡çš„å›¾åƒã€‚gt UV mapç”¨çš„æ˜¯ä¹‹å‰registrationåå¾—åˆ°çš„ã€‚ï¼ˆæˆ‘ä¸ç†è§£è¯¶ï¼Œè¿™æ ·çœŸçš„èƒ½è·ŸHTMLå…¬å¹³æ¯”è¾ƒå—ï¼Ÿä¸€æ–¹é¢ä½ çš„handyå°±æ˜¯ä»è¿™äº›æ•°æ®ä¸­æ¥çš„ï¼Œå½“ç„¶èƒ½å¯¹in-distributionçš„ä¸œè¥¿æ‹Ÿåˆå¾—æ›´å¥½å•Šï¼Ÿå¦ä¸€æ–¹é¢HTMLç”Ÿæˆçš„UV mapå’Œä½ çš„å®šä¹‰æ˜¯ä¸€æ ·çš„å—ï¼Ÿè¿™ä¸ªgt UV mapå¯¹å®ƒæ¥è¯´æœ‰ç”¨å—ï¼Ÿï¼‰</li><li>ç»“è®ºæ˜¯ï¼šhandy+GANèƒ½å¾—åˆ°é«˜é¢‘ç»†èŠ‚ï¼Œç”šè‡³çš±çº¹ã€æˆ’æŒ‡ã€çº¹èº«ã€æŒ‡ç”²æ²¹ã€ç™½ç™œé£ä¹‹ç±»çš„ï¼›handy+PCAä¼šè¿‡æ¸¡å¹³æ»‘ï¼Œç”šè‡³å¯¹è‚¤è‰²çš„é‡å»ºå¤±è´¥ï¼›HTMLæ›´ä¸è¡Œã€‚</li></ol></li><li>Test on FreiHand åˆ·æ–°äº†æŒ‡æ ‡ï¼Œ7.8 MPVPE and MPJPEâ€¦â€¦</li><li>ä»ç‚¹äº‘é‡å»ºå½¢çŠ¶å’Œå§¿åŠ¿ã€‚é™ç»´æ‰“å‡»äº†MANOå’ŒLISAï¼Œå³ä½¿ç”¨Hand+MANO+10ä¸ªPCA Componentsï¼Œä¹Ÿæ¯”å…¶ä»–æ–¹æ³•å¥½å¾ˆå¤šâ€¦â€¦</li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Computer Vision </tag>
            
            <tag> NeRF </tag>
            
            <tag> Hand </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGPT Applications to be explored</title>
      <link href="/blog/ChatGPT-Applications-to-be-explored/"/>
      <url>/blog/ChatGPT-Applications-to-be-explored/</url>
      
        <content type="html"><![CDATA[<p>ä»Šå¤©é€› githubï¼Œå‘ç°äº†ä¸€äº›å¾ˆ amazing çš„chatgpt applicationsï¼Œæ‘˜å½•ä¸€äº›æ„Ÿå…´è¶£çš„ç²¾ååœ¨æ­¤ã€‚çœŸæ˜¯æ„Ÿæ…¨ï¼šLLM ä»¥æ¥å¤©å¤©é£äº‘å˜å¹»ï¼Œå¼„æ½®å„¿åœ¨å‰é¢å…´é£ä½œæµªï¼Œæˆ‘åœ¨åé¢æœ›å…¶é¡¹èƒŒâ€¦â€¦</p><ol type="1"><li><p>(Useful) egoist / openai-proxy</p><p>ç”¨ Vercel å¼€ä¸€ä¸ªå°çš„ Proxy serverï¼Œè½¬å‘ gpt APIï¼Œè¿™æ ·å¯ä»¥ç»•å¼€æœ‰äº›å›½å®¶åœ°åŒºçš„ IP é™åˆ¶</p></li><li><p>BuilderIO / ai-shell</p><p>åœ¨å‘½ä»¤è¡Œé‡Œä½¿ç”¨ chatgptï¼ŒæŠŠè‡ªç„¶è¯­è¨€è½¬åŒ–æˆ Linux commandsï¼Œå‘½ä»¤æ˜¯ <code>ai [texts]</code></p></li><li><p>eli64s / readme-ai</p><p>ä¸€ä¸ªè½»é‡çš„ scriptï¼Œæ ¹æ® repository ç”Ÿæˆé…·ç‚«çš„ readme æ–‡ä»¶</p></li><li><p>efJerryYang / chatgpt-cli</p><p>å‘½ä»¤è¡Œ chatgpt client</p></li><li><p>yufeikang / ai-cli</p><p>å¦ä¸€ä¸ªå‘½ä»¤è¡Œ chatgpt clientï¼ˆå®æµ‹çš„æ—¶å€™å†å¯¹æ¯”ä¸€ä¸‹è¿™ä¿©ï¼‰</p></li><li><p>mukulpatnaik / researchgpt</p><p>è¾“å…¥è®ºæ–‡ PDF æ–‡ä»¶ï¼Œç„¶åå’Œ gpt èŠè®ºæ–‡ã€‚ä¸€ä¸ªç”¨ Flask å¼€å‘çš„ web client è²Œä¼¼ï¼Œå¯ä»¥å†ä»”ç»†çœ‹ä¸€ä¸‹å’‹å®ç°çš„ï¼ŒæŒºæœ‰æ„æ€</p></li><li><p>(â­ï¸ Amazing) AntonOsika / gpt-engineer</p><p>å¾ˆæ–¹ä¾¿å®‰è£…ï¼Œpip install å°±å¥½äº†ï¼ç›´æ¥é€šè¿‡æè¿° + AI è¿½é—® + è¡¥å……ç»†èŠ‚ï¼Œç”Ÿæˆä¸€ä¸ªä»£ç é¡¹ç›®</p></li><li><p>(â­ï¸ Amazing) Yidadaa / ChatGPT-Next-Web</p><p>å¥½åƒå¾ˆå®ç”¨çš„ web GUIï¼ä¸€é”®éƒ¨ç½²åˆ° Vercelã€‚æˆ‘æ‰¾è¿™ç©æ„ä¸»è¦æ˜¯ä¸ºäº†ç›´æ¥ç”¨ API è®¿é—® GPT-4ï¼Œå°±ä¸ç”¨è®¢é˜…æ¯ä¸ªæœˆçš„ ChatGPT Plus äº†ï¼Œåè€…å¤ªè´µäº†ï¼Œä¹Ÿç”¨ä¸äº†é‚£ä¹ˆå¤š</p></li></ol><p><img src="https://s2.loli.net/2023/07/17/QU2X5ckaCmyJeGv.png" /></p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Back to Homeland</title>
      <link href="/blog/Back-to-Homeland/"/>
      <url>/blog/Back-to-Homeland/</url>
      
        <content type="html"><![CDATA[<p>ä»Šå¤©ï¼Œæ©æ–½çš„å‚æ™šä¼¼ä¹æ²¡æœ‰å¤•é˜³ã€‚å¤©ç©ºæ˜¯æ·±è“è‰²ï¼Œæ·±å±±ä¸Šæœ‰å‡ åªé»‘é¸Ÿæ è¿‡ã€‚åšé‡çš„ç‹—å«å£°ã€‚æˆ‘é—»åˆ°ä¸€ç§ä½¿æˆ‘æ„Ÿåˆ°æ‚²æˆšçš„æ°”å‘³ï¼Œæˆ–è®¸æ˜¯å’Œè¿‡å»æŸç§æ‚²æˆšçš„å›å¿†è”ç³»èµ·æ¥ã€‚å…·ä½“å›å¿†å†…å®¹å€’ä¹Ÿè®°ä¸æ¸…ã€‚æ˜å¤©æˆ‘è¦èµ°äº†ï¼Œæˆ‘è¯´è¦åƒç‚¹è¾£çš„ï¼Œå»äº†å—è¾¹åˆæ²¡æœ‰äº†ã€‚ç»“æœè¿˜æ˜¯å»åƒäº†æ½®æ±•ç‰›è‚‰ã€‚å¦ˆå¦ˆä»Šå¤©æ¯”è¾ƒæ˜“æ€’ï¼Œæˆ‘çœ‹å¾—åˆ°å¥¹çš„ä¼¤æ„Ÿã€‚</p><p>å›å›½ï¼Œè¿™ä¸ªè¯å¯¹æˆ‘æ¥è¯´ä¸ç®—ä»€ä¹ˆï¼Œèº«å¤„å›½å†…çš„æ—¶å€™ï¼Œè‡ªå·±æ˜¯æ²¡æœ‰æ¦‚å¿µçš„ã€‚åªæœ‰åœ¨å›½å¤–çš„æ—¶å€™ï¼Œæ‰å¯¹å›½å†…æœ‰æ¦‚å¿µã€‚è¿™ä¸€ç‚¹çœŸæ˜¯è®½åˆºã€‚å›½å†…çš„æ—¶é—´åŒ†åŒ†è€Œè¿‡ï¼Œä»¥å‰ä¼šä¸ºäº†åœ¨å›½å¤–å¤šå¾…å‡ å¹´è€Œäº‰å–ï¼Œäº‰å–åˆ°äº†åˆé›€è·ƒã€‚ç°åœ¨çœŸè¦å¾…é‚£ä¹ˆå¤šå¹´äº†ï¼Œæ‰æ„Ÿåˆ°è‡ªå·±æ­£ååœ¨ä»€ä¹ˆé£é©°è€Œè¿œå»çš„åˆ—è½¦ä¸Šã€‚</p><p>äººç”Ÿåœ¨ä¸–ï¼Œå¥”å¤´è¿™ä¸ªè¯ï¼Œæˆ‘æ˜¯æƒ³è§£æ„å®ƒçš„ã€‚äººæ´»ç€ä¸ä¸ºäº†ä»€ä¹ˆï¼Œæ´»å°±æ´»äº†ã€‚å¯æ˜¯æˆ‘åˆ†æ˜åœ¨åŠªåŠ›ä»€ä¹ˆï¼Œåœ¨æŠ“ä½äº›ä»€ä¹ˆã€‚å˜´ç¡¬ç½¢äº†ï¼Œè°èƒ½è¶…å‡¡è„±ä¿—ï¼Œæ²¡ç‚¹æƒ¦å¿µçš„ä¸œè¥¿ï¼Ÿäº²äººï¼Œå‘å¾€çš„æŸç§ç”Ÿæ´»ï¼Œæˆå°±æ„Ÿï¼Œè¿™å°±æ˜¯æˆ‘çš„å¥”å¤´ã€‚åªæ˜¯ï¼šåœåœ¨åŸåœ°åŸæ¥æ˜¯ä¸€ç§å¹¸è¿ï¼Œä¹Ÿæ˜¯ä¸€ç§ç‰¹æƒã€‚</p>]]></content>
      
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>å¥¶å¥¶</title>
      <link href="/blog/Grandma/"/>
      <url>/blog/Grandma/</url>
      
        <content type="html"><![CDATA[<p>å¥¶å¥¶èº«ä½“è¿˜å¥½ï¼Œä½†å¿ƒæ€ä¸å¥½ã€‚å¥å¿˜ã€å›ºæ‰§ï¼Œè‡ªé—­ï¼Œæ‚²è§‚ã€‚å¥¹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆç›¼å¤´äº†ã€‚å¥¹è§‰å¾—æ´»å¾—å¤±è´¥ï¼Œæ´»å¾—ä¸å¥½ã€‚å¥¹è§‰å¾—å¦‚ä»Šè„¸ä¸Šæ— å…‰ã€‚å¥¹è¯´ï¼š</p><p>æˆ‘æœ€ç—›æ¨åˆ«äººï¼ŒåŠç†Ÿä¸ç†Ÿçš„äººè§é¢ï¼Œè·Ÿä½ æ‰“æ‹›å‘¼ï¼Œé—®ä½ å®¶é‡Œè¿‘å†µæ€ä¹ˆæ ·ã€‚æˆ‘å¾ˆç”Ÿæ°”ï¼Œç®€ç›´æƒ³éª‚å›å»ã€‚å¯æ˜¯ä½ éª‚äº†å§ï¼Œäººå®¶è§‰å¾—ä½ æ˜¯ç¥ç»ç—…ã€‚å¯æ˜¯æˆ‘è¦æ€ä¹ˆå›ç­”å‘¢ï¼Ÿæˆ‘è¿™ä¸¤ä¸ªå„¿å­ï¼Œä¸€ä¸ªå·¥ä½œéƒ½ä¸¢äº†ï¼Œä¸€ä¸ªèº«ä½“åˆé‚£æ ·ä¸å¥½ã€‚</p><p>æˆ‘å‘½è‹¦å•Šã€‚æˆ‘ä»å°å®¶é‡Œç©·ï¼Œä¹Ÿæ²¡äººç®¡æˆ‘ã€‚13å²å°±å‡ºå»å·¥ä½œå…»æ´»è‡ªå·±äº†ã€‚è·Ÿäº†ä½ çˆ·çˆ·ï¼Œè¿‡çš„éƒ½æ˜¯è‹¦æ—¥å­ã€‚ä»–ä¸€ä¸ªæœˆä¸‰åå››å—äº”è§’é’±ï¼ŒäºŒåå—é’±ç»™ä»–çˆ¸å¦ˆï¼Œé›·æ‰“ä¸åŠ¨çš„ã€‚å®¶é‡Œé¥­éƒ½åƒä¸èµ·äº†ï¼Œä¹Ÿè¦ç»™ã€‚å‰©ä¸‹åå››å—äº”ï¼Œåå—é’±ç»™æˆ‘ï¼Œä»–ç•™å››å—äº”ã€‚æŠ½æœ€å·®çš„çƒŸï¼Œèµ°å¾—é‚£ä¹ˆæ—©ã€‚</p><p>æˆ‘è¦ç®¡å®¶é‡Œæ‰€æœ‰äº‹ã€‚ä»–ä»€ä¹ˆä¹Ÿä¸ç®¡å•Šï¼Œä¸€æ—¥ä¸‰é¤ï¼Œä»–å¦ˆï¼Œä¸¤ä¸ªå„¿å­ï¼Œå»æ²³è¾¹æ´—è¡£æœã€‚æˆ‘è¿˜è¦ä¸Šç­ã€‚æˆ‘æ¯å¤©éƒ½å¥½ç´¯ã€‚</p><p>å¥½ä¸å®¹æ˜“ï¼Œæ—¥å­ç¨å¾®å¥½ä¸€ç‚¹äº†ã€‚ä»–åˆèµ°äº†ï¼æˆ‘é‚£æ—¶ç®€ç›´æ¨ä»–ã€‚</p><p>åˆ«çœ‹æˆ‘ç°åœ¨è¿™æ ·ï¼Œç°åœ¨æ˜¯æˆ‘æœ€è½»æ¾çš„æ—¥å­ã€‚å…±äº§å…šç»™æˆ‘å‘é’±ã€‚æ‰€ä»¥æˆ‘è¯´ï¼Œæ„Ÿè°¢å…±äº§å…šã€‚æˆ‘æ‹¿è‡ªå·±çš„é’±ï¼Œè¿‡è‡ªå·±çš„æ—¥å­ï¼å°±æ˜¯è®°æ€§ä¸å¥½ã€‚äººæ´»åˆ°è¿è‡ªç†èƒ½åŠ›ä¹Ÿæ²¡æœ‰äº†ï¼Œè¿˜æœ‰ä»€ä¹ˆæ„æ€ã€‚è·Ÿä½ çˆ¸è¯´å¥½äº†ï¼Œåˆ°æ—¶å€™æˆ‘æ­»åœ¨è¿™é—´å±‹å­äº†ï¼Œå°±ç«åŒ–ã€‚æˆ‘éƒ½çœ‹å¾—å¼€ã€‚</p><p>æ­»ï¼æˆ‘å¬åˆ°å°±çœ¼æ³ªç›´æ¹æ¹åœ°æµã€‚å¥¶å¥¶è¯´ï¼Œä¸è¯´è¿™ä¸ªäº†ã€‚å¥¹ä¹ŸæŠ¹çœ¼æ³ªã€‚</p><p>å¥¶å¥¶ä¸€ä¸ªæœˆé¢†ä¸‰åƒå¤šé€€ä¼‘å·¥èµ„ï¼Œè‡ªå·±çœåƒä¿­ç”¨ï¼ŒåªèŠ±å¾—äº†ä¸€åƒå¤šã€‚å‰©çš„ï¼Œå­˜åœ¨é‚£ï¼Œæ¯å¹´å¯’æš‘å‡æˆ‘å’Œå¦¹å¦¹å»çœ‹å¥¹ï¼Œå¥¹å‘ç»™æˆ‘ä»¬ã€‚</p><p>æˆ‘è¯´çˆ¸çˆ¸ï¼Œä½ å¸®æˆ‘æŠŠé’±é€€ç»™å¥¹ã€‚</p><p>çˆ¸çˆ¸è¯´ä½ æ‹¿ç€ã€‚å¥¹è‡ªå·±èŠ±ä¸å®Œï¼Œè¿™é’±ä¸ç»™ä½ ä»¬ï¼Œå¥¹æ‹¿ç€æœ‰ä»€ä¹ˆç”¨äº†ï¼Ÿç»™äº†å¥¹å®‰å¿ƒã€‚</p><p>æˆ‘èµ°çš„æ—¶å€™ï¼Œå¥¹é€å‡ºå±‹å­ï¼Œé€åˆ°æ¥¼æ¢¯å£ã€‚</p><p>æˆ‘èµ°è¿œäº†ï¼Œçˆ¸çˆ¸è¯´ï¼Œä½ å›å¤´å†ç»™å¥¶å¥¶æŒ¥æŒ¥æ‰‹ï¼Œä½ çœ‹å¥¹åœ¨é˜³å°ä¸Šçœ‹ä½ å‘¢ã€‚</p><p>æˆ‘è½¬å¤´çœ‹åˆ°ä¸€ç°‡èŠ±ç™½çš„å¤´å‘ï¼Œè¿œè¿œçš„ï¼Œå¾ˆå°ã€‚</p><p>èƒ½ä¸èƒ½ä¸è¦æœ‰ç¦»åˆ«å‘¢ï¼Ÿ</p>]]></content>
      
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Solutions to Common Problems in Pytorch3D Rendering</title>
      <link href="/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/"/>
      <url>/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/</url>
      
        <content type="html"><![CDATA[<h1 id="pytorch3d-rendering-çš„ä¸€äº›ç–‘éš¾æ‚ç—‡">Pytorch3D Rendering çš„ä¸€äº›ç–‘éš¾æ‚ç—‡</h1><p>æœ‰å“ªäº›ï¼Ÿ</p><ol type="1"><li>æœ‰äº†ç›¸æœºå†…å‚ Kï¼Œè€Œrenderåˆéœ€è¦NDCåæ ‡ç³»ï¼Œé‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœºï¼Ÿ</li><li>å›¾åƒçš„é»„è“è‰²åäº†ï¼Ÿ</li><li>render å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡ï¼Ÿæ€ä¹ˆæŠ—é”¯é½¿ï¼ˆAntialiasingï¼‰ï¼Ÿ</li><li>çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºï¼Œå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·ï¼Œæ€æ ·æ›´è‡ªç„¶ï¼Ÿ</li><li>æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªï¼Œæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†ï¼Ÿ</li><li>æ²¡è§£å†³çš„é—®é¢˜ï¼šPBRï¼ˆphysical based renderingï¼‰</li></ol><span id="more"></span><h3 id="æœ‰äº†ç›¸æœºå†…å‚-kè€Œrenderåˆéœ€è¦ndcåæ ‡ç³»é‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœº">1. æœ‰äº†ç›¸æœºå†…å‚ Kï¼Œè€Œrenderåˆéœ€è¦NDCåæ ‡ç³»ï¼Œé‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœºï¼Ÿ</h3><p>è¿™é‡Œçš„å‘åœ¨äºï¼Œcameraæœ¬èº«æ”¯æŒä»»æ„åæ ‡ç³»ï¼Œæ¯”å¦‚Freihandæä¾›çš„æ˜¯screenæ˜¯224*224çš„ç›¸æœºåæ ‡ç³»ã€‚ä½†æ˜¯ï¼Œrenderæ˜¯é»˜è®¤NDCåæ ‡ç³»çš„ï¼ä¹Ÿå°±æ˜¯normalized coordinate systemï¼Œxå’Œyæ˜¯normalizedåˆ°[-1,1]çš„ã€‚</p><p>ä¸€å¼€å§‹æˆ‘ç›´æ¥æŠŠç›¸æœºå†…å‚ä¼ ç»™<code>PerspectiveCameras</code>ï¼Œå¹¶ä¸”å®šä¹‰æˆ‘çš„ç›¸æœºscreenæ˜¯224*224ï¼Œåƒè¿™æ ·ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>å®Œå…¨ä¸æŠ¥é”™ï¼Œå°±æ˜¯æœ‰é—®é¢˜ï¼šrender è¿‡åæ²¡ä¸œè¥¿åœ¨ç”»é¢ä¸Šã€‚</p><h4 id="è§£å†³">è§£å†³ï¼š</h4><p>æˆ‘æœ€ååœ¨<a href="https://pytorch3d.org/docs/cameras">å®˜æ–¹æ–‡æ¡£</a>æ‰¾åˆ°ä¸èµ·çœ¼çš„ä¸€å¥ï¼š</p><blockquote><p>The PyTorch3D renderer for both meshes and point clouds assumes that the camera transformed points, meaning the points passed as input to the rasterizer, are in PyTorch3D's NDC space.</p></blockquote><figure><img src="https://user-images.githubusercontent.com/669761/145090051-67b506d7-6d73-4826-a677-5873b7cb92ba.png" alt="ï¼ˆä¸–ç•Œåæ ‡ç³» -&gt; ç›¸æœºåæ ‡ç³» -&gt; ndcåæ ‡ç³» -&gt; å›¾åƒåæ ‡ç³»ï¼‰" /><figcaption>ï¼ˆä¸–ç•Œåæ ‡ç³» -&gt; ç›¸æœºåæ ‡ç³» -&gt; ndcåæ ‡ç³» -&gt; å›¾åƒåæ ‡ç³»ï¼‰</figcaption></figure><p>æˆ‘ä¸€çœ‹ï¼ŒåŸæ¥é»˜è®¤PerspectiveCamerasæ˜¯ndcåæ ‡ç³»çš„ï¼Œ<code>in_ndc = False</code> by defaultï¼</p><p>æ‰€ä»¥è§£å†³æ–¹æ³•å°±æ˜¯ï¼š</p><blockquote><p>Screen space camera parameters are common and for that case the user needs to set <code>in_ndc</code> to <code>False</code> and also provide the <code>image_size=(height, width)</code> of the screen, aka the image.</p></blockquote><p>é‚£ä¹ˆåŠ ä¸€ä¸ªå‚æ•°å°±å¥½äº†ï¼Œå¯æ˜¯è°çŸ¥é“è¿™é—®é¢˜å›°æ‰°äº†æˆ‘æ•´æ•´ä¸¤ä¸‰å¤©ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, in_ndc=<span class="literal">False</span>, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>å¦å¤–ï¼Œæˆ‘è¿˜æ‰¾åˆ°äº†å¦‚ä¸‹è¿™ä¸ªç­‰ä»·æ–¹æ³•ï¼Œæ˜¯å…ˆæŠŠå†…å‚è½¬åˆ°NDCåæ ‡ç³»ï¼Œå†ä¼ ç»™<code>PerspectiveCameras</code>ã€‚ï¼ˆè‡³äºä¸ºä»€ä¹ˆæ¢ç´¢åˆ°è¿™ä¸ªæ–¹æ³•ï¼Œåœ¨åé¢é—®é¢˜ 3 é‡Œå¯ä»¥æ‰¾åˆ°åŸå› â€¦ï¼‰</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ndc_fcl_prp</span>(<span class="params">Ks</span>):</span></span><br><span class="line">        ndc_fx = Ks[:, <span class="number">0</span>, <span class="number">0</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_fy = Ks[:, <span class="number">1</span>, <span class="number">1</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_px = - (Ks[:, <span class="number">0</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_py = - (Ks[:, <span class="number">1</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        focal_length = torch.stack([ndc_fx, ndc_fy], dim=-<span class="number">1</span>)</span><br><span class="line">        principal_point = torch.stack([ndc_px, ndc_py], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> focal_length, principal_point</span><br><span class="line"></span><br><span class="line">fcl, prp = get_ndc_fcl_prp(Ks)</span><br><span class="line">cameras = PerspectiveCameras(focal_length=-fcl, principal_point=prp)</span><br></pre></td></tr></table></figure><p>æ³¨æ„<code>focal_length=-fcl</code>ï¼Œè¿™ä¸ªè´Ÿå·æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯å¦ä¸€ä¸ªå‘äº†å“ˆå“ˆå“ˆå“ˆã€‚</p><p>ç­”æ¡ˆæ˜¯ï¼špytorch3dåæ ‡ç³»çš„conventionå’Œæˆ‘çš„ç›¸æœºä¸ä¸€æ ·ï¼Œå®ƒæ˜¯+XæŒ‡å‘å·¦ï¼Œ+YæŒ‡å‘ä¸Šï¼Œ+ZæŒ‡å‘å›¾åƒå¹³é¢å¤–ã€‚è¿™å…¶ä¸­æœ‰ä¸ªä¸Šä¸‹å·¦å³é•œåƒçš„å…³ç³»ã€‚</p><h3 id="å›¾åƒçš„é»„è“è‰²åäº†">2. å›¾åƒçš„é»„è“è‰²åäº†ï¼Ÿ</h3><p>cv2çš„å›¾åƒæ˜¯BGRï¼ˆè€ç”Ÿå¸¸è°ˆäº†ï¼‰ï¼Œpytorch3dçš„æ˜¯RGBã€‚å¦‚æœå›¾åƒçš„é»„è“è‰²ç›¸åäº†ï¼ŒåŸºæœ¬å°±æ˜¯è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦ç¿»è½¬ä¸€ä¸‹ï¼Œå¯ä»¥ç”¨torchçš„<code>clip(dim=(2,))</code></p><h3 id="render-å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡æ€ä¹ˆæŠ—é”¯é½¿antialiasing">3. render å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡ï¼Ÿæ€ä¹ˆæŠ—é”¯é½¿ï¼ˆAntialiasingï¼‰ï¼Ÿ</h3><p>é”¯é½¿å°±æ˜¯è¯´åƒä¸‹å›¾è¿™æ ·ï¼Œç‰©ä½“çš„è¾¹ç¼˜å¾ˆå°–é”ï¼Œåƒç´ ç‚¹ç²’ç²’åˆ†æ˜ï¼</p><figure><img src="https://s2.loli.net/2023/04/23/6gx5DEXJK9uMGwS.png" alt="rand_4_skin_rendered_bad" /><figcaption>rand_4_skin_rendered_bad</figcaption></figure><p>ä¸‹é¢æ˜¯æˆ‘æŠ—é”¯é½¿å¤„ç†åçš„æ•ˆæœï¼Œå¯ä»¥çœ‹è§è¾¹ç¼˜æŸ”å’Œäº†å¾ˆå¤šï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/LAchPHVfImRtkDs.png" alt="rand_4_skin_rendered" /><figcaption>rand_4_skin_rendered</figcaption></figure><p>ï¼ˆæˆ‘çœŸçš„æäº†ä¸€å‘¨è¿™ä¸ªé—®é¢˜â€¦â€¦çœ‹çœ‹æˆ‘çš„å¿ƒè·¯å†ç¨‹ï¼š</p><ol type="1"><li>æ˜¯ä¸æ˜¯ camera æ²¡æœ‰ç”¨ NDCï¼Œè€Œæ˜¯ç›´æ¥ç”¨224x224çš„åæ ‡ç³»ï¼Œå¯¼è‡´æŠ•å½±è¿‡ç¨‹æœ‰æŸå¤±ï¼Ÿæ‰€ä»¥æˆ‘è¯•äº†å…ˆè½¬æ¢æˆ NDC åæ ‡ç³»çš„ç›¸æœºï¼Œå†renderã€‚ç­”æ¡ˆæ˜¯ï¼Œæ²¡æœ‰å½±å“ã€‚</li><li>æ˜¯ä¸æ˜¯ Shader çš„å‚æ•°è®¾ç½®å¾—ä¸å¯¹ï¼Œæ¯”å¦‚ <code>blur_radius</code> å’Œ <code>faces_per_pixel</code> åº”è¯¥è°ƒå¤§ä¸€äº›ï¼Ÿè¿™å…¶å®æ˜¯ä¸€ä¸ªå¾ˆç›´è§‚çš„æƒ³æ³•äº†ï¼Œç”šè‡³ä¸€ä¸ªæœ‰ç»éªŒçš„å­¦é•¿çœ‹äº†ä¹‹åéƒ½å‘Šè¯‰æˆ‘åº”è¯¥æ˜¯è¿™ä¸ªé—®é¢˜ã€‚å¯æ˜¯å½“æˆ‘ç–¯ç‹‚è°ƒå¤§è¿™ä¸¤ä¸ªå‚æ•°ï¼Œå‘ç°å¹¶æ²¡æœ‰æ”¹å˜è¿™ä¸ªé—®é¢˜ã€‚blur_radius åªä¼šè®©ç‰©ä½“å†…éƒ¨çš„æè´¨æ›´æ¨¡ç³Šï¼Œä½†æ˜¯è¾¹ç¼˜çš„é”¯é½¿å®Œå…¨æ²¡æ”¹å˜ã€‚faces_per_pixelæ›´æ˜¯æ— ç›Šï¼Œå‡ ä¹ä¸å½±å“æ•ˆæœã€‚</li><li>æ˜¯ä¸æ˜¯å›¾åƒå°ºå¯¸å¤ªå°äº†ï¼ˆ224x224ï¼‰ï¼Œåªèƒ½è¾¾åˆ°è¿™ä¹ˆä¸ªæ•ˆæœï¼Ÿæˆ‘é¦–å…ˆæµ‹è¯•äº†è°ƒå¤§å›¾åƒå°ºå¯¸ï¼Œåˆ°<code>1024x1024</code>ï¼Œå‘ç°é”¯é½¿è¾¹ç¼˜çš„ç¡®æ˜¯ä¸æ˜æ˜¾äº†ï¼å¯æ˜¯æˆ‘åˆçœ‹äº†ç›¸æœºæ‹æ‘„çš„åŸå§‹å›¾åƒï¼Œè™½ç„¶æ˜¯æœ‰ç‚¹æ¨¡ç³Šï¼Œä½†æ˜¯ä¸è‡³äºè¿™ä¹ˆå¤§çš„é”¯é½¿å‘€ï¼Œè‚¯å®šè¿˜æœ‰åˆ«çš„é—®é¢˜ã€‚ï¼‰</li></ol><h4 id="è§£å†³-1">è§£å†³ï¼š</h4><p>ç»ˆäºï¼Œåœ¨è¿™ä¸ªissueé‡Œæ‰¾åˆ°åŒæ ·çš„é—®é¢˜ï¼šhttps://github.com/facebookresearch/pytorch3d/issues/399</p><p>è§£å†³æ–¹æ¡ˆæ˜¯ï¼š</p><blockquote><p>render at a higher resolution and then use average pooling to reduce back to the target resolution</p></blockquote><p>å±…ç„¶è¿™ä¹ˆæš´åŠ›â€¦â€¦ä¸è¿‡issueé‡Œé¢æœ‰å¾ˆè¯¦ç»†çš„è§£é‡Šï¼Œä¹Ÿèƒ½ç†è§£ï¼Œè¿™å°±æ˜¯renderåŸç†ä¹‹å¤–éœ€è¦è€ƒè™‘çš„äº‹æƒ…ï¼Œç”šè‡³ç®—ä¸ä¸Šä»€ä¹ˆbugã€‚</p><p>ä»£ç å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">aa_factor = <span class="number">3</span> <span class="comment"># Anti-aliasing factor</span></span><br><span class="line">raster_settings_soft = RasterizationSettings(</span><br><span class="line">        image_size=<span class="number">224</span> * aa_factor, </span><br><span class="line">    )</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">images = renderer(mesh)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># NHWC -&gt; NCHW</span></span><br><span class="line">images = F.avg_pool2d(images, kernel_size=aa_factor, stride=aa_factor)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; NHWC</span></span><br></pre></td></tr></table></figure><h3 id="çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·æ€æ ·æ›´è‡ªç„¶">4. çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºï¼Œå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·ï¼Œæ€æ ·æ›´è‡ªç„¶ï¼Ÿ</h3><p>ä¸€å¼€å§‹ï¼Œçš®è‚¤ render å‡ºæ¥åƒè¿™æ ·ï¼Œè·Ÿé™¶ç“·ä¼¼çš„ï¼Œåƒè¯å—ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/8NWzr7txYKyqRk6.png" alt="rand_4_skin_rendered_bad2" /><figcaption>rand_4_skin_rendered_bad2</figcaption></figure><p>æ”¹è¿›åï¼Œæ•ˆæœè¿™æ ·ï¼Œè‡ªç„¶å¤šäº†ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/4qHNOs58ZhEI6Ry.png" alt="rand_4_skin_rendered_big" /><figcaption>rand_4_skin_rendered_big</figcaption></figure><h4 id="è§£å†³-2">è§£å†³ï¼š</h4><p>å…¶å®ææ¸…æ¥šæè´¨ç›¸å…³çš„ä¸€äº›å‚æ•°å°±å¥½äº†ã€‚ä¸»è¦æ¥è¯´ï¼Œè¿™ä¸ªåå…‰æ˜¯ç”±è¿™ä¸¤ä¸ªé‡å†³å®šçš„ï¼š</p><ol type="1"><li><code>specular_color</code>: specular reflectivity of the materialï¼ŒæŒ‡å®šé•œé¢åå°„é¢œè‰²ï¼Œåœ¨è¡¨é¢æœ‰å…‰æ³½å’Œé•œé¢èˆ¬çš„åœ°æ–¹çœ‹åˆ°çš„é¢œè‰²ã€‚</li><li><code>shininess</code>ï¼šå®šä¹‰æè´¨ä¸­é•œé¢åå°„é«˜å…‰çš„ç„¦ç‚¹ã€‚ å€¼é€šå¸¸ä»‹äº 0 åˆ° 1000 ä¹‹é—´ï¼Œè¾ƒé«˜çš„å€¼ä¼šäº§ç”Ÿç´§å¯†ã€é›†ä¸­çš„é«˜å…‰ã€‚</li></ol><p>æ³¨æ„è¿™é‡Œæ˜¯æ”¹ç‰©ä½“materialçš„è¿™äº›å‚æ•°ã€‚è™½ç„¶lightingä¹Ÿæœ‰è¿™äº›å‚æ•°å®šä¹‰ï¼Œä½†è¿™æ˜¯å…³äºå…‰æºçš„ï¼Œå’Œè¿™ä¸ªåå…‰æ²¡æœ‰å…³ç³»ã€‚</p><p>æ‰€ä»¥ä¿®æ”¹å¾ˆç®€å•ï¼šå®šä¹‰materialsç±»ï¼Œè°ƒæ•´<code>specular_color</code>ã€‚é»˜è®¤æ˜¯<code>1,1,1</code>ï¼Œå°±æ˜¯çº¯ç™½è‰²ï¼›è°ƒæˆ<code>0.2,0.2,0.2</code>æ¯”è¾ƒé€‚åˆäººçš„çš®è‚¤ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch3d.renderer <span class="keyword">import</span> Materials</span><br><span class="line"></span><br><span class="line">materials = Materials(</span><br><span class="line">    specular_color=((<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),), <span class="comment"># é»˜è®¤æ˜¯1,1,1ï¼Œå°±æ˜¯çº¯ç™½è‰²ï¼›æµ‹è¯•å‘ç°è°ƒæˆ0.2,0.2,0.2æ¯”è¾ƒé€‚åˆäººçš„çš®è‚¤ã€‚</span></span><br><span class="line">    shininess=<span class="number">30</span>, <span class="comment"># é»˜è®¤å€¼æ˜¯ 64ï¼Œçœ‹ä¸Šå»é«˜å…‰ç¨å¾®æœ‰ç‚¹èšé›†äº†ï¼Œæ”¹æˆ30çš„è¯ç•¥è‡ªç„¶ï¼Œå·®åˆ«ä¸å¤ªæ˜æ˜¾</span></span><br><span class="line">)</span><br><span class="line"> renderer_p3d = MeshRenderer(</span><br><span class="line">    rasterizer=MeshRasterizer(),</span><br><span class="line">    shader=HardPhongShader(</span><br><span class="line">        materials=materials,</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†">5. æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªï¼Œæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†ï¼Ÿ</h3><p>è¿˜æ˜¯ä¸€åªæ‰‹çš„æ¨¡å‹ï¼Œrender å‡ºæ¥å±…ç„¶åªæœ‰åŠä¸ªæ‰‹èƒŒï¼Œè·ç¦»ç›¸æœºæ›´è¿œçš„éƒ¨åˆ†åƒæ˜¯è¢«æˆªæ–­äº†ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/imzqovVyWfnK3Fs.png" alt="rand_1_skin_rendered_half" /><figcaption>rand_1_skin_rendered_half</figcaption></figure><p>æ”¹è¿›åï¼Œæ­£å¸¸çš„æ•ˆæœåº”è¯¥æ˜¯è¿™æ ·æ‰å¯¹ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/gSJ5wm74RUjHODY.png" alt="rand_1_skin_rendered_full" /><figcaption>rand_1_skin_rendered_full</figcaption></figure><p>æ‰€ä»¥é—®é¢˜å‡ºåœ¨å“ªå‘¢ï¼Ÿçš„ç¡®æ˜¯â€œæ›´è¿œçš„éƒ¨åˆ†è¢«æˆªæ‰äº†â€ã€‚æˆ‘æ‰¾åˆ°äº†<code>RasterizationSettings</code>é‡Œæœ‰è¿™ä¹ˆä¸€ä¸ªç›¸å…³çš„å‚æ•°ï¼š</p><ul><li>z_clip_value: if not None, then triangles will be clipped (and possibly subdivided into smaller triangles) such that z &gt;= z_clip_value. This avoids camera projections that go to infinity as z-&gt;0. Default is None as clipping affects rasterization speed and should only be turned on if explicitly needed. See clip.py for all the extra computation that is required.</li></ul><p>å¯æ˜¯é—®é¢˜ä¸åœ¨è¿™ä¸ªå‚æ•°ä¸Šï¼Œå› ä¸ºå®ƒçš„é»˜è®¤å€¼å°±æ˜¯Noneï¼Œåº”è¯¥åœ¨åç»­éƒ½æ²¡æœ‰å½±å“ã€‚</p><h4 id="è§£å†³-3">è§£å†³ï¼š</h4><p>ç»è¿‡ä»”ç»†çœ‹æºç ï¼Œæˆ‘å‘ç°é—®é¢˜å‡ºåœ¨<code>SoftPhongShader</code>â€¦â€¦å…·ä½“æ¥è¯´ï¼Œåœ¨<code>shader.py</code> ç¬¬138-139è¡Œï¼Œ<code>SoftPhongShader</code>çš„<code>forward</code>å‡½æ•°é‡Œï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">znear = kwargs.get(<span class="string">&quot;znear&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;znear&quot;</span>, <span class="number">1.0</span>))</span><br><span class="line">zfar = kwargs.get(<span class="string">&quot;zfar&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;zfar&quot;</span>, <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p>å±…ç„¶æœ‰ä¸€ä¸ªé»˜è®¤çš„zèŒƒå›´[1,100]â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦æ‰€ä»¥å…¶å®æ˜¯æˆ‘çš„meshçš„scaleå¤ªå¤§äº†ï¼Œå†åŠ ä¸Šç›¸æœºçš„distæ¯”è¾ƒå¤§ï¼Œæ•´ä¸ªæ·±åº¦å°±è¶…è¿‡zfaräº†ã€‚æ‰€ä»¥æœ‰ä¸¤ç§æ–¹æ³•ï¼Œè¦ä¹ˆç¼©å°ä¸€ä¸‹meshçš„å°ºåº¦ï¼›è¦ä¹ˆä¸æƒ³æ”¹å˜åŸæ•°æ®çš„è¯ï¼Œåœ¨renderçš„æ—¶å€™ï¼ŒæŠŠ<code>znear</code> <code>zfar</code>å‚æ•°é¢å¤–ä¼ å…¥ï¼Œå¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">images = renderer(mesh, ..., znear=-<span class="number">2.0</span>, zfar=<span class="number">1000.0</span>)</span><br></pre></td></tr></table></figure><h3 id="æ²¡è§£å†³çš„é—®é¢˜pbrphysical-based-rendering">6. æ²¡è§£å†³çš„é—®é¢˜ï¼šPBRï¼ˆphysical based renderingï¼‰</h3><p>æˆ‘çš„æ•°æ®ä¸­3D meshçš„æè´¨ç”¨äº†PBRï¼ˆphysical based renderingï¼‰ã€‚å®ƒæä¾›ä¸‰å¼ è´´å›¾å›¾åƒï¼šdiffuse mapï¼Œspecular mapå’Œnormal mapã€‚</p><p>ä½†æ˜¯pytorch3dç›®å‰å¹¶ä¸æ”¯æŒPBR inspired shadingï¼ˆsee <a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>ï¼‰ã€‚</p><p>æ‰€ä»¥ç›®å‰æˆ‘åªèƒ½æŠŠdiffuse mapä½œä¸ºä¸€èˆ¬æ„ä¹‰ä¸Šçš„texture mapï¼Œè€Œå¿½ç•¥äº†specular mapå’Œnormal mapè¿™ä¸¤å¼ å›¾ã€‚</p><p>æˆ‘ä¸ç¡®å®šèƒ½ä¸èƒ½è‡ªå·±å®ç°è¿™éƒ¨åˆ†åŠŸèƒ½ï¼Œæ¯”å¦‚è‡ªå®šä¹‰ <code>phong_shading</code>å‡½æ•°ï¼ˆå‚è€ƒ<a href="https://github.com/facebookresearch/pytorch3d/issues/865">issue</a>ï¼‰ã€‚ä½†è¿™æœ‰ç‚¹è¶…å‡ºæˆ‘çš„èƒ½åŠ›èŒƒå›´å’Œç²¾åŠ›èŒƒå›´ï¼Œæ‰€ä»¥æš‚æ—¶æç½®äº†ã€‚å¦‚æœèƒ½å®ç°çš„è¯ï¼ŒPyTorch3D ä¼¼ä¹æ˜¯æ¬¢è¿contributionçš„ï¼ˆ<a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>ï¼‰</p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> 3D Computer Vision </tag>
            
            <tag> Pytorch3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Camera projection with the pinhole model</title>
      <link href="/blog/Camera-Model-Notes/"/>
      <url>/blog/Camera-Model-Notes/</url>
      
        <content type="html"><![CDATA[<p>A camera is a mapping between the 3D world (object space) and a 2D image.</p><p>In general, the camera projection matrix P has 11 degrees of freedom: <span class="math display">\[P=K[R\ \ \ t]\]</span></p><table><thead><tr class="header"><th>Component</th><th># DOF</th><th>Elements</th><th>Known As</th></tr></thead><tbody><tr class="odd"><td>K</td><td>5</td><td><span class="math inline">\(f_x, f_y, s,p_x, p_y\)</span></td><td>Intrinsic Parameters; camera calibration matrix</td></tr><tr class="even"><td>R</td><td>3</td><td><span class="math inline">\(\alpha,\beta,\gamma\)</span></td><td>Extrinsic Parameters</td></tr><tr class="odd"><td>t (or <span class="math inline">\(\tilde{C}\)</span>)</td><td>3</td><td><span class="math inline">\((t_x,t_y,t_z)\)</span></td><td>Extrinsic Parameters</td></tr></tbody></table><p>3D world frame ----- R, t ----&gt; 3D camera frame ------ K -----&gt; 2D image</p><p>Explanation:</p><ul><li><p>P: Projective camera, maps 3D world points to 2D image points.</p></li><li><p>K: Camera calibration matrix, 3 x 3, <span class="math inline">\(x=K[I|0]X_{cam}\)</span>, given 3D points in camera coordinate frame <span class="math inline">\(X_{cam}\)</span>, we can project it into 2D points on image <span class="math inline">\(x\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/FEe9lM5t3TJKgyL.png" alt="K" style="zoom:50%;" /></p></li><li><p>R and t: Camera Rotation and Translation, rigid transformation. <span class="math inline">\(X_{cam}=( X,Y,Z,1)^T\)</span> is expressed in the camera coordinate frame. In general, 3D points are expressed in a different Euclidean coordinate frame, known as the <strong>world coordinate frame</strong>. The two frames are related via a rigid transformation (R, t).</p></li></ul><h3 id="some-other-terms-you-may-see">Some other terms you may see</h3><ul><li><p><strong>P</strong>: 3x4, homogeneous, camera projection matrix, <span class="math inline">\(P=diag(f,f,1)[I|0]\)</span>. P is K without considering <span class="math inline">\((x_{cam},y_{cam})\)</span> in the image. (In other words, it simplify <span class="math inline">\((p_x, p_y)=(0,0)\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/TlcW7QFhOAXK6sY.png" alt="P" style="zoom:50%;" /></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Configure Academic Page in Jekyll + Blog in Hexo together</title>
      <link href="/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/"/>
      <url>/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/</url>
      
        <content type="html"><![CDATA[<h2 id="difficult-situations">Difficult situations:</h2><ol type="1"><li>The Academic page is powered by <a href="http://jekyllrb.com/">Jekyll</a>, while the blog website is powered by Hexo.</li><li>And they are maintained in two separated repositories on Github.</li><li>Besides <code>[username].github.io</code>, I have a domain <code>jyzhu.top</code>, and want to use my custom domain.</li><li>All in all, I hope to visit the academic page is at <a href="jyzhu.top" class="uri">jyzhu.top</a>, while visit the blog is at <a href="jyzhu.top/blog" class="uri">jyzhu.top/blog</a>.</li></ol><h2 id="now-lets-configure.">Now let's configure.</h2><ol type="1"><li><p>Rename the blog repo as <code>blog</code>; rename the academic page repo as <code>[username].github.io</code>.</p></li><li><p>Edit the blog's Hexo config file:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="string">https://jyzhu.top/blog</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/blog/</span></span><br></pre></td></tr></table></figure><p><em>While no need to move all the files into a subfolder <code>blog</code> of your repo.</em></p><p>The Jekyll config is simple. Nothing needs to specify.</p></li><li><p>Edit the Github repo settings. Set the academic repo's <strong>custom domain</strong> as <code>jyzhu.top</code>. A <code>CNAME</code> file will be automatically added in the root. Now obviously, the <code>jyzhu.top</code> successfully refers to the academic page.</p><p>Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of <code>[username].github.io</code> by Github. Then coz <code>[username].github.io</code> is mapped to <code>[url]</code>, everything will be there, including <code>[url]/blog</code> for the <code>blog</code> repo.</p></li></ol><h2 id="todo">TODO</h2><p>The <code>hexo-douban</code> plugin cannot render styles now. Need to fix.</p><p><em>Update</em>:</p><ol type="1"><li>updated hexo-douban to the latest version</li><li>edit the file path of <code>loading.gif</code> in the <code>index.js</code> of this plugin</li></ol><p>then it seems ok now.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
            <tag> Hexo </tag>
            
            <tag> Jekyll </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>é½æ¬¡åæ ‡ç³»</title>
      <link href="/blog/homogeneous-coordinates/"/>
      <url>/blog/homogeneous-coordinates/</url>
      
        <content type="html"><![CDATA[<h1 id="é½æ¬¡åæ ‡ç³»">é½æ¬¡åæ ‡ç³»</h1><p>ä¹‹å‰ä¸ç†è§£ä¸ºä»€ä¹ˆè¦ç”¨ä¸€ä¸ªå’Œä»å°åˆ°å¤§å­¦çš„ç¬›å¡å°”åæ ‡ç³»ä¸åŒçš„é½æ¬¡åæ ‡ç³»æ¥è¡¨ç¤ºä¸œè¥¿ï¼Œå¹¶ä¸”å¼„å¾—å¾ˆå¤æ‚ï¼›å­¦äº†å„ç§å…¬å¼ä¹Ÿå¾ˆç³Šæ¶‚ã€‚ç°åœ¨ç»ˆäºæ˜ç™½äº†</p><h2 id="é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰">é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰</h2><p>å°±æ˜¯ç”¨æ¥è¡¨ç¤ºç°å®ä¸–ç•Œä¸­æˆ‘ä»¬çœ¼ç›çœ‹åˆ°çš„æ ·å­ï¼šä¸¤æ¡å¹³è¡Œçº¿åœ¨æ— é™è¿œå¤„èƒ½ç›¸äº¤ã€‚ <embed src="https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp" /></p><h2 id="é½æ¬¡åæ ‡ç³»çš„æœ¬è´¨">é½æ¬¡åæ ‡ç³»çš„æœ¬è´¨ï¼š</h2><p>å°±æ˜¯ç”¨N+1ç»´æ¥ä»£è¡¨Nç»´åæ ‡ã€‚</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸæœ¬äºŒç»´ç©ºé—´çš„ç‚¹<span class="math inline">\((X,Y)\)</span>ï¼Œå¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œç”¨<span class="math inline">\((x,y,w)\)</span>æ¥è¡¨ç¤ºã€‚æŠŠé½æ¬¡åæ ‡è½¬æ¢æˆç¬›å¡å°”åæ ‡æ˜¯å¾ˆç®€å•çš„ï¼Œå¯¹å‰ä¸¤ä¸ªç»´åº¦åˆ†åˆ«é™¤ä»¥æœ€åä¸€ä¸ªç»´åº¦çš„å€¼ï¼Œå°±å¥½äº†ï¼Œå³ <span class="math display">\[X=\frac x w,\  Y=\frac y w\\ (X,Y)=(\frac x w,\frac y w)\]</span> <embed src="https://pic2.zhimg.com/80/v2-da28eed57fda0fc6a06b7122be5f2a1d_1440w.webp" /></p><p>è¿™æ ·åšå°±å¯ä»¥è¡¨ç¤ºä¸¤æ¡å¹³è¡Œçº¿åœ¨è¿œå¤„èƒ½ç›¸äº¤äº†ï¼whyï¼Ÿ</p><p>è¦è§£é‡Šè¿™ä¸ªï¼Œéœ€è¦å…ˆè§£é‡Šä¸€ä¸ªé½æ¬¡åæ ‡ç³»çš„ç‰¹ç‚¹ï¼šè§„æ¨¡ä¸å˜æ€§ï¼ˆä¹Ÿæ˜¯å«homogeneousè¿™ä¸ªåå­—çš„åŸå› ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹ä»»æ„éé›¶çš„kï¼Œ<span class="math inline">\((x,y,w)\)</span>å’Œ<span class="math inline">\((kx,ky,kw)\)</span>éƒ½è¡¨ç¤ºäºŒç»´ç©ºé—´ä¸­åŒä¸€ä¸ªç‚¹<span class="math inline">\((\frac x w,\frac y w)\)</span>ã€‚ï¼ˆå› ä¸º<span class="math inline">\(\frac{kx}{kw}=\frac xw\)</span>å˜›ã€‚ï¼‰</p><p>é¦–å…ˆï¼Œç”¨åŸæœ¬ç¬›å¡å°”åæ ‡ç³»ä¸­çš„è¡¨ç¤ºæ–¹æ³•ï¼Œæ— é™è¿œå¤„çš„ç‚¹ä¼šè¢«è¡¨ç¤ºæˆ<span class="math inline">\((\infty,\infty)\)</span>ï¼Œä»è€Œå¤±å»æ„ä¹‰ã€‚ä½†æ˜¯æˆ‘ä»¬å‘ç°ç”¨é½æ¬¡åæ ‡ï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªæ–¹æ³•æ˜ç¡®è¡¨ç¤ºæ— é™è¿œå¤„çš„ä»»æ„ç‚¹ï¼Œå³ï¼Œ<span class="math inline">\((x,y,0)\)</span>ã€‚ï¼ˆä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæŠŠå®ƒè½¬æ¢å›ç¬›å¡å°”åæ ‡ï¼Œä¼šå¾—åˆ°<span class="math inline">\((\frac x 0,\frac y 0)=(\infty,\infty)\)</span>ï¼‰ã€‚</p><p>ç°åœ¨ï¼Œç”¨åˆä¸­æ‰€å­¦ï¼Œè”ç«‹ä¸¤æ¡ç›´çº¿çš„æ–¹ç¨‹ï¼Œå¾—åˆ°çš„è§£æ˜¯ä¸¤æ¡ç›´çº¿çš„äº¤ç‚¹ã€‚å‡å¦‚æœ‰ä¸¤æ¡å¹³è¡Œçº¿<span class="math inline">\(Ax+By+C=0\)</span>å’Œ<span class="math inline">\(Ax+By+D=0\)</span>ï¼Œæ±‚äº¤ç‚¹ï¼Œåˆ™ <span class="math display">\[\left\{\matrix{Ax+By+C=0 \\Ax+By+D=0}\right.\]</span> åœ¨ç¬›å¡å°”åæ ‡ç³»ä¸­ï¼Œå¯çŸ¥å”¯ä¸€è§£æ˜¯<span class="math inline">\(C=D\)</span>ï¼Œå³ä¸¤æ¡çº¿ä¸ºåŒä¸€æ¡ç›´çº¿ã€‚</p><p>ä½†æ˜¯ï¼Œå¦‚æœæŠŠå®ƒæ¢æˆé½æ¬¡åæ ‡ï¼Œå¾—åˆ° <span class="math display">\[\left\{\matrix{A\frac x w+B\frac y w+C=0\\ A\frac x w + B\frac y w +D=0}\right.\]</span></p><p><span class="math display">\[\left\{\matrix{Ax+By+Cw=0\\Ax+By+Dw=0}\right.\]</span></p><p>å½“<span class="math inline">\(w=0\)</span>ï¼Œä¸Šå¼å˜æˆ<span class="math inline">\(Ax+By=0\)</span>ï¼Œå¾—åˆ°è§£<span class="math inline">\((x,-\frac {A}Bx,0)\)</span>ã€‚å…¶å®è¿™é‡Œçš„xå’Œyæ˜¯ä»€ä¹ˆä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯w=0ï¼Œæ„å‘³ç€è¿™æ˜¯ä¸ªæ— é™è¿œå¤„çš„ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸¤æ¡å¹³è¡Œçº¿åœ¨æ— é™è¿œå¤„ç›¸äº¤äº†ï¼ç”šè‡³èƒ½æ˜ç¡®æ±‚å‡ºäº¤ç‚¹ï¼</p><blockquote><p>Reference:</p><p>http://www.songho.ca/math/homogeneous/homogeneous.html</p><p>https://zhuanlan.zhihu.com/p/373969867</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> 3D Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to NeRF</title>
      <link href="/blog/Introduction-to-NeRF/"/>
      <url>/blog/Introduction-to-NeRF/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h1 id="introduction-to-nerf">1. Introduction to NeRF</h1><h2 id="what-is-nerf">What is NeRF</h2><blockquote><p>Reference: Original NeRF paper; an online ariticle</p></blockquote><p>åœ¨å·²çŸ¥è§†è§’ä¸‹å¯¹åœºæ™¯è¿›è¡Œä¸€ç³»åˆ—çš„æ•è· (åŒ…æ‹¬æ‹æ‘„åˆ°çš„å›¾åƒï¼Œä»¥åŠæ¯å¼ å›¾åƒå¯¹åº”çš„å†…å¤–å‚)ï¼Œåˆæˆæ–°è§†è§’ä¸‹çš„å›¾åƒã€‚</p><p>NeRF æƒ³åšè¿™æ ·ä¸€ä»¶äº‹ï¼Œä¸éœ€è¦ä¸­é—´ä¸‰ç»´é‡å»ºçš„è¿‡ç¨‹ï¼Œä»…æ ¹æ®ä½å§¿å†…å‚å’Œå›¾åƒï¼Œç›´æ¥åˆæˆæ–°è§†è§’ä¸‹çš„å›¾åƒã€‚ä¸ºæ­¤ NeRF å¼•å…¥äº†è¾å°„åœºçš„æ¦‚å¿µï¼Œè¿™åœ¨å›¾å½¢å­¦ä¸­æ˜¯éå¸¸é‡è¦çš„æ¦‚å¿µï¼Œåœ¨æ­¤æˆ‘ä»¬ç»™å‡ºæ¸²æŸ“æ–¹ç¨‹çš„å®šä¹‰ï¼š</p><p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p><p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p><p>é‚£ä¹ˆè¾å°„å’Œé¢œè‰²æ˜¯ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿç®€å•è®²å°±æ˜¯ï¼Œå…‰å°±æ˜¯ç”µç£è¾å°„ï¼Œæˆ–è€…è¯´æ˜¯æŒ¯è¡çš„ç”µç£åœºï¼Œå…‰åˆæœ‰æ³¢é•¿å’Œé¢‘ç‡ï¼Œ<span class="math inline">\(æ³¢é•¿\times é¢‘ç‡=å…‰é€Ÿ\)</span>ï¼Œå…‰çš„é¢œè‰²æ˜¯ç”±é¢‘ç‡å†³å®šçš„ï¼Œå¤§å¤šæ•°å…‰æ˜¯ä¸å¯è§çš„ï¼Œäººçœ¼å¯è§çš„å…‰è°±ç§°ä¸ºå¯è§å…‰è°±ï¼Œå¯¹åº”çš„é¢‘ç‡å°±æ˜¯æˆ‘ä»¬è®¤ä¸ºçš„é¢œè‰²ï¼š</p><p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p><p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p><h3 id="implementation">Implementation</h3><h4 id="mlp-structure">MLP Structure</h4><ol type="1"><li>The net is constrained to be multi-view consistent by restricting the predicting of <span class="math inline">\(\sigma\)</span> to be independent of viewing direction</li><li>While the color <span class="math inline">\(\bold c\)</span> depends on both viewing direction and in-scene coordinate.</li></ol><p>How is this implemented?</p><p>The MLP is designed to be two-stages:</p><ol type="1"><li><span class="math inline">\(F_{\theta_1}(\bold x) = (\sigma, \text{&lt;256 dim features&gt;})\)</span></li><li><span class="math inline">\(F_{\theta_2}(\text{&lt;256 dim features&gt;}, \bold d)=\bold c\)</span></li></ol><h4 id="novel-view-synthesis">Novel view synthesis</h4><p>For each pixel, sample points along the camera ray through this pixel;</p><p>For each sampling point, compute local color and density;</p><p>Use volume rendering, an integral along the camera ray through pixels is used: <span class="math display">\[C(\bold r)=\int_{t_1}^{t_2} T(t)\cdot \sigma (\bold r(t))\cdot \bold c(\bold r(t),\bold d)\cdot dt \\T(t)=\exp (-\int_{t_1}^t \sigma(\bold r(u))\cdot du)\]</span> We can get the color C of the pixel.</p><p>This can be implemented by sampling approaches.</p><p>Now everything can be approximated: <span class="math display">\[\hat C(\bold r)=\sum_{i=1}^N \alpha_iT_i\bold c_i \\T_i=\exp (-\sum_{j=1}^{i-1}\sigma_i\delta_j) \\\alpha_i=1-\exp(\sigma_i\delta_i)\\\delta_i=\text{distance between sampling point i and i+1}\]</span></p><ul><li>Loss is just L2 on color of the pixels:</li></ul><p><span class="math display">\[L=\sum_{r\in R}\| \hat C(\bold r)-C_{gt}(\bold r)\|^2_2\]</span></p><h4 id="depth-regularization">Depth regularization</h4><p>Similar to the above formulas, expected depth can also be calculated, and can be used to regularize the depth smoothness.</p><h4 id="positional-encoding">Positional encoding</h4><p>It is required to greatly improve the fine detail results.</p><p>There are many other positional encoding techs, including trainable parametric, integral, and hierarchical variants</p><h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3><p>SDFæ˜¯ä¸€ç§è®¡ç®—å›¾å½¢å­¦ä¸­å®šä¹‰è·ç¦»çš„å‡½æ•°ã€‚SDFå®šä¹‰äº†ç©ºé—´ä¸­çš„ç‚¹åˆ°éšå¼æ›²é¢çš„è·ç¦»ï¼Œè¯¥ç‚¹åœ¨æ›²é¢å†…å¤–å†³å®šäº†å…¶SDFçš„æ­£è´Ÿæ€§ã€‚</p><p>ç›¸è¾ƒäºå…¶ä»–åƒç‚¹äº‘ï¼ˆpoint cloudï¼‰ã€ä½“ç´ ï¼ˆvoxelï¼‰ã€é¢äº‘ï¼ˆmeshï¼‰é‚£æ ·çš„ç»å…¸3Dæ¨¡å‹è¡¨ç¤ºæ–¹æ³•ï¼ŒSDFæœ‰å›ºå®šçš„æ•°å­¦æ–¹ç¨‹ï¼Œæ›´å…³æ³¨ç‰©ä½“çš„è¡¨é¢ä¿¡æ¯ï¼Œå…·æœ‰å¯æ§çš„è®¡ç®—æˆæœ¬ã€‚</p><h2 id="features-of-nerf">Features of NeRF</h2><ul><li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.<ul><li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li></ul></li><li>Neural Field has advantages:<ol type="1"><li>Compactness ç´§è‡´:</li><li>Regularization: nn itself as inductive bias makes it easy to learn</li><li>Domain Agonostic: cheap to add a dimension</li></ol></li><li>also problems<ul><li>Editability / Manipulability</li><li>Computational Complexity</li><li>Spectral Bias</li></ul></li></ul><h2 id="problem-formulation">Problem Formulation</h2><ul><li>Input: multiview images</li><li>Output: 3D Geometry and appearance</li><li>Objective:</li></ul><p><span class="math display">\[\arg \min_x\|y-F(x)\|+\lambda P(x)\]</span></p><p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p><p>F can be differentiable, then you can supervise this.</p><ul><li>nnæœ¬èº«å°±æ˜¯æŸç§constraintsï¼Œä½ å°±ä¸éœ€è¦åŠ å¤ªå¤šhandicraft constraints</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning NeRF</title>
      <link href="/blog/Learning-NeRF/"/>
      <url>/blog/Learning-NeRF/</url>
      
        <content type="html"><![CDATA[<h1 id="learning-nerf">Learning NeRF</h1><p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="reading-list">Reading List</h2><h3 id="classical">Classical</h3><ul><li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p><p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li><li><p><a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li></ul><h3 id="survey">Survey</h3><ul><li><a href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li></ul><h3 id="cvpr">2021CVPR</h3><p>2021å¹´CVPRè¿˜æœ‰è®¸å¤šç›¸å…³çš„ç²¾å½©å·¥ä½œå‘è¡¨ã€‚ä¾‹å¦‚ï¼Œæå‡ç½‘ç»œçš„æ³›åŒ–æ€§ï¼š</p><ul><li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>ï¼šå°†æ¯ä¸ªåƒç´ çš„ç‰¹å¾å‘é‡è€Œéåƒç´ æœ¬èº«ä½œä¸ºè¾“å…¥ï¼Œå…è®¸ç½‘ç»œåœ¨ä¸åŒåœºæ™¯çš„å¤šè§†å›¾å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ åœºæ™¯å…ˆéªŒï¼Œç„¶åæµ‹è¯•æ—¶ç›´æ¥æ¥æ”¶ä¸€ä¸ªæˆ–å‡ ä¸ªè§†å›¾ä¸ºè¾“å…¥åˆæˆæ–°è§†å›¾ã€‚</li><li><a href="https://ibrnet.github.io/">IBRNet</a>ï¼šå­¦ä¹ ä¸€ä¸ªé€‚ç”¨äºå¤šç§åœºæ™¯çš„é€šç”¨è§†å›¾æ’å€¼å‡½æ•°ï¼Œä»è€Œä¸ç”¨ä¸ºæ¯ä¸ªæ–°çš„åœºæ™¯éƒ½æ–°å­¦ä¹ ä¸€ä¸ªæ¨¡å‹æ‰èƒ½æ¸²æŸ“ï¼›ä¸”ç½‘ç»œç»“æ„ä¸Šç”¨äº†å¦ä¸€ä¸ªæ—¶é«¦çš„ä¸œè¥¿ Transformerã€‚</li><li><a href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>ï¼šè®­ç»ƒä¸€ä¸ªå…·æœ‰æ³›åŒ–æ€§èƒ½çš„å…ˆéªŒç½‘ç»œï¼Œåœ¨æ¨ç†çš„æ—¶å€™åªç”¨3å¼ è¾“å…¥å›¾ç‰‡å°±é‡å»ºä¸€ä¸ªæ–°çš„åœºæ™¯ã€‚</li></ul><p>é’ˆå¯¹åŠ¨æ€åœºæ™¯çš„NeRF:</p><ul><li><a href="https://nerfies.github.io/">Nerfies</a>ï¼šå¤šä½¿ç”¨äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¥æ‹Ÿåˆå½¢å˜çš„SE(3) fieldï¼Œä»è€Œå»ºæ¨¡å¸§é—´åœºæ™¯å½¢å˜ã€‚Nerfies: Deformable Neural Radiance Fields</li><li><a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>ï¼šå¤šä½¿ç”¨äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¥æ‹Ÿåˆåœºæ™¯å½¢å˜çš„displacementã€‚</li><li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>ï¼šå¤šæå‡ºäº†ä¸€ä¸ªscene flow fieldsæ¥æè¿°æ—¶åºçš„åœºæ™¯å½¢å˜ã€‚</li></ul><p>å…¶ä»–åˆ›æ–°ç‚¹ï¼š</p><ul><li><a href="https://kai-46.github.io/PhySG-website/">PhySG</a>ï¼šç”¨çƒçŠ¶é«˜æ–¯å‡½æ•°æ¨¡æ‹ŸBRDFï¼ˆé«˜çº§ç€è‰²çš„ä¸Šå¤ç¥å™¨ï¼‰å’Œç¯å¢ƒå…‰ç…§ï¼Œé’ˆå¯¹æ›´å¤æ‚çš„å…‰ç…§ç¯å¢ƒï¼Œèƒ½å¤„ç†éæœ—ä¼¯è¡¨é¢çš„åå°„ã€‚</li><li><a href="https://nex-mpi.github.io/">NeX</a>ï¼šç”¨MPIï¼ˆMulti-Plane Image ï¼‰ä»£æ›¿NeRFçš„RGBÏƒä½œä¸ºç½‘ç»œçš„è¾“å‡ºã€‚</li></ul><h3 id="cvpr-1">2022 CVPR</h3><p><a href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p><h2 id="useful-references"><strong>Useful References:</strong></h2><blockquote><p><a href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p><p><a href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p><p><a href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p><p><a href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p></blockquote><p>Bigger to learn:</p><ul><li>[ ] Above NeRF: neural rendering</li><li>[ ] Related theories in graphics and computer vision</li><li>[ ] NeRFçš„ä¸€ä½œBen Mildenhallåœ¨SIGGRAPH 2021 Course <a href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>ä¸­ä»æ¦‚ç‡çš„è§’åº¦æ¨å¯¼äº†NeRFçš„ä½“æ¸²æŸ“å…¬å¼ã€‚</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Manipulate Neural Fields</title>
      <link href="/blog/Manipulate-Neural-Fields/"/>
      <url>/blog/Manipulate-Neural-Fields/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2><p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p><figure><img src="https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption></figure><p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p><p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p><figure><img src="https://s2.loli.net/2023/01/12/S7HWcQPh1FtdJaw.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption></figure><p>The following examples è½åœ¨ä¸åŒçš„è±¡é™ã€‚</p><h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3><ul><li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li><li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li><li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p><figure><img src="https://s2.loli.net/2023/01/12/y4bGulHpfOwWkqN.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption></figure><ul><li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p><figure><img src="https://s2.loli.net/2023/01/12/SBzGy3rnUaqLFI8.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption></figure></li></ul></li><li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p><figure><img src="https://s2.loli.net/2023/01/12/UbFu74iCQ15mK3B.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption></figure></li></ul><h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/zxFElDIuSnPioJ7.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption></figure><p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p><p>We need to define the consistency here, so that the network can learn through forward and backward:</p><figure><img src="https://s2.loli.net/2023/01/12/VS1K3rQxHXYPRIg.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption></figure><h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3><p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p><figure><img src="https://s2.loli.net/2023/01/12/w2XEyYn5qbh41OB.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption></figure><ul><li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li></ul><figure><img src="https://s2.loli.net/2023/01/12/ruER6MJPpljSZiX.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption></figure><ul><li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/HL61itcqsIaEThX.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/y7eCcKDmdUY4VOu.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption></figure><ul><li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/2lTvenQixfRm8Po.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption></figure><ul><li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li></ul><h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/5Ohb7ExW4osc1n2.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption></figure><ul><li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li><li>Beyond geometry, we can also manipulate <strong>color</strong></li></ul><figure><img src="https://s2.loli.net/2023/01/12/vM1QwkT4BJqGNIR.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption></figure><p>It decomposes the network into shape and color networks, and we can edit each independently.</p><figure><img src="https://s2.loli.net/2023/01/12/38HNsE9GnF1pydQ.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption></figure><ul><li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF Differentiable Forward Maps</title>
      <link href="/blog/NeRF-Differentiable-Forward-Maps/"/>
      <url>/blog/NeRF-Differentiable-Forward-Maps/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2><figure><img src="https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption></figure><h3 id="differentiable-rendering">Differentiable rendering</h3><figure><img src="https://s2.loli.net/2023/01/12/1Ng8wz2KP4oTiVH.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption></figure><p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p><h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3><ul><li>BRDF Shading? details later.</li></ul><h3 id="differentiation-itself">Differentiation itself</h3><p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p><figure><img src="https://s2.loli.net/2023/01/12/Gi6IaAkhvlBxpoe.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption></figure><p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p><p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p><h3 id="special-identity-operator">Special: Identity Operator</h3><p><span class="math display">\[\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\\text{Reconstruction} == \text{Sensor domain}\]</span></p><p>Q&amp;A:</p><ul><li>Can we obtain a neural network in just one forward, without optimization?</li><li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF Hybrid representations</title>
      <link href="/blog/NeRF-Hybrid-representations/"/>
      <url>/blog/NeRF-Hybrid-representations/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="hybrid-representations">2.2. Hybrid representations</h2><h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3><figure><img src="https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/sDg8FHQcjrGRW1b.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption></figure><p>You may choose one proper representation depending on your own application</p><h3 id="grid">1. Grid</h3><figure><img src="https://s2.loli.net/2023/01/12/ZpYEMbXvRdeOqiy.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption></figure><p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p><figure><img src="https://s2.loli.net/2023/01/12/lMi3tK9NPgcWUaI.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption></figure><p>NeRFusion CVPR22: online!</p><h3 id="point-cloud">2. point cloud</h3><figure><img src="https://s2.loli.net/2023/01/12/zhYHQnFsgxBLCfM.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption></figure><p>Cons:</p><ol type="1"><li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li><li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li></ol><p><img src="https://s2.loli.net/2023/01/12/37EbFsANOCc9voX.png" alt="image-20221208163050867" style="zoom:50%;" /></p><h3 id="mesh">3. Mesh</h3><p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p><figure><img src="https://s2.loli.net/2023/01/12/Dwir9hsm3VgZjQk.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/nPMw7T3hqRAU2iv.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption></figure><h3 id="multiplanar-images">4. Multiplanar Images</h3><p>Something like project a 3D grid into an axis to get levels of planes.</p><figure><img src="https://s2.loli.net/2023/01/12/CmhFDT5NoiMAOnv.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption></figure><p>Pros:</p><ol type="1"><li>Compact</li><li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li></ol><p>Cons:</p><ol type="1"><li>Resolution bias on plane axis: coz it is discrete betweens planes.</li></ol><p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p><p><img src="https://s2.loli.net/2023/01/12/UDO6HlWAp3y7qF1.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p><h3 id="multiresolution-grids">5. Multiresolution grids</h3><figure><img src="https://s2.loli.net/2023/01/12/TlmkK1NDj2dAtUp.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption></figure><p>Pros:</p><ol start="2" type="1"><li>Stable coz you indeed need both low and high resolution info</li></ol><h3 id="hash-grids">6. Hash grids</h3><p><img src="https://s2.loli.net/2023/01/12/NZtH7wfxpSPVkGe.png" alt="image-20221208170131069" /> <span class="math display">\[[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}\]</span> Pros:</p><ol type="1"><li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li><li>Can be online!</li></ol><p>Cons:</p><ol type="1"><li>May still need large codebooks</li><li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li></ol><h3 id="codebook-grids">7. Codebook grids</h3><figure><img src="https://s2.loli.net/2023/01/12/pnBJGxM6jNHD2v5.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption></figure><p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p><p>cons:</p><ol type="1"><li>To make the indexing operation differentiable, the computing complexity rises here.</li><li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li></ol><h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3><figure><img src="https://s2.loli.net/2023/01/12/LQ6fzh21OltTJxq.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption></figure><p>Commonly used method in computer graphics</p><h3 id="others-voxel">9. Others (voxel)</h3><figure><img src="https://s2.loli.net/2023/01/12/5sE1MYpOLB4mPCF.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption></figure><ul><li>For dynamic nerfs, is there any better hybrid representation? Sure.</li><li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF Network Architecture</title>
      <link href="/blog/NeRF-Network-Architecture/"/>
      <url>/blog/NeRF-Network-Architecture/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="network-architecture">2.1. Network Architecture</h2><h3 id="input-encoding">1. Input Encoding</h3><p>Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the å…‰çº¿çš„æ•£å°„</p><h3 id="activation-functions">2. Activation functions</h3><p>ReLU is not perfect for this task. Because it ä¸èƒ½è§£å†³å¯¹é«˜é˜¶å¯¼æœ‰constraintsçš„å‡½æ•°ã€‚</p><p>SIREN is a replacement.</p><h3 id="symmetry-invariance-equivariance">3. Symmetry, Invariance &amp; Equivariance</h3><figure><img src="https://s2.loli.net/2023/01/12/tynBhWCulMINrU3.png" alt="image-20221205193423558" /><figcaption>image-20221205193423558</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/Rjx5h2kfY34JZct.png" alt="image-20221205193614341" /><figcaption>image-20221205193614341</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF Notebook</title>
      <link href="/blog/NeRF-Notebook/"/>
      <url>/blog/NeRF-Notebook/</url>
      
        <content type="html"><![CDATA[<p>I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.</p><h1 id="contents">Contents</h1><p><a href="https://jyzhu.top/Learning-NeRF/">Learning NeRF</a>: Reading list, learning references, and plans</p><p><strong>Notes of CVPR22' Tutorial:</strong></p><p><a href="https://jyzhu.top/Introduction-to-NeRF/">1. Introduction to NeRF</a>: What is NeRF and its features</p><p>2. Techniques</p><p>â€‹ <a href="https://jyzhu.top/NeRF-Network-Architecture/">2.1. Network Architecture</a></p><p>â€‹ <a href="https://jyzhu.top/NeRF-Hybrid-representations/">2.2. Hybrid representations</a></p><p>â€‹ <a href="https://jyzhu.top/NeRF-Differentiable-Forward-Maps/">2.3. Differentiable Forward Maps</a></p><p>â€‹ <a href="https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/">2.4. Prior-based reconstruction of neural fields</a></p><p>â€‹ <a href="https://jyzhu.top/Manipulate-Neural-Fields/">2.5. Manipulate Neural Fields</a></p><p>3. Applications</p><p><em>TBC</em></p><p><strong>Notes of paper reading</strong>:</p><p><a href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/">Reading NeuMan: Neural Human Radiance Field from a Single Video</a></p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prior based reconstruction of neural fields</title>
      <link href="/blog/Prior-based-reconstruction-of-neural-fields/"/>
      <url>/blog/Prior-based-reconstruction-of-neural-fields/</url>
      
        <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="prior-based-reconstruction-of-neural-fields">2.4. Prior-based reconstruction of neural fields</h2><p>Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let's learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.</p><figure><img src="https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png" alt="image-20221211234430727" /><figcaption>image-20221211234430727</figcaption></figure><h3 id="how-does-the-latent-code-look-like">How does the latent code look like?</h3><figure><img src="https://s2.loli.net/2023/01/12/A41EOlYCBXrUxto.png" alt="image-20221211234923290" /><figcaption>image-20221211234923290</figcaption></figure><ul><li>Global: not local. A small latent code represents a neural field<ul><li>main limitation: can only represent very simple (single) object. coz if you have multiple objects in a scene, the degree of freedom grows non-linearly.</li><li><strong>How about giving the natural language descriptions as conditions???</strong></li></ul></li><li>Local: you get different latent codes considering the locality where you are. So, you have a prior 3D data structure to store the latent codes.<ul><li>3D point clouds -&gt; grids -&gt; triplanes interpolation</li></ul></li></ul><blockquote><p>Convolutional Occupancy Networks</p></blockquote><h3 id="autodecoder-instead-of-encoder-decoder">Autodecoder instead of Encoder-decoder</h3><figure><img src="https://s2.loli.net/2023/01/12/zr2GnpBXeZDJUqa.png" alt="image-20221212005012783" /><figcaption>image-20221212005012783</figcaption></figure><ul><li><p>Encoder is a 2D CNN structure.</p></li><li><p>But while using autodecoder, the backpropogate through the forward map (i.e., the neural renderer) will give the 3D structural information to the latent codes directly. <span class="math display">\[\text{latent code }\hat z=\arg \min_z \|\text{Render(}\Phi)-g.t.\|\]</span> <img src="https://s2.loli.net/2023/01/12/FQjMgibmNtIP4ca.png" alt="image-20221212004946040" /></p></li></ul><p><strong>Instead of trying to build the encoder, sometimes just use the backpropogation through the forward map is helpful.</strong></p><h3 id="light-field-networks----dont-need-to-render-anymore">Light field networks -- Don't need to render anymore</h3><figure><img src="https://s2.loli.net/2023/01/12/FVP15fmBYz8rEoe.png" alt="image-20221212005908926" /><figcaption>image-20221212005908926</figcaption></figure><p>Instead of learning a NeRF that you use a neural renderer to generate all points along a ray; you can learn a network to directly give you a color along a ray. So you do not use a 3d coordinate as the query, instead, use a ray.</p><p>But this do not work in complicated task yet.</p><figure><img src="https://s2.loli.net/2023/01/12/mAFx6pzqaIuUMin.png" alt="image-20221212010316991" /><figcaption>image-20221212010316991</figcaption></figure><h3 id="outlook">Outlook</h3><ul><li>You don't need to use 600 images of a scene to reconstruct it. Synthesis images?</li><li>Open minds: other ways to skip the expensive forward map? (e.g., the light field)</li><li>Understanding the scene like humans do: disentangle different objects</li><li>Local conditioning methods? Regular grids are easy to tackle with, but it's harder for point clouds / factorized representations</li><li>Transformers: seems like local conditioning</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The difference between RNN&#39;s output and h_n</title>
      <link href="/blog/The-difference-between-RNN-s-output-and-h-n/"/>
      <url>/blog/The-difference-between-RNN-s-output-and-h-n/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Reference: <a href="https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm" class="uri">https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm</a></p></blockquote><p>I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(s_{t-1}\)</span> as input, and outputs <span class="math inline">\(s_t\)</span>, i.e., <span class="math inline">\(s_t = RNN(y_{t-1}, s_{t-1})\)</span>.</p><p>But the pytorch implementation of RNN is: <span class="math inline">\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)</span>, which takes in a sequence of elements, computes in serials, and outputs a sequence also.</p><p>I was confused about what is the <span class="math inline">\(s_t\)</span>. Is it the <span class="math inline">\(outputs\)</span>, or the <span class="math inline">\(hidden\_states\)</span>?</p><p>This is the very helpful picture:</p><p><img src="https://i.stack.imgur.com/SjnTl.png" /></p><p>The <span class="math inline">\(output\)</span> here is the <span class="math inline">\(hidden\_states\)</span> of the last layer among all elements in the sequence (time steps), while the <span class="math inline">\(h_n,c_n = hidden\_last\)</span> is the <span class="math inline">\(hidden\_states\)</span> of the last time step among all layers.</p><p>The former is the <span class="math inline">\(H\)</span>, hidden state collection, which can be used in subsequent calculations, like attentions or scores; and the latter is the hidden state that can be directly used in the next iteration.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading 3D Photography using Context-aware Layered Depth Inpainting</title>
      <link href="/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/"/>
      <url>/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼š<a href="https://shihmengli.github.io/3D-Photo-Inpainting" class="uri">https://shihmengli.github.io/3D-Photo-Inpainting</a></p><p>ä½œè€…ï¼š<a href="https://shihmengli.github.io/">Meng-Li Shih</a>, <a href="https://lemonatsu.github.io/">Shih-Yang Su</a>, <a href="https://johanneskopf.de/">Johannes Kopf</a>, <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></p><p>å‘è¡¨ï¼š CVPR2020</p><p>é“¾æ¥ï¼š <a href="https://github.com/vt-vl-lab/3d-photo-inpainting" class="uri">https://github.com/vt-vl-lab/3d-photo-inpainting</a></p><hr /><h2 id="why">Whyï¼š</h2><p>ä¹‹å‰çš„ç…§ç‰‡3DåŒ–çš„æ–¹æ³•ï¼Œä¼šåœ¨è§†è§’å˜åŠ¨åå‡ºç°çš„æ–°åƒç´ åŒºåŸŸ å¡«å……å¾ˆæ¨¡ç³Šçš„èƒŒæ™¯ï¼›è¿™ä¸ªæ–¹æ³•ä¸»è¦æ˜¯ç”¨inpaintingçš„æ–¹æ³•æé«˜æ–°èƒŒæ™¯ç”Ÿæˆçš„æ•ˆæœ</p><figure><img src="https://s2.loli.net/2022/10/22/dfjuy9vNrBak1oP.png" alt="image-20221007084129382" /><figcaption>image-20221007084129382</figcaption></figure><h2 id="what">Whatï¼š</h2><ol type="1"><li>ä»»åŠ¡æ˜¯3D photographyï¼Œå›¾åƒ3DåŒ–ï¼ŒæŠŠä¸€å¼ 2D+æ·±åº¦ä¿¡æ¯çš„RGB-Då›¾åƒè½¬åŒ–æˆ3Dé£æ ¼çš„å›¾åƒã€‚</li><li>ç°åœ¨çš„å¤šé•œå¤´æ™ºèƒ½æ‰‹æœºæ‹çš„ç…§ç‰‡éƒ½èƒ½æä¾›æ·±åº¦ä¿¡æ¯ã€‚æ²¡æœ‰çš„è¯ï¼Œä¹Ÿèƒ½ç”¨å…¶ä»–æ¨¡å‹é¢„æµ‹æ·±åº¦ã€‚</li><li>ç”¨åˆ†å±‚æ·±åº¦å›¾åƒï¼ˆLayered Depth Imageï¼‰æ¥è¡¨ç¤ºå›¾åƒï¼šèƒ½æ˜¾å¼åœ°è¡¨ç¤ºåƒç´ ç‚¹ä¹‹é—´çš„è¿é€šæ€§ã€‚å’Œæ™®é€šçš„2Då›¾åƒç›¸æ¯”ï¼Œå¯ä»¥æŠŠåƒç´ ç‚¹åˆ†æˆå¤šå±‚æ¥è¡¨ç¤ºï¼ŒåŒä¸€ä¸ªåæ ‡å¤„å¯ä»¥æœ‰é‡åˆçš„ä¸åŒå±‚æ¬¡çš„åƒç´ ç‚¹ã€‚</li><li>æå‡ºä¸€ä¸ªåŸºäºå­¦ä¹ çš„ inpainting æ–¹æ³•å¡«å……é‡å åŒºåŸŸçš„åƒç´ ï¼Œè®©3Då›¾åƒè§†è§’å˜åŒ–çš„æ—¶å€™å‡ºç°çš„æ–°èƒŒæ™¯æ•ˆæœå¾ˆå¥½ã€‚</li></ol><h2 id="how">Howï¼š</h2><p>æ˜¯ä¸€ä¸ªå¾ˆæ¸…æ™°çš„æµç¨‹ï¼š</p><ol type="1"><li><p>è¾“å…¥ä¸ºå•å¼ RGB-Då›¾åƒã€‚Dä¸ºdepthï¼Œä¸€èˆ¬å¤šé•œå¤´æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡éƒ½èƒ½æä¾›æ·±åº¦ä¿¡æ¯ï¼›æ²¡æœ‰çš„è¯å°±ç”¨å…¶ä»–æ¨¡å‹é¢„æµ‹æ·±åº¦ï¼Œæ¯”å¦‚MegaDepth, MiDas, and Kinect depth sensor</p></li><li><p>å°†è¾“å…¥å›¾åƒè½¬åŒ–æˆåˆ†å±‚æ·±åº¦å›¾åƒï¼ˆLayered Depth Imageï¼‰ã€‚LDIä¸­çš„æ¯ä¸ªåƒç´ ç‚¹ä¿å­˜é¢œè‰²å’Œæ·±åº¦ä¿¡æ¯ï¼Œä»¥åŠä¸Šä¸‹å·¦å³å››ä¸ªæ–¹å‘çš„é‚»å±…åƒç´ ç‚¹ã€‚åŒä¸€ä¸ªåæ ‡å¤„å¯ä»¥æœ‰é‡åˆçš„ä¸åŒæ·±åº¦çš„åƒç´ ç‚¹ã€‚</p></li><li><p>å›¾åƒé¢„å¤„ç†ï¼šæ£€æµ‹æ·±åº¦ä¸è¿è´¯çš„è¾¹ç¼˜</p><figure><img src="https://s2.loli.net/2022/10/22/6tjCgUHTpVyIAFN.png" alt="image-20221007090910332" /><figcaption>image-20221007090910332</figcaption></figure><p>ç”¨filteræŠŠæ·±åº¦è¾¹ç¼˜è¿‡æ»¤å¾—æ›´é”åˆ©ï¼Œç„¶åæ¸…ç†ä¸€äº›ä¸è¿è´¯çš„è¾¹ç¼˜ï¼Œæœ€åæ ¹æ®è¿é€šæ€§åˆ’åˆ†ä¸åŒçš„æ·±åº¦è¾¹ï¼ˆå¦‚å›¾2 ï¼ˆfï¼‰ä¸­ï¼Œä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒæ·±åº¦è¾¹ï¼‰ã€‚</p></li><li><p>å¯¹äºæ¯ä¸€ä¸ªæ·±åº¦è¾¹ï¼ŒæŠŠLDIå›¾ä¸­çš„åƒç´ ç‚¹åˆ‡å‰²å¼€ï¼Œå¹¶åœ¨èƒŒæ™¯å±‚æ‰©å±•ä¸€äº›åƒç´ ç‚¹ï¼Œå¯¹æ‰©å±•åŒºåŸŸè¿›è¡Œç”Ÿæˆ</p><figure><img src="https://s2.loli.net/2022/10/22/Ap62PxdYEDKzBkJ.png" alt="image-20221007091217942" /><figcaption>image-20221007091217942</figcaption></figure><ol type="1"><li><p>æ‰¾åˆ°ä¸€ä¸ªæ·±åº¦è¾¹ï¼ŒæŠŠä¸¤å±‚çš„åƒç´ ç‚¹åˆ‡å‰²å¼€</p></li><li><p>å¯¹äºèƒŒæ™¯å±‚ï¼Œç”¨flood-fill likeç®—æ³•è¿­ä»£åœ°é€‰å–ä¸€å®šçš„å·²çŸ¥åŒºåŸŸä½œä¸ºcontext regionï¼Œä»¥åŠæ‰©å±•ä¸€å®šçš„æœªçŸ¥åŒºåŸŸä½œä¸ºsynthesis region</p></li><li><p>åˆ©ç”¨å·²çŸ¥context region ç”ŸæˆæœªçŸ¥synthesis region çš„æ·±åº¦å’Œé¢œè‰²ï¼šé‡‡ç”¨åŸºäºå­¦ä¹ çš„inpaintingæ–¹æ³•</p><figure><img src="https://s2.loli.net/2022/10/22/h8uGZm4XAEcL5SP.png" alt="image-20221007091716266" /><figcaption>image-20221007091716266</figcaption></figure><p>è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œæœ€å…³é”®çš„å°±æ˜¯åœ¨é¢„æµ‹colorå’Œdepthä¹‹å‰ï¼Œå…ˆé¢„æµ‹äº†ä¸€ä¸‹depth edgesï¼Œç„¶åæŠŠè¿™ä¸ªedgesä¿¡æ¯åŠ è¿›å»ï¼Œå¯ä»¥æ›´å¥½åœ°é¢„æµ‹colorå’Œdepthã€‚</p></li><li><p>å°†ç”Ÿæˆå®Œæ¯•çš„åƒç´ èåˆå›LDIå›¾åƒ</p></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> 3DCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos</title>
      <link href="/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/"/>
      <url>/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2112.13715</p><p>ä½œè€…ï¼š<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng%2C+A">Ailing Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ju%2C+X">Xuan Ju</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J">Jiefeng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+Q">Qiang Xu</a></p><p>å‘è¡¨ï¼š ECCV 2022</p><p>é“¾æ¥ï¼š <a href="https://github.com/cure-lab/SmoothNet" class="uri">https://github.com/cure-lab/SmoothNet</a></p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><h2 id="why">Whyï¼š</h2><ol type="1"><li>ä»è§†é¢‘ä¼°è®¡äººä½“å§¿åŠ¿æ—¶ï¼ŒæŠ–åŠ¨æ˜¯ä¸ªé—®é¢˜</li><li>é™¤äº†è½»å¾®æŠ–åŠ¨ä»¥å¤–ï¼Œæœ‰ä¸€äº›long- termæŠ–åŠ¨ï¼Œè¿˜æœ‰å› ä¸ºé‡å ã€å§¿åŠ¿å°‘è§ç­‰åŸå› é€ æˆçš„ä¼°æµ‹å›°éš¾</li></ol><h2 id="what">Whatï¼š</h2><figure><img src="https://s2.loli.net/2022/10/22/fRIB2t9hw8na6my.png" alt="image-20220909143919787" /><figcaption>image-20220909143919787</figcaption></figure><ol type="1"><li>ä¸€ä¸ªä»…åŸºäºæ—¶åºçš„ç²¾ç‚¼ç½‘ç»œï¼Œä»¥å…¶ä»–ç½‘ç»œçš„å§¿åŠ¿ä¼°è®¡ç»“æœä½œä¸ºè¾“å…¥ã€‚</li><li>æœ‰ç›‘ç£çš„</li><li>é‡‡ç”¨æ»‘åŠ¨çª—å£ï¼ŒåŸºäºTCN</li><li>å¹¶ä¸æ˜¯å¸¸è§çš„é‚£ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œå³é‡‡ç”¨æ—¶é—´-ç©ºé—´æ¨¡å‹æ¥åŒæ—¶ä¼˜åŒ–é€å¸§çš„å‡†ç¡®ç‡å’Œæ—¶åºçš„å¹³æ»‘æ€§ã€‚è¿™ä¸ªæ–¹æ³•é€šè¿‡å­¦ä¹ æ¯ä¸€ä¸ªå…³èŠ‚åœ¨é•¿æ—¶é—´èŒƒå›´çš„è¿åŠ¨ç‰¹å¾ï¼ˆè€Œä¸æ˜¯å…³èŠ‚ä¹‹é—´çš„å…³ç³»ï¼‰ï¼Œæ¥è‡ªç„¶åœ°å»ºæ¨¡èº«ä½“è¿åŠ¨ä¸­çš„å¹³æ»‘ç‰¹å¾ã€‚</li><li>ç”±äºå®ƒä»…ä»…éœ€è¦æ—¶åºä¿¡æ¯ï¼Œæ‰€ä»¥å¯ä»¥æ³›åŒ–åˆ°å¾ˆå¤šç§ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬2Då’Œ3Dçš„å§¿åŠ¿ä¼°è®¡ã€body recoveryç­‰</li></ol><h2 id="how">Howï¼š</h2><ol type="1"><li><p>æ ¹æ®æŒç»­æ—¶é•¿ï¼Œå°†æŠ–åŠ¨å½’ç±»ä¸ºsudden jitterå’Œlong- term jitterä¸¤ç§ã€‚ä¸ºäº†è§£å†³long- termçš„æŠ–åŠ¨é—®é¢˜ï¼Œç°æœ‰é‚£äº›æ–¹æ³•éƒ½ä¸å¤§è¡Œã€‚</p><p>æ ¹æ®ç¨‹åº¦ï¼Œåˆå¯ä»¥å°†æŠ–åŠ¨åˆ†ä¸ºå°æŠ–åŠ¨å’Œå¤§æŠ–åŠ¨ã€‚å°æŠ–åŠ¨ä¸€èˆ¬ç”±äºä¸å¯é¿å…çš„è¯¯å·®ï¼Œæˆ–è€…æ ‡æ³¨ä¸Šçš„è¯¯å·®ï¼›å¤§æŠ–åŠ¨åˆ™æ˜¯ç”±äºå›¾åƒè´¨é‡å·®ã€å§¿åŠ¿å°‘ã€é‡å ä¸¥é‡ç­‰ã€‚</p><figure><img src="https://s2.loli.net/2022/10/22/pNa7bcVqzi9lGZw.png" alt="image-20220909143902334" /><figcaption>image-20220909143902334</figcaption></figure></li><li><p>å°†è¯¯å·®å½’ç±»ä¸ºç›¸é‚»å¸§ä¹‹é—´çš„æŠ–åŠ¨é€ æˆçš„è¯¯å·®ï¼ˆjitter errorï¼‰å’Œæ¨¡å‹ä¼°è®¡ç»“æœä¸çœŸå®ç»“æœä¹‹é—´çš„åå·®ï¼ˆbias errorï¼‰è¿™ä¸¤ç§ã€‚ç°æœ‰é‚£äº›æ–¹æ³•å¹¶æ²¡æœ‰å°†è¿™ä¸¤ç±»è¯¯å·®è§£è€¦</p></li><li><p>æå‡ºäº†basic smoothnetå’Œæ­£ç»smoothnetã€‚</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/10/22/Zf6lpahbHrRG7kF.png" alt="image-20220909154626864" /><figcaption>image-20220909154626864</figcaption></figure><p>Basic smoothnetï¼ŒFCNæ˜¯backboneã€‚é€šè¿‡é•¿åº¦ä¸ºTçš„æ»‘çª—ï¼Œæ¯æ¬¡ä¼ å…¥Tå¸§å›¾åƒï¼ŒåŒ…å«Cä¸ªchannelsã€‚</p><figure><img src="https://s2.loli.net/2022/10/22/hRcX2MCx8SnHZfz.png" alt="image-20220909155504588" /><figcaption>image-20220909155504588</figcaption></figure><p>æƒé‡<span class="math inline">\(w_t^l\)</span>å’Œåå·®<span class="math inline">\(b^l\)</span>æ˜¯ç¬¬<span class="math inline">\(t_{th}\)</span>å¸§çš„ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„channelä¹‹é—´æ˜¯å…±äº«çš„ã€‚</p></li><li><figure><img src="https://s2.loli.net/2022/10/22/uCJgF8M6tG5S1PB.png" alt="image-20220909155701901" /><figcaption>image-20220909155701901</figcaption></figure><p>å®Œæ•´çš„motion- aware smoothnetå°±æ˜¯åŠ ä¸Šäº†é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ä¸¤ä¸ªæ¨¡å—ã€‚</p><p>å› ä¸ºjitterçš„ä¸€ä¸ªè¡¡é‡æ–¹å¼å°±æ˜¯åŠ é€Ÿåº¦ï¼Œæ‰€ä»¥æŠŠåŠ é€Ÿåº¦ç›´è§‚åœ°æ˜¾ç¤ºåœ¨æ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªå¾ˆæ˜¾ç„¶çš„æ–¹å¼ã€‚ç»™å®šé¢„æµ‹å‡ºçš„å§¿åŠ¿<span class="math inline">\(\hat Y\)</span>ï¼Œé€Ÿåº¦å°±æ˜¯ä¸¤å¸§ä¹‹é—´ç›¸å‡ï¼Œå¾—åˆ° <span class="math display">\[\hat V_{i,t} = \hat Y_{i,t} âˆ’ \hat Y_{i,tâˆ’1}\]</span> åŠ é€Ÿåº¦å°±æ˜¯é€Ÿåº¦ä¹‹é—´çš„å·®ï¼š <span class="math display">\[\hat A_{i,t} = \hat V_{i,t} âˆ’ \hat V_{i,tâˆ’1}\]</span></p></li></ol></li><li><p>losså°±æ˜¯ä¸¤ä¸ªï¼š</p><ol type="1"><li><p>ground truth poseå’Œä¼°è®¡poseä¹‹é—´çš„è¯¯å·®ï¼š <span class="math display">\[L_{pose} = \frac{1}{T\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G_{i,t} âˆ’ Y_{i,t}|,\]</span></p></li><li><p>ground truth åŠ é€Ÿåº¦å’Œä¼°è®¡åŠ é€Ÿåº¦ä¹‹é—´çš„è¯¯å·®ï¼š <span class="math display">\[L_{acc} = \frac{1}{(T-2)\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G&#39;&#39;_{i,t} âˆ’ A_{i,t}|,\]</span></p></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>To Midnight</title>
      <link href="/blog/To-Midnight/"/>
      <url>/blog/To-Midnight/</url>
      
        <content type="html"><![CDATA[<p>å¤±çœ çš„è¯</p><p>å¤œæ™šå°±å˜å¾—ç»µé•¿</p><p>ç¬¬å‡ èŠ‚è„Šæ¤ä¸Šäº†å‘æ¡ï¼Œæ‹§åŠ¨æ—¶</p><p>å¥‡å¼‚çš„å¯¹å¶çš„è¯—æµå‘æ–°çš„å°èŠ‚</p><p>è€Œç»µé•¿çš„å¤œä¸è¯¥æœ‰è¯—</p><p>ä¹Ÿä¸è¯¥æœ‰ç³Ÿç³•çš„æ¯”å–»å’Œæ¯”å–»ä¸€æ ·çš„é£</p><p>è¯¥ç»™æ¯ä¸€é˜µæµ·æµªå‘½ä»€ä¹ˆåå‘¢</p><p>è¿™æ˜¯å½“ä¸‹æœ€è¦ç´§çš„å¹½æš—é—®é¢˜</p><p>åˆ«æ— è¦ç´§äº‹</p><p>ä¸€ä¸ªäººæœ‰å¤šæƒ³å‚ä¸ç”Ÿæ´»</p><p>åˆè·Ÿç”Ÿæ´»å¯¹ååœ¨é•¿æ¡Œä¸¤ç«¯</p><p>é£å…‰é£å…‰ï¼ŒåŠå°”ä¸€æ¯é…’</p><p>å¤©äº®ååˆåˆ°äº†é…’ç¥çš„æ¢¦</p>]]></content>
      
      
      <categories>
          
          <category> poems </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Reading NeuMan: Neural Human Radiance Field from a Single Video</title>
      <link href="/blog/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/"/>
      <url>/blog/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2203.12575</p><p>ä½œè€…ï¼šJiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag</p><p>å‘è¡¨ï¼š ECCV22</p><p>å¼€æºä»£ç ï¼š https://github.com/apple/ml-neuman</p><h2 id="why">Whyï¼š</h2><ol type="1"><li><p>äººä½“çš„æ¸²æŸ“å’Œæ–°å§¿åŠ¿ç”Ÿæˆåœ¨å¢å¼ºç°å®çš„åº”ç”¨ä¸­å¾ˆé‡è¦</p></li><li><p>NeRFçš„å‡ºç°è®©æ–°è§†è§’ç”Ÿæˆä»»åŠ¡å–å¾—å¾ˆå¤§è¿›æ­¥</p></li><li><p>ä½†æ˜¯ç°æœ‰å·¥ä½œéƒ½æ²¡æœ‰å®ç°ï¼šæ ¹æ®å•æ®µwildè§†é¢‘ï¼Œç”Ÿæˆæ–°çš„äººç‰©å’Œæ–°çš„åœºæ™¯</p><figure><img src="https://s2.loli.net/2022/08/24/NdDrEhojmBLCJz9.png" alt="image-20220824123146868" /><figcaption>image-20220824123146868</figcaption></figure></li></ol><h2 id="what">Whatï¼š</h2><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>NeRFå’Œäººä½“SMPLæ¨¡å‹æ˜¯æ€ä¹ˆæœ‰æœºç»Ÿä¸€çš„ğŸ¤”</li></ol><h2 id="how">Howï¼š</h2><ol type="1"><li><p>è¾“å…¥æ˜¯ä¸€æ®µwildè§†é¢‘ï¼Œmoving cameraçš„ã€‚ç”¨ç°å­˜æ–¹æ³•ä¼°è®¡äººä½“å§¿åŠ¿ã€äººä½“å½¢çŠ¶ã€äººä½“maskï¼ˆMask-RCNNï¼‰ã€ç›¸æœºposeã€sparse scene modelã€depth maps</p></li><li><p>ç„¶åè®­ç»ƒä¸¤ä¸ª NeRF æ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºäººä½“ï¼Œä¸€ä¸ªç”¨äºç”± Mask-RCNN ä¼°è®¡çš„åˆ†å‰²maskå¼•å¯¼çš„èƒŒæ™¯ã€‚ æ­¤å¤–ï¼Œé€šè¿‡å°†æ¥è‡ªå¤šè§†å›¾é‡å»ºå’Œå•ç›®æ·±åº¦å›å½’çš„æ·±åº¦ä¼°è®¡èåˆåœ¨ä¸€èµ·æ¥è§„èŒƒåœºæ™¯ NeRF æ¨¡å‹</p></li><li><p>å…³äºNeRFï¼š(å‚è€ƒï¼š<a href="https://zhuanlan.zhihu.com/p/360365941">zhihu</a>ï¼‰</p><ol type="1"><li><p>NeRFæ˜¯ç”¨ç¥ç»è¾å°„åœºå»ºæ¨¡ä¸€ä¸ªåœºæ™¯ï¼Œå¥½å¤„æ˜¯å¯ä»¥ç”Ÿæˆæ–°è§†è§’çš„å›¾åƒã€‚é’ˆå¯¹ä¸€ä¸ªé™æ€åœºæ™¯ï¼Œéœ€è¦æä¾›å¤§é‡ç›¸æœºå‚æ•°å·²çŸ¥çš„å›¾ç‰‡ã€‚åŸºäºè¿™äº›å›¾ç‰‡è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼Œå³å¯ä»¥ä»ä»»æ„è§’åº¦æ¸²æŸ“å‡ºå›¾ç‰‡ç»“æœäº†ã€‚</p></li><li><p>å®ƒç”¨MLPï¼ŒæŠŠä¸€ä¸ª3dåœºæ™¯éšå¼åœ°ç¼–ç è¿›ç¥ç»ç½‘ç»œé‡Œã€‚è¾“å…¥ä¸º3dç©ºé—´ä¸­ä¸€ä¸ªç‚¹çš„åæ ‡<span class="math inline">\(\bold x = (x,y,z)\)</span>å’Œç›¸æœºè§†è§’ <span class="math inline">\(\bold d = (\theta, \phi)\)</span>ï¼Œè¾“å‡ºä¸ºè¯¥ç‚¹å¯¹åº”çš„ä½“ç´ çš„å¯†åº¦opacityï¼Œä»¥åŠé¢œè‰²<span class="math inline">\(\bold c = (r,g,b)\)</span>ã€‚å…¬å¼å°±æ˜¯ <span class="math display">\[f(\bold{x},\bold{v})=(\bold c, \sigma)\]</span></p></li><li><p>åœ¨å…·ä½“çš„å®ç°ä¸­ï¼Œ x é¦–å…ˆè¾“å…¥åˆ°MLPç½‘ç»œä¸­ï¼Œå¹¶è¾“å‡º Ïƒ å’Œä¸­é—´ç‰¹å¾ï¼Œä¸­é—´ç‰¹å¾å’Œ d å†è¾“å…¥åˆ°é¢å¤–çš„å…¨è¿æ¥å±‚ä¸­å¹¶é¢„æµ‹é¢œè‰²ã€‚å› æ­¤ï¼Œä½“ç´ å¯†åº¦åªå’Œç©ºé—´ä½ç½®æœ‰å…³ï¼Œè€Œé¢œè‰²åˆ™ä¸ç©ºé—´ä½ç½®ä»¥åŠè§‚å¯Ÿçš„è§†è§’éƒ½æœ‰å…³ç³»ã€‚åŸºäºview dependent çš„é¢œè‰²é¢„æµ‹ï¼Œèƒ½å¤Ÿå¾—åˆ°ä¸åŒè§†è§’ä¸‹ä¸åŒçš„å…‰ç…§æ•ˆæœã€‚</p></li><li><p>NeRF å‡½æ•°å¾—åˆ°çš„æ˜¯ä¸€ä¸ª3Dç©ºé—´ç‚¹çš„é¢œè‰²å’Œå¯†åº¦ä¿¡æ¯ï¼Œä½†å½“ç”¨ä¸€ä¸ªç›¸æœºå»å¯¹è¿™ä¸ªåœºæ™¯æˆåƒæ—¶ï¼Œæ‰€å¾—åˆ°çš„2D å›¾åƒä¸Šçš„ä¸€ä¸ªåƒç´ å®é™…ä¸Šå¯¹åº”äº†ä¸€æ¡ä»ç›¸æœºå‡ºå‘çš„å°„çº¿ä¸Šçš„æ‰€æœ‰è¿ç»­ç©ºé—´ç‚¹ã€‚åç»­å°±æœ‰å„ç§å„æ ·é«˜æ•ˆçš„æ–¹å¼æ¥è¿›è¡Œå¯å¾®æ¸²æŸ“äº†ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯ä»è¿™æ¡å°„çº¿ä¸Šé‡‡æ ·ï¼Œè·å¾—å¹³å‡çš„é¢œè‰²ä¿¡æ¯ã€‚</p></li></ol></li></ol><h3 id="äººä½“æ¨¡å‹nerfsmpl">äººä½“æ¨¡å‹ï¼šNeRF+SMPL</h3><p>æˆ‘ä¸»è¦å…³æ³¨çš„å°±æ˜¯äººä½“æ¨¡å‹è¿™éƒ¨åˆ†äº†ã€‚æ€»ä½“æ¥è¯´ï¼Œåšæ³•å°±æ˜¯ï¼š</p><p>é¦–å…ˆç”Ÿæˆäººä½“NeRFæ¨¡å‹ï¼Œç„¶åç”¨ROMPç”Ÿæˆé€å¸§çš„äººä½“SMPLæ¨¡å‹ï¼Œç„¶åå®šä¹‰ä¸€ä¸ªcanonicalçš„äººä½“æ¨¡å‹ï¼ˆä¸»è¦æ˜¯å»æ‰å§¿åŠ¿è¿™ä¸ªå˜é‡ï¼Œå˜æˆå¤§å­—å‹äººä½“ï¼‰ï¼Œæ ¹æ®åƒç´ ç‚¹åœ¨SMPLæ¨¡å‹ä¸Šå¯¹åº”çš„ä½ç½®ï¼Œå†å¯¹åº”åˆ°canonicalæ¨¡å‹ä¸Šï¼Œå­¦åˆ°äººä½“çš„å¤–è²Œã€‚ï¼ˆå…¶å®è®­ç»ƒä¸­NeRFå’ŒSMPLæ¨¡å‹æ˜¯ä¸€èµ·å­¦çš„ï¼Œæ²¡æœ‰åˆ†å¾—é‚£ä¹ˆå¼€çš„å…ˆåé¡ºåºã€‚ï¼‰</p><figure><img src="https://s2.loli.net/2022/08/24/JSycDH34UlMBsEW.png" alt="image-20220824180150642" /><figcaption>image-20220824180150642</figcaption></figure><p>å…·ä½“æ¥è¯´ï¼š</p><ol type="1"><li><p>å¯¹äºæŸä¸€å¸§å›¾åƒï¼Œç”¨ROMPä¼°è®¡äººä½“çš„SMPLæ¨¡å‹ï¼Œä½†é‡‡å–äº†ä¸€äº›æ”¹è‰¯ï¼š</p><ol type="1"><li>åˆ©ç”¨denseposeä¼°è®¡äººä½“çš„silhouetteï¼Œä»¥åŠMMPoseä¼°è®¡äººä½“çš„2D jointsï¼›æ ¹æ®è¿™äº›ç»“æœä¼˜åŒ–SMPLå‚æ•°</li></ol></li><li><p>æŠŠåˆšåˆšå¾—åˆ°çš„SMPLæ¨¡å‹warpæˆä¸€ä¸ªcanonicalçš„å¤§å­—å‹äººä½“æ¨¡å‹ï¼Œè¿™ä¸ªwarpå˜æ¢ç§°ä¸º<span class="math inline">\(\mathcal T\)</span></p></li><li><p>æ€ä¹ˆæŠŠå›¾åƒä¸­çš„åƒç´ ç‚¹å¯¹åº”åˆ°canonicalçš„å¤§å­—å‹äººä½“æ¨¡å‹ä¸Šå‘¢ï¼Ÿ</p><ol type="1"><li><p>é¦–å…ˆç”Ÿæˆäººä½“NeRFæ¨¡å‹</p></li><li><p>å¯¹äºç©ºé—´ä¸­çš„æ¯ä¸ªç‚¹<span class="math inline">\(\bold x_f=\bold r_f(t)\)</span> ï¼ˆè¿™é‡Œçš„fæ˜¯ç¬¬få¸§å›¾åƒï¼‰ï¼Œå®ƒéƒ½å¯ä»¥ç”±ä¸€æ¡å°„çº¿<span class="math inline">\(\bold r\)</span>ä¸Šå¯¹åº”çš„åƒç´ ç‚¹æ¸²æŸ“è€Œæ¥ï¼›é‚£ä¹ˆå¯¹è¿™ä¸ªç‚¹ç›´æ¥åº”ç”¨å‰é¢çš„å˜æ¢<span class="math inline">\(\mathcal{T}\)</span>ï¼Œå°±å¾—åˆ°å®ƒåœ¨canonicalç©ºé—´ä¸­å¯¹åº”çš„ç‚¹äº†ï¼Œ<span class="math inline">\(\bold x&#39;_f = \mathcal{T}_{\theta_f}(\bold x_f)\)</span></p></li><li><p>ä½†æ˜¯å› ä¸ºSMPLçš„ä¼°è®¡ä¸æ˜¯å¾ˆå‡†ç¡®ï¼Œè¿™ä¸ªå˜æ¢<span class="math inline">\(\mathcal{T}\)</span>ä¹Ÿä¸æ˜¯å¾ˆå‡†ç¡®ï¼Œæ‰€ä»¥è¿™é‡Œæå‡ºæ¥ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­åŒæ—¶ä¼˜åŒ–SMPLæ¨¡å‹ <span class="math inline">\(\theta_f\)</span>å’Œäººä½“NeRFæ¨¡å‹çš„æ–¹å¼ï¼Œå¯ä»¥æå‡æ•ˆæœã€‚</p></li><li><p>è¿˜æœ‰ï¼Œè¿˜åŠ äº†ä¸€ä¸ªMLPæ”¹é”™ç½‘ç»œ<span class="math inline">\(\mathcal{E}\)</span>æ”¹æ­£warpingçš„è¯¯å·®ã€‚æœ€ç»ˆç»“æœå°±æ˜¯ï¼š <span class="math display">\[\bold {\tilde x&#39;_f} = \mathcal{T}_{\theta_f}(\bold x_f) + \mathcal{E}(\bold x_f, f)\]</span></p></li><li><p>æ­¤æ—¶ç›¸æœºè§†è§’ä¹Ÿéœ€è¦æ ¡æ­£ï¼šå¯¹äºå°„çº¿rayä¸Šçš„ç¬¬iä¸ªæ ·æœ¬ç‚¹ï¼Œ <span class="math display">\[\bold d(t_i)&#39;_f = \bold {\hat x}&#39;_f(t_i) - \bold {\hat x}&#39;_f(t_{i-1})\]</span></p></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> 3D Generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>It seems impossible to access USB devices in Docker on MacOS</title>
      <link href="/blog/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/"/>
      <url>/blog/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/</url>
      
        <content type="html"><![CDATA[<p>According to <a href="https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd" class="uri">https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd</a>, it seems impossible to forward USB to a Docker container on MacOS, coz the Docker is running in a virtual environment via <a href="https://github.com/docker/for-mac/issues/900">hyperkit</a>.</p><p>First of all, ports of the host (i.e., MacOS) cannot be directly accessed by any virtual environment (i.e., Docker) on it. So, &quot;you first have to expose it to the virtual machine where Docker is running&quot;. However, Docker is running on hyperkit, which doesn't support usb forwarding.</p><p>The author provided another way to do it, that is to use<code>docker-machine</code>, which uses a Virtualbox VM to host the <code>dockerd</code> daemon, to replace the original docker. Then... why bother still using Docker, instead of just using Virtualbox to run the seperated environment?</p>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Video-to-Video Synthesis</title>
      <link href="/blog/Reading-Video-to-Video-Synthesis/"/>
      <url>/blog/Reading-Video-to-Video-Synthesis/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf</p><p>ä½œè€…ï¼š<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>å‘è¡¨ï¼š NeurIPS 2018</p><p>Projectï¼šhttps://tcwang0509.github.io/vid2vid/</p><p>Githubï¼šhttps://github.com/NVIDIA/vid2vid</p><hr /><blockquote><p><strong>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</strong></p><p>è¿™ç¯‡è®ºæ–‡çš„èƒŒæ™¯æ˜¯å½“æ—¶å·²ç»æœ‰æ¯”è¾ƒå¥½çš„pic2picç”Ÿæˆæ¨¡å‹äº†ã€‚è¦è®©vid2vidä¹Ÿworkçš„è¯ï¼Œæœ€é‡è¦çš„åº”è¯¥å°±æ˜¯å¸§ä¸å¸§ä¹‹é—´consistencyçš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä¼šæƒ³åœ¨å°†pic2picç”Ÿæˆæ¨¡å‹åº”ç”¨åœ¨videoçš„åŸºç¡€ä¸Šï¼Œå¯¹å¸§ä¹‹é—´åŠ ä¸Šconsistency lossã€‚ä½†ç›´æ¥è¿™æ ·è‚¯å®šæ•ˆç‡å¾ˆä½ï¼Œå› ä¸ºä¸€ä¸ªè§†é¢‘ä¸­å¸§ä¸å¸§ä¹‹é—´è‚¯å®šä¼šåŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯å˜›ï¼Œåº”è¯¥è¿˜éœ€è¦æƒ³åŠæ³•è®©å¸§ä¹‹é—´ä¿¡æ¯å…±äº«ï¼Œè¿™æ ·æ¨¡å‹åªéœ€è¦é¢„æµ‹åä¸€å¸§ä¸å‰ä¸€å¸§ä¸åŒçš„åœ°æ–¹ï¼Œå‡å°‘è¿ç®—ã€‚ä¾‹å¦‚ï¼Œç”¨ä¸€ä¸ªå°ç½‘ç»œé¢„æµ‹å›¾åƒä¸­é™æ€ä¸åŠ¨æ€çš„éƒ¨åˆ†ï¼Œä¸çŸ¥é“å¯ä¸å¯è¡Œã€‚</p><p><strong>çœ‹å®Œæ–‡ç« å</strong>ï¼šæˆ‘æ„Ÿè§‰æˆ‘åœ¨å¤§ä½“æ€è·¯ä¸ŠæŠŠæ¡å‡†äº†ï¼Œä¸»è¦çŸ›ç›¾ç¡®å®å¦‚æ­¤ã€‚ä½†</p><ol type="1"><li>ä½œè€…æ²¡æœ‰ç”¨consistency lossï¼Œè€Œæ˜¯ç”¨gançš„æ€è·¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ¡ä»¶è§†é¢‘é‰´åˆ«å™¨ <span class="math inline">\(D_V\)</span>ï¼Œé‰´åˆ«è§†é¢‘åœ¨æ—¶åºä¸Šçš„åŠ¨æ€æ˜¯å¦çœŸå®è‡ªç„¶ã€‚</li><li>æˆ‘æ²¡æœ‰optical flowï¼Œå…‰æµï¼Œè¿™æ–¹é¢çš„çŸ¥è¯†å‚¨å¤‡ï¼›ä½œè€…åˆ©ç”¨ä¸€ä¸ªç½‘ç»œé¢„æµ‹optical flowï¼Œå°±å¯ä»¥ç›´æ¥æ ¹æ®å‰ä¸€å¸§å›¾åƒå¾—åˆ°åä¸€å¸§å›¾åƒä¸­å¯¹åº”çš„åƒç´ ç‚¹äº†ï¼Œè€Œä¸”è¿™æ ·çš„ç»“æœèƒ½å¤Ÿå¾ˆconsistentã€‚å¯¹äºå‰ä¸€å¸§å›¾åƒä¸­æ²¡æœ‰å¯¹åº”çš„åƒç´ ï¼Œå†ç”¨ä¸€ä¸ªè¡¥æ´ç½‘ç»œè¡¥æ´ã€‚è¿™æ ·å°±è§£å†³äº†æ•ˆç‡é—®é¢˜ã€‚</li><li>ä½œè€…è¿˜åˆ©ç”¨äº†ç‰¹å¾åµŒå…¥æ–¹æ³•ï¼Œå®ç°äº†å¤šæ¨¡æ€è§†é¢‘çš„ç”Ÿæˆï¼Œè¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡äº†è§£åˆ°çš„æ–¹æ³•ï¼Œæ„Ÿè§‰å¾ˆæœ‰è¶£ã€‚</li></ol></blockquote><h2 id="why">Whyï¼š</h2><ol type="1"><li>å›¾åƒæ°´å¹³ä¸Šçš„ç”Ÿæˆè¢«ç ”ç©¶å¾—å¾ˆå¥½ï¼Œä½†æ˜¯è§†é¢‘ä¸Šçš„æ­¤å‰å´æ¯”è¾ƒå°‘ï¼›å›¾åƒç”Ÿæˆçš„æˆæœå¦‚æœç›´æ¥æ”¾åœ¨è§†é¢‘ä¸Šçš„è¯ï¼Œæ•ˆæœä¸å¤ªå¦™ï¼Œå› ä¸ºå¸§ä¸å¸§ä¹‹é—´ç¼ºä¹è¿è´¯æ€§ã€‚æ‰€ä»¥æ˜¯éœ€è¦ä¸€äº›temporalä¸Šçš„æ”¹è¿›çš„</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>åˆ©ç”¨GANå’Œä¸€äº›æ—¶é—´-ç©ºé—´å¯¹æŠ—ç›®æ ‡ï¼ˆspatio-temporal adverbial objectiveï¼‰ï¼Œæ¥å®ç°video to videoçš„ç”Ÿæˆã€‚</li><li>ç”¨ä¸åŒç±»å‹çš„è¾“å…¥æ¥ç”Ÿæˆæ–°çš„ç…§ç‰‡å†™å®é£æ ¼çš„è§†é¢‘ï¼Œæ•ˆæœå¾ˆå¥½ã€‚</li><li>æ˜¯ä¸€ä¸ªå…¨æ–°å®šä¹‰çš„vid2vidä»»åŠ¡ï¼Œä¸»è¦æ–°ç‚¹åœ¨äºï¼šè¾“å…¥çš„vidå¹¶ä¸æ˜¯å®Œæ•´çš„è§†é¢‘å¸§ï¼Œè€Œæ˜¯ä¸€äº›å¯ä»¥æ“æ§çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¾‹å¦‚segmentation masks, sketches, and poses</li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>åœ¨æ­¤ä¹‹å‰æ²¡æœ‰æ¯”è¾ƒå¥½çš„vid2vidï¼Œè¿™ç¯‡è®ºæ–‡æ˜¯åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹å®ç°äº†å¾ˆå¥½çš„vid2vidæ•ˆæœå‘¢ï¼Œæ¯”å¦‚å…¶ä»–æ–¹é¢çš„æŠ€æœ¯é©æ–°ï¼Ÿæˆ‘è§‰å¾—ä¸»è¦æ˜¯åˆ©ç”¨äº†ganï¼Œä¸€ä¸ªå›¾åƒé‰´åˆ«å™¨+ä¸€ä¸ªè§†é¢‘é‰´åˆ«å™¨ç›¸é…åˆï¼Œå–å¾—å¾ˆå¥½çš„ç”Ÿæˆæ•ˆæœã€‚é™¤æ­¤ä¹‹å¤–æˆ‘è§‰å¾—optical flowç”¨åœ¨è¿™é‡Œä¹Ÿå¾ˆå¥½ï¼Œæ•ˆç‡é«˜è€Œä¸”ç”Ÿæˆæ•ˆæœè¿è´¯ï¼ˆä½†æ˜¯ä¸çŸ¥é“æ–°ä¸æ–°ï¼‰ï¼›å¦å¤–ç‰¹å¾åµŒå…¥æ–¹æ³•ç”¨åœ¨è¿™é‡Œä¹Ÿå¾ˆå¥½ï¼Œå®ç°äº†æ ¹æ®è¯­ä¹‰ä¿¡æ¯æ¥ç”Ÿæˆæ–°è§†é¢‘ï¼Œè€Œä¸”å¯ä»¥æ˜¯å¤šæ¨¡æ€çš„è§†é¢‘</li><li>æ‘˜è¦é‡Œå¼ºè°ƒçš„æ—¶é—´-ç©ºé—´å¯¹æŠ—ç›®æ ‡ï¼ˆspatio-temporal adverbial objectiveï¼‰åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘æ„Ÿè§‰æ­£æ–‡é‡Œå¥½åƒæ²¡æœ‰å†ç‰¹åˆ«å¼ºè°ƒæ—¶é—´-ç©ºé—´è¿™ä¸€å¯¹ç›®æ ‡äº†â€¦â€¦æ ¹æ®æˆ‘è‡ªå·±çš„ç†è§£çš„è¯ï¼Œä¸»è¦å°±æ˜¯é‚£ä¸€å¯¹é‰´åˆ«å™¨ï¼šæ—¶é—´ä¸Š--è§†é¢‘é‰´åˆ«å™¨é‰´åˆ«åœ¨æ—¶åºä¸Šçš„åŠ¨æ€æ˜¯å¦çœŸå®è‡ªç„¶ï¼Œç©ºé—´ä¸Š--å›¾åƒé‰´åˆ«å™¨é‰´åˆ«ä¸€å¼ å›¾åƒåœ¨ç©ºé—´ä¸Šæ˜¯å¦çœŸå®è‡ªç„¶ã€‚</li></ol><h2 id="how">Howï¼š</h2><h3 id="å®šä¹‰vid2vidä»»åŠ¡">å®šä¹‰vid2vidä»»åŠ¡</h3><ol type="1"><li><p>å®šä¹‰è¾“å…¥çš„è¯­ä¹‰ä¿¡æ¯åºåˆ—ä¸º<span class="math inline">\(s\)</span>ï¼Œå¯¹åº”çš„çœŸå®è§†é¢‘åºåˆ—ä¸º<span class="math inline">\(x\)</span>ï¼Œæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘åºåˆ—ä¸º<span class="math inline">\(\tilde x\)</span>ï¼Œåˆ™æ¨¡å‹çš„ç›®æ ‡æ˜¯åœ¨ç»™å®š<span class="math inline">\(s\)</span>çš„æ¡ä»¶ä¸‹ï¼Œè®©<span class="math inline">\(\tilde x\)</span>çš„æ¡ä»¶åˆ†å¸ƒæ‹Ÿåˆ<span class="math inline">\(x\)</span>çš„æ¡ä»¶åˆ†å¸ƒ</p></li><li><figure><img src="https://s2.loli.net/2022/09/01/VfJE42QpgUB5zxt.png" alt="image-20220524143313609" /><figcaption>image-20220524143313609</figcaption></figure><p>Dæ˜¯discriminatorï¼ŒGæ˜¯generatorã€‚æ•´ä¸ªä»»åŠ¡å°±å˜æˆäº†ä¸€ä¸ªæœ€å¤§æœ€å°ä¼˜åŒ–é—®é¢˜ï¼Œè®ºæ–‡ä¸»è¦é€šè¿‡è®¾è®¡ç½‘ç»œå’Œæ—¶ç©ºä¼˜åŒ–ç›®æ ‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p></li><li><p>ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œåšäº†ä¸€ä¸ªMarkovå‡è®¾ï¼šå½“å‰ç¬¬tå¸§ç”Ÿæˆçš„è§†é¢‘<span class="math inline">\(\tilde x_t\)</span>ï¼Œç”±ä¸”ä»…ç”±ç¬¬tå¸§è¾“å…¥<span class="math inline">\(s_t\)</span> + å‰Lå¸§è¾“å…¥<span class="math inline">\(s_{t-L}^{t-1}\)</span> + å‰Lå¸§ç”Ÿæˆçš„è§†é¢‘<span class="math inline">\(\tilde x_{t-L}^{t-1}\)</span>è¿™ä¸‰ä¸ªå› ç´ å†³å®šã€‚<img src="https://s2.loli.net/2022/09/01/Q9MPRD5otda3mAq.png" alt="image-20220524144030781" /></p><p>å®éªŒé‡Œï¼ŒLå–äº†ä¸ªä¸å¤§ä¸å°çš„2.å»ºç«‹äº†ä¸€ä¸ªç¥ç»ç½‘ç»œFï¼Œé€’å½’åœ°é€å¸§ç”Ÿæˆè§†é¢‘ã€‚</p></li></ol><h3 id="ç½‘ç»œæ¶æ„">ç½‘ç»œæ¶æ„</h3><ol type="1"><li><p>ç½‘ç»œFå®šä¹‰å¦‚ä¸‹ï¼š</p><figure><img src="https://s2.loli.net/2022/09/01/5OA4ptSUjI9Mn7J.png" alt="image-20220524144722968" /><figcaption>image-20220524144722968</figcaption></figure><p>ç»™å®š<span class="math inline">\((\tilde x_{t-L}^{t-1}, s_{t}^{t-1})\)</span>ä½œä¸ºè¾“å…¥ã€‚å¯¹äºä¸ä¸Šä¸€å¸§å›¾åƒæœ‰å…³è”çš„åƒç´ ç‚¹ï¼Œç½‘ç»œä¼šåˆ©ç”¨optical flowæ¥warpï¼ˆæ‰­æ›²ï¼Ÿï¼‰ä¸Šä¸€å¸§åƒç´ ç‚¹ï¼Œå¾—åˆ°è¿™ä¸€å¸§æ–°çš„åƒç´ ç‚¹ã€‚å¯¹åº”ç­‰å¼çš„å‰åŠéƒ¨åˆ†ã€‚è¿˜æœ‰ä¸€äº›åƒç´ æ˜¯ä¸Šä¸€å¸§å›¾åƒé‡Œæ²¡æœ‰çš„ï¼Œè¿™æ—¶å€™å°±éœ€è¦ç”Ÿæˆæ¥å¡«å……ã€‚å¯¹åº”ç­‰å¼çš„ååŠéƒ¨åˆ†ã€‚å…·ä½“æ¥è¯´ï¼š</p><ul><li>ç”¨ä¸€ä¸ªoptical flowé¢„æµ‹ç½‘ç»œWæ¥ä¼°è®¡ä»ä¸Šä¸€å¸§åˆ°è¿™ä¸€å¸§çš„optical flow <span class="math inline">\(\tilde w_{t-1}\)</span>.</li><li>ç”¨ä¸€ä¸ªç”Ÿæˆå™¨Hæ¥ç”Ÿæˆéœ€è¦å¡«å……çš„åƒç´ <span class="math inline">\(\tilde h_t\)</span>.</li><li>ç”¨ä¸€ä¸ªmaské¢„æµ‹ç½‘ç»œMæ¥ç”Ÿæˆmask <span class="math inline">\(\tilde m_t\)</span>. è¿™ä¸ªmaskä¸æ˜¯é0å³1çš„ï¼Œè€Œæ˜¯åŒ…å«äº†0-1ä¹‹é—´çš„è¿ç»­å€¼ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†æ›´å¥½åœ°èåˆWå’ŒHç”Ÿæˆçš„ç»“æœã€‚æ¯”å¦‚è¯´ï¼Œåœ¨ zoom in çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç‰©ä½“é€æ¸é è¿‘ï¼Œé‚£ä¹ˆå®ƒä¼šé€å¸§æ”¾å¤§ã€‚å¦‚æœä»…ä»…åˆ©ç”¨optical flowçš„æ‰­æ›²ç»“æœï¼Œé‚£ä¹ˆè¿™ä¸ªç‰©ä½“å°±ä¼šå˜å¾—æ¨¡ç³Šã€‚å› æ­¤ï¼Œè¿˜éœ€è¦ç”Ÿæˆå™¨æ¥å¡«å……ä¸€äº›ç»†èŠ‚ã€‚æœ‰äº†soft maskï¼Œwarpçš„åƒç´ å’Œç”Ÿæˆçš„åƒç´ å°±å¯ä»¥èåˆã€‚</li></ul></li><li><p>ç”¨äº†coarse-to-fineçš„æ–¹æ³•æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è§†é¢‘</p></li><li><p>ç”¨äº†2ä¸ªä¸åŒçš„discriminatoræ¥å‡è½»ganè®­ç»ƒä¸­çš„mode collapseé—®é¢˜ï¼ˆæ¨¡å¼å€’å¡Œï¼Œå³ç”Ÿæˆçš„ç»“æœæ˜¯å¾ˆé€¼çœŸï¼Œä½†æ˜¯å¤šæ ·æ€§ä¸è¶³ï¼‰ã€‚</p><ol type="1"><li>æ¡ä»¶å›¾åƒé‰´åˆ«å™¨ <span class="math inline">\(D_I\)</span>ï¼Œé¡¾åæ€ä¹‰ï¼Œé‰´åˆ«æ¯ä¸€å¸§å›¾åƒæ˜¯å¦çœŸå®çš„</li><li>æ¡ä»¶è§†é¢‘é‰´åˆ«å™¨ <span class="math inline">\(D_V\)</span>ï¼Œé‰´åˆ«è§†é¢‘åœ¨æ—¶åºä¸Šçš„åŠ¨æ€æ˜¯å¦çœŸå®è‡ªç„¶ã€‚ç»™å®šoptical flowï¼Œé‰´åˆ«Kä¸ªè¿ç»­çš„å¸§</li></ol></li></ol><h3 id="losses">losses</h3><p><span class="math display">\[\mathop{min}\limits_{F} ( \mathop{max}\limits_{D_I}\mathcal{L}_I (F, D_I ) + \mathop{max}\limits_{D_V}  \mathcal{L}_V (F, D_V )) + Î»_W L_W (F ),\]</span></p><ol type="1"><li><span class="math inline">\(\mathcal{L}_I\)</span> æ˜¯æ¡ä»¶å›¾åƒé‰´åˆ«å™¨ <span class="math inline">\(D_I\)</span>çš„gan lossï¼š<img src="https://s2.loli.net/2022/09/01/J1mVkCnzWYZiaR9.png" alt="image-20220530194307049" />ï¼Œå…¶ä¸­ï¼Œ<span class="math inline">\(\phi_I\)</span> å°±æ˜¯ä»ç¬¬1ï½Tå¸§ä¸­éšæœºå–1å¸§çš„æ“ä½œ</li><li><span class="math inline">\(\mathcal{L}_V\)</span>æ˜¯æ¡ä»¶è§†é¢‘é‰´åˆ«å™¨ <span class="math inline">\(D_V\)</span>çš„ gan lossï¼š<img src="https://s2.loli.net/2022/09/01/rq6yiaRSHUnEYzg.png" alt="image-20220530194419594" />ï¼Œå’Œå›¾åƒçš„å¦‚å‡ºä¸€è¾™ï¼Œ<span class="math inline">\(\phi_V\)</span> å°±æ˜¯ä»ç¬¬1ï½Tå¸§ä¸­éšæœºå–è¿ç»­Kå¸§çš„æ“ä½œ</li><li><span class="math inline">\(L_W\)</span> æ˜¯flow estimation lossï¼š<img src="https://s2.loli.net/2022/09/01/AuZf1zlcihE3KsU.png" alt="image-20220530194534485" />ï¼ŒåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œ1. çœŸå®flowå’Œä¼°è®¡flowçš„ç«¯ç‚¹è¯¯å·® 2. æŠŠå‰ä¸€å¸§æ‰­æ›²åˆ°åä¸€å¸§çš„warp loss</li></ol><h3 id="å‰æ™¯-èƒŒæ™¯å…ˆéªŒ">å‰æ™¯-èƒŒæ™¯å…ˆéªŒ</h3><p>é€šè¿‡è¯­ä¹‰åˆ†å‰²ï¼Œç»™æ¨¡å‹æä¾›äº†ä¸€ä¸ªå‰æ™¯ã€èƒŒæ™¯çš„å…ˆéªŒä¿¡æ¯ï¼ŒåŒæ—¶æŠŠè¡¥æ´ç½‘ç»œæ‹†åˆ†æˆäº†ä¸¤ä¸ªï¼š</p><ol type="1"><li>èƒŒæ™¯è¡¥æ´ç½‘ç»œï¼šè¿™ä¸ªå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæ•´ä¸ªå¤§èƒŒæ™¯å…¶å®å¯ä»¥ç”±optical flowå¾ˆå‡†ç¡®åœ°é¢„æµ‹å‡ºæ¥ï¼Œè¡¥æ´ç½‘ç»œåªéœ€è¦è¡¥ä¸€ç‚¹ç‚¹ä»ç”»é¢å¤–é¢åˆšè¿›æ¥çš„éƒ¨åˆ†</li><li>å‰æ™¯è¡¥æ´ç½‘ç»œï¼šè¿™ä¸ªæ¯”è¾ƒéš¾ï¼Œå› ä¸ºå‰æ™¯ç‰©å“å¾€å¾€å æ¯”ä¸å¤§ï¼Œä½†æ˜¯åˆåŠ¨ä½œå¹…åº¦å¾ˆå¤§ï¼Œå‰æ™¯è¡¥æ´ç½‘ç»œéœ€è¦ä»é›¶å¼€å§‹ç”Ÿæˆå¾ˆå¤šä¸œè¥¿</li></ol><p>é€šè¿‡ç”¨æˆ·å®éªŒï¼Œè¯æ˜å¤§éƒ¨åˆ†äººè§‰å¾—æœ‰è¿™ä¸ªå‰æ™¯-èƒŒæ™¯å…ˆéªŒä¹‹åï¼Œæ•ˆæœæ›´å¥½ã€‚</p><h3 id="å¤šæ¨¡æ€ç”Ÿæˆ">å¤šæ¨¡æ€ç”Ÿæˆ</h3><p>ç½‘ç»œFæ˜¯ä¸€ä¸ªå•æ¨¡æ€æ˜ å°„å‡½æ•°ï¼Œè¿™æ„å‘³ç€è¾“å…¥ä¸€ä¸ªè§†é¢‘ï¼Œå®ƒä¹Ÿåªèƒ½ç”Ÿæˆä¸€ä¸ªè§†é¢‘ã€‚é‚£æ€æ ·è®©å®ƒæ ¹æ®åŒä¸€ä¸ªè¾“å…¥è§†é¢‘ï¼Œè¾“å‡ºå¤šä¸ªä¸åŒçš„è§†é¢‘å‘¢ï¼Ÿè¿™é‡Œé‡‡ç”¨äº†ç‰¹å¾åµŒå…¥æ–¹æ³•ï¼ˆfeature embedding schemeï¼‰ã€‚</p><ol type="1"><li>è¾“å…¥æºè§†é¢‘çš„åŒæ—¶ï¼Œä¹Ÿè¾“å…¥instanceçº§åˆ«çš„è¯­ä¹‰åˆ†å‰²mask <span class="math inline">\(s_t\)</span></li><li>è®­ç»ƒä¸€ä¸ªå›¾åƒç¼–ç å™¨Eï¼Œå®ƒæŠŠæ¯ä¸€å¸§çœŸå®å›¾åƒ<span class="math inline">\(x_t\)</span>ç¼–ç æˆdç»´çš„ç‰¹å¾mapï¼ˆè®ºæ–‡ä¸­då–3ï¼‰ã€‚ç„¶åç”¨ä¸€ä¸ªinstance-wiseçš„å¹³å‡æ± åŒ–ï¼Œæ¥è®©åŒä¸€ä¸ªç‰©ä½“çš„æ‰€æœ‰åƒç´ åˆ†äº«å…±åŒçš„ç‰¹å¾å‘é‡ã€‚å¾—åˆ°çš„è¿™ä¸ªinstance-wiseå¹³å‡æ± åŒ–åçš„å›¾åƒç‰¹å¾mapç§°ä¸º<span class="math inline">\(z_t\)</span></li><li>è¿™ä¸ª<span class="math inline">\(z_t\)</span>ï¼ŒåŠ ä¸Šmask <span class="math inline">\(s_t\)</span>ï¼Œå†è¢«è¾“å…¥åˆ°ç½‘ç»œF</li><li>ä»¥ä¸Šæ˜¯è®­ç»ƒçš„è¿‡ç¨‹ã€‚è®­ç»ƒç»“æŸåï¼Œå¯¹æ¯ä¸ªç±»å‹çš„å¯¹è±¡çš„ç‰¹å¾å‘é‡çš„é«˜æ–¯åˆ†å¸ƒæ‹Ÿåˆä¸€ä¸ªæ··åˆé«˜æ–¯åˆ†å¸ƒã€‚</li><li>æµ‹è¯•çš„æ—¶å€™ï¼Œåˆ©ç”¨æ¯ä¸ªç‰©ä½“æ‰€å¯¹åº”çš„ç±»å‹çš„åˆ†å¸ƒï¼Œå¯ä»¥sampleç‰¹å¾å‘é‡ï¼Œè¿›è€Œç”Ÿæˆæ–°è§†é¢‘äº†ã€‚ç»™å‡ºä¸åŒçš„ç‰¹å¾å‘é‡ï¼ŒFå°±èƒ½ç”Ÿæˆä¸åŒçš„è§†é¢‘äº†</li></ol><figure><img src="https://tcwang0509.github.io/vid2vid/paper_gifs/cityscapes_change_styles.gif" alt="img" /><figcaption>img</figcaption></figure><h3 id="å®ç°çš„ç»†èŠ‚">å®ç°çš„ç»†èŠ‚</h3><ol type="1"><li>coarse-to-fineçš„è®­ç»ƒï¼š512 Ã— 256, 1024 Ã— 512, and 2048 Ã— 1024 resolutionsï¼Œè¿™ä¸‰ç§åˆ†è¾¨ç‡ï¼Œå…ˆä»ä½çš„å¼€å§‹è®­ç»ƒèµ·ï¼Œé€æ¸å¢åŠ åˆ°é«˜çš„ã€‚</li><li>maské¢„æµ‹ç½‘ç»œMå’Œflowé¢„æµ‹ç½‘ç»œWå…±äº«æƒé‡ï¼Œåªæœ‰è¾“å‡ºå±‚ä¸ä¸€æ ·ã€‚</li><li>å›¾åƒé‰´åˆ«å™¨æ˜¯ä¸€ä¸ªå¤šå°ºåº¦PatchGAN</li><li>é™¤äº†ç©ºé—´ä¸Šçš„å¤šå°ºåº¦ï¼Œè§†é¢‘é‰´åˆ«å™¨è¿˜ä¼šè€ƒè™‘å¤šä¸ªå¸§ç‡ï¼Œå³æ—¶é—´ä¸Šçš„å¤šå°ºåº¦ï¼Œç¡®ä¿çŸ­æœŸå’Œé•¿æœŸéƒ½èƒ½consistent</li><li>2kåˆ†è¾¨ç‡ï¼Œ8ä¸ªv100 gpusï¼Œè®­ç»ƒ10å¤©â€¦â€¦</li><li>datasetsï¼š<ol type="1"><li>Cityscapesï¼šè®­ç»ƒDeepLabV3ç½‘ç»œæ¥è·å¾—æ‰€æœ‰çš„è¯­ä¹‰åˆ†å‰²maskï¼Œç”¨FlowNet2çš„ç»“æœä½œä¸ºoptical flowçš„ground truthï¼Œç”¨Mask R- CNNçš„ç»“æœä½œä¸ºinstance- level è¯­ä¹‰maskçš„gt</li><li>Apolloscape</li><li>Face video datasetï¼šFaceForensics dataseté‡Œçš„çœŸå®è§†é¢‘</li><li>Dance video datasetï¼šä»YouTubeä¸‹è½½çš„è·³èˆè§†é¢‘ğŸ’ƒ</li></ol></li></ol><h3 id="ç»“æœ">ç»“æœ</h3><p>å›¾åƒç”Ÿæˆæ¨¡å‹pix2pixHDå’Œè§†é¢‘é£æ ¼è¿ç§»æ¨¡å‹COVSTä½œä¸ºbaselineã€‚FIDï¼ˆè®ºæ–‡å®šä¹‰çš„è§†é¢‘å˜ç§ï¼‰è·Ÿbaselineæ¯”ç•¥å¥½ï¼Œä½†human preference scoreï¼ˆè®ºæ–‡å®šä¹‰çš„ç”±äººæ¥æ‰“åˆ†ï¼Œè¯„ä¼°å“ªä¸ªè§†é¢‘æ›´çœŸå®ï¼‰é«˜å¾ˆå¤šã€‚</p><p>é€šè¿‡æ›´æ”¹è¯­ä¹‰maskï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆè§†é¢‘ä¸­çš„ç‰©ä½“ç§ç±»ï¼›é€šè¿‡æ›´æ”¹ç‰¹å¾å‘é‡ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆè§†é¢‘ä¸­çš„ç‰©ä½“å¤–è§‚ï¼›åœ¨æœªæ¥è§†é¢‘é¢„æµ‹ä¸Šä¹Ÿæœ‰å¾ˆå¥½çš„æ€§èƒ½ã€‚</p><h3 id="å±€é™æ€§">å±€é™æ€§</h3><ol type="1"><li>ç¼ºå°‘ä¸€ä¸ªç‰©ä½“å†…éƒ¨çš„å…·ä½“ä¿¡æ¯ï¼Œç”Ÿæˆè½¬å¼¯çš„è½¦çš„æ—¶å€™æ•ˆæœæ¯”è¾ƒå·®ã€‚è®ºæ–‡æå‡ºæˆ–è®¸å¯ä»¥é€šè¿‡å¢åŠ 3Dä¿¡æ¯ä½œä¸ºè¾“å…¥æ¥è§£å†³</li><li>åœ¨æ•´ä¸ªè§†é¢‘ä¸­ï¼Œä¸€ä¸ªç‰©ä½“çš„å¤–è§‚æœ‰æ—¶å€™è¿˜æ˜¯å‰åä¸ä¸€è‡´</li><li>å¶ç„¶æƒ…å†µä¸‹ï¼Œä¸€è¾†è½¦çš„é¢œè‰²å¯èƒ½ä¼šé€æ¸å‘ç”Ÿå˜åŒ–</li><li>å½“é€šè¿‡æ›´æ”¹è¯­ä¹‰ä¿¡æ¯æ¥æ“çºµè§†é¢‘ç”Ÿæˆçš„æ—¶å€™ï¼Œä¾‹å¦‚æŠŠæ ‘æ”¹æˆæˆ¿å­ï¼Œå¶ç„¶ä¼šå‡ºç°ä¸€éƒ¨åˆ†å˜æˆæˆ¿å­ï¼Œå¦ä¸€éƒ¨åˆ†æ ‘å˜äº†å½¢çš„æƒ…å†µï¼ˆï¼Ÿæ˜¯è¿™ä¸ªæ„æ€å—ï¼‰ã€‚è¿™æˆ–è®¸å¯ä»¥é€šè¿‡é‡‡ç”¨æ›´ç²—ç³™çš„è¯­ä¹‰æ ‡ç­¾çš„æ–¹å¼è§£å†³ï¼Œå› ä¸ºè¿™æ ·æ¨¡å‹å°±ä¸ä¼šå¯¹æ ‡ç­¾å½¢çŠ¶è¿‡äºæ•æ„Ÿ</li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Video Generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Few-shot Video-to-Video Synthesis</title>
      <link href="/blog/Reading-Few-shot-Video-to-Video-Synthesis/"/>
      <url>/blog/Reading-Few-shot-Video-to-Video-Synthesis/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttp://arxiv.org/abs/1910.12713</p><p>ä½œè€…ï¼š<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Andrew Tao, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>å‘è¡¨ï¼š NeurIPS 2019</p><p>Projectï¼š https://nvlabs.github.io/few-shot-vid2vid</p><p>Githubï¼šhttps://github.com/NVLabs/few-shot-vid2vid</p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p><p>é¦–å…ˆè¿™ä¸ªä»»åŠ¡é€‰é¢˜å¯¹æˆ‘æ¥è¯´å¾ˆæ–°ï¼Œæˆ‘ä¹‹å‰éƒ½æ²¡æœ‰æ„è¯†åˆ°è¿‡è¿™æ–¹é¢çš„é—®é¢˜ã€‚å¦‚æœå‘Šè¯‰æˆ‘æœ‰è¿™æ ·çš„é—®é¢˜ï¼Œéœ€è¦å»è§£å†³çš„è¯ï¼Œæˆ‘çš„ç›´è§‚çš„æƒ³æ³•ä¼šå—åˆ°è¿™ç¯‡è®ºæ–‡ä½œè€…çš„ä¸Šä¸€ç¯‡ä¸­æåˆ°çš„ ç‰¹å¾åµŒå…¥æ–¹æ³• æ‰€å½±å“ï¼šä¼šæƒ³ä¹Ÿé€šè¿‡å°†ä¸€ç±»ç‰©ä½“çš„ç‰¹å¾ç¼–ç èµ·æ¥ï¼Œç„¶åé€šè¿‡å­¦ä¹ ä¸åŒä¸ªä½“çš„ç‰¹å¾ç¼–ç ï¼Œæ¥å®ç°ä¸åŒé£æ ¼çš„è§†é¢‘ç”Ÿæˆã€‚</p></blockquote><h2 id="why">Whyï¼š</h2><p>å½“ä»Švid2vidæ–¹æ³•çš„ä¸¤ä¸ªå±€é™æ€§ï¼š</p><ol type="1"><li>éœ€è¦å¤§é‡æ•°æ®ï¼Œå°¤å…¶æ˜¯éœ€è¦ç”Ÿæˆçš„è¿™ä¸ªäººçš„è§†é¢‘æ•°æ®</li><li>æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œæ¯”å¦‚è¯´åªèƒ½åœ¨è®­ç»ƒé›†ä¸­åŒ…å«çš„äººä¸Šç”Ÿæˆæ–°çš„pose-to-humanè§†é¢‘ï¼Œä¸èƒ½æ³›åŒ–åˆ°è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„äººä¸Š</li></ol><p>æ‰€ä»¥è¿™ç¯‡è®ºæ–‡å°±æ˜¯æƒ³è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚</p><h2 id="what">Whatï¼š</h2><ol type="1"><li><p>ä»»åŠ¡æ˜¯Video-to-video synthesisï¼Œå³åˆ©ç”¨è¾“å…¥çš„è¯­ä¹‰è§†é¢‘ï¼ˆä¾‹å¦‚äººçš„å§¿åŠ¿ã€è¡—æ™¯ï¼‰ï¼Œç”Ÿæˆå†™å®çš„è§†é¢‘ã€‚ä¾‹å¦‚è¯´ï¼Œäººä½“å§¿åŠ¿ç”Ÿæˆçš„ä»»åŠ¡ï¼Œå°±æ˜¯é¦–å…ˆæ”¶é›†ä¸€ä¸ªäººåšå¤§é‡ä¸åŒåŠ¨ä½œçš„è§†é¢‘ï¼Œä½œä¸ºè®­ç»ƒé›†ï¼›ç„¶åå‘æ¨¡å‹ä¸­è¾“å…¥åŠ¨ä½œåºåˆ—ï¼Œè®©æ¨¡å‹ç”Ÿæˆè¯¥äººåšè¯¥åŠ¨ä½œçš„è§†é¢‘ã€‚å†æ¯”å¦‚è¡—æ™¯ç”Ÿæˆï¼Œä¹Ÿæ˜¯ä»¥å¤§é‡è¡—æ™¯è§†é¢‘ä½œä¸ºè®­ç»ƒé›†ï¼Œç„¶åå‘æ¨¡å‹ä¸­è¾“å…¥è¯­ä¹‰maskåºåˆ—ï¼Œè®©å®ƒç”Ÿæˆé£æ ¼ç±»ä¼¼çš„å…¨æ–°è¡—æ™¯ã€‚</p></li><li><p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç½‘ç»œï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªç½‘ç»œæƒé‡ç”Ÿæˆæ¨¡å—ï¼ˆnovel network weight generation moduleï¼‰å’Œattentionæœºåˆ¶</p></li><li><p>è¿™ä¸ªæ–¹æ³•çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œåªéœ€è¦åœ¨æµ‹è¯•æ—¶ï¼Œå‘æ¨¡å‹æä¾›å°‘é‡çš„åœ¨è®­ç»ƒé›†ä¸­æ²¡å‡ºç°è¿‡çš„æ–°çš„äººç‰©çš„å›¾åƒï¼Œå®ƒå°±èƒ½ç”Ÿæˆè¿™ä¸ªæ–°çš„äººçš„è§†é¢‘ã€‚</p><figure><img src="https://s2.loli.net/2022/08/10/IUKDziV4AtOwsxu.png" alt="image-20220513171420180" /><figcaption>image-20220513171420180</figcaption></figure><p>ä¸Šå›¾ä¸­ï¼Œå·¦è¾¹æ˜¯ç°å­˜æ–¹æ³•ï¼Œå®ƒä»¬åŸºæœ¬ä¸Šå¯¹äºæ¯ä¸ªäººï¼Œéƒ½éœ€è¦åœ¨å•ç‹¬çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒã€‚å³è¾¹æ˜¯è¿™ç¯‡è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼Œåªéœ€è¦è®­ç»ƒä¸€æ¬¡ï¼Œç„¶åè¾“å…¥ä¸€äº›ç¤ºèŒƒå›¾åƒï¼Œå°±å¯ä»¥æ³›åŒ–åˆ°æ–°çš„äººä¸Šã€‚</p></li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>è¯´æ˜¯åˆ©ç”¨å°‘é‡çš„æ–°çš„äººç‰©çš„ç¤ºèŒƒå›¾åƒï¼Œç”Ÿæˆç½‘ç»œæƒé‡ã€‚æ„æ€æ˜¯ä»¥åŸæœ¬çš„vid2vidç½‘ç»œçš„æƒé‡ä½œä¸ºè¾“å‡ºï¼Ÿä¸ºä»€ä¹ˆï¼Ÿæˆ‘çš„æ›´ç›´è§‚çš„æƒ³æ³•æ˜¯ï¼Œç›´æ¥ç”¨ä¸€ä¸ªæ–°ç½‘ç»œï¼Œå­¦ä¹ æ–°äººç‰©çš„å›¾åƒï¼Œç„¶åæŠŠoutputç»™concatæˆ–è€…åŠ è¿›æ—§ç½‘ç»œçš„outputä¸­â€¦â€¦å¦å¤–ï¼Œç›´æ¥ä½œç”¨äºç½‘ç»œæƒé‡ä¸Šï¼Œåœ¨æˆ‘çš„ç²—æµ…ç†è§£ä¸­ï¼Œä¼šä¸ä¼šé€ æˆä¿¡æ¯çš„æŸå¤±å‘¢ï¼Ÿè¿˜æ˜¯è¯´æœ¬è´¨ä¸Šæ²¡å·®ï¼Ÿ related worké‡Œæåˆ°è¿™ç±»ç½‘ç»œå±äºadaptive networkï¼Œè·Ÿå¸¸è§„ç½‘ç»œç›¸æ¯”æœ‰ä¸åŒçš„inductive biasï¼ˆæƒ³æƒ³ä¹Ÿæ˜¯ï¼‰ï¼Œæœ‰å¯¹åº”çš„åº”ç”¨ä»»åŠ¡ã€‚æˆ–è®¸æˆ‘ä¹‹åå†äº†è§£ä¸€ä¸‹è¿™å—ã€‚</li><li>æ ‡é¢˜ä¸­çš„few-shotæ˜¯ä»€ä¹ˆæ„æ€ï¼Œå°±æ˜¯æŒ‡æ›´å°‘çš„dataã€æ›´é«˜çš„æ³›åŒ–æ€§å—ï¼Ÿè¿™æ˜¯ä¸€ç±»ä»»åŠ¡å§ï¼Œä»å°‘é‡æ ‡æ³¨çš„æ ·æœ¬ä¸­å­¦ä¹ çš„æ„æ€ã€‚è¿™ä¸ªç¡®å®å°±æ˜¯å•Šï¼Œåªéœ€è¦ä¸€ç‚¹ç‚¹ç¤ºèŒƒå›¾åƒï¼Œå°±å¯ä»¥ç”Ÿæˆå›¾ä¸­è¿™ä¸ªäºº/ç‰©çš„æ–°videoã€‚</li><li>ä½œè€…çš„ä¸Šç¯‡è®ºæ–‡æ˜¯åˆ©ç”¨ganï¼Œè¿™ç¯‡åˆç”¨ä¸Šäº†attentionï¼Œä¸ºä»€ä¹ˆä½œå‡ºè¿™æ ·æœ¬è´¨çš„æ”¹å˜å‘¢ï¼Ÿ</li></ol><h2 id="how">Howï¼š</h2><ol type="1"><li>è§†é¢‘ç”Ÿæˆä»»åŠ¡å¯ä»¥åˆ†æˆ3ç±»ï¼š<ol type="1"><li>unconditional synthesisï¼šéšæœºç”Ÿæˆè§†é¢‘ç‰‡æ®µ</li><li>future video prediction</li><li>vid2vidï¼šæŠŠè¯­ä¹‰è¾“å…¥è½¬å˜æˆç°å®é£çš„è§†é¢‘ã€‚è¿™ç¯‡è®ºæ–‡å°±æ˜¯å±äºè¿™ä¸ªä»»åŠ¡ï¼Œä¸è¿‡å®ƒèšç„¦çš„ç‚¹åœ¨äºfew shotï¼Œå³é€šè¿‡åœ¨æµ‹è¯•çš„æ—¶å€™è¾“å…¥å°‘é‡å›¾åƒï¼Œè®©ç”Ÿæˆçš„è§†é¢‘å¯ä»¥æ³›åŒ–åˆ°æ²¡è§è¿‡çš„domainä¸Š</li></ol></li><li><p>vid2vidæ˜¯å‰ä¸€ç¯‡å·¥ä½œçš„å†…å®¹å•¦ã€‚referenceï¼š<a href="jyzhu.top/reading-video-to-video-synthesis"><em>Reading vid2vid</em></a></p></li><li><p>few-shotæœ¬è´¨ä¸Šå°±æ˜¯å¤šåŠ äº†ä¸ªç½‘ç»œEï¼Œç”¨æ¥ç”ŸæˆåŸè¡¥æ´ç½‘ç»œHçš„æƒé‡ã€‚è‡³äºåŸæœ¬è¿˜æœ‰ä¸¤ä¸ªç½‘ç»œWå’ŒMï¼Œä»–ä»¬éƒ½ä¸éœ€è¦æ”¹åŠ¨ï¼Œå› ä¸ºä»–ä»¬éƒ½æ˜¯åŸºäºä¸Šä¸€å¸§ç”Ÿæˆçš„å›¾åƒè¿›è¡Œå˜å½¢çš„ï¼Œä»£è¡¨ä¸€ç§è¿åŠ¨ï¼Œè€Œå’Œè§†é¢‘æœ¬è´¨çš„å†…å®¹æ²¡æœ‰å…³ç³»ã€‚</p></li><li><p>ç²¾é«“ä¸€å›¾ï¼š</p><figure><img src="https://s2.loli.net/2022/08/10/T6is35cjyHlvaNe.png" alt="image-20220810193845710" /><figcaption>image-20220810193845710</figcaption></figure></li><li><p>ç”¨æœ€æ–°çš„SOTAè¯­ä¹‰å›¾åƒç”Ÿæˆæ¨¡å‹SPADEä»£æ›¿äº†ä¸Šä¸€ç¯‡è®ºæ–‡ä¸­çš„ç½‘ç»œHã€‚SPADEåŒ…å«several spatial modulation branches and a main image synthesis branchã€‚ä¸è¿‡ç½‘ç»œEåªæ›´æ–°SPADEæ¨¡å‹ä¸­çš„spatial modulation branchesçš„æƒé‡ï¼Œå› ä¸º1è¿™æ ·é‡æ¯”è¾ƒå°ï¼Œ2è¿™æ ·å¯ä»¥é¿å…ä¸€ä¸ªç›´æ¥ä»input imageåˆ°output imageçš„çŸ­è·¯ï¼ˆæˆ‘å°šæ²¡æœ‰æ·±ç©¶åŸå› ï¼‰ã€‚</p></li><li><p>æƒé‡ç”Ÿæˆæ¨¡å—Eã€‚</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Video </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Live demo of CodeTyping via Pythonanywhere</title>
      <link href="/blog/live-demo-of-code-typing-via-pythonanywhere/"/>
      <url>/blog/live-demo-of-code-typing-via-pythonanywhere/</url>
      
        <content type="html"><![CDATA[<figure><img src="https://user-images.githubusercontent.com/39082096/149967763-a9bb56c5-6411-4d86-90d3-f1e22845e2a8.png" alt="image" /><figcaption>image</figcaption></figure><p><a href="http://jyzhu.pythonanywhere.com/">Code Typing Practice</a> (or source code on Github: <a href="https://github.com/viridityzhu/code-typing">here</a>) is a tiny web page that I wrote for fun last semester, which is for me myself to practice code typing. It is a naive Django web app (with bugsğŸ¤ª). BUT! I find a service to deploy it lively today. That's what is worth noting down now.</p><span id="more"></span><h2 id="note">Note</h2><p>Initially I tried to deploy this demo on Vercel.com. But it was too troublesome coz it does not support Django by default. Thankfully, I found <a href="https://www.pythonanywhere.com/">pythonanywhere</a>, on which each user can deploy one web app without payment. What's the best is that it is really easy to deploy: it provides access to Bash console.</p><p>Two things to be noted:</p><ol type="1"><li>Every 3 months, I have to login into the pythonanywhere to extend my web app, otherwise it will be killed.</li><li>The bug cost most of my time is that in the <code>view.py</code> I had used relative path to the static code files. However, I should use absolute path, with adding <code>BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</code> ahead.</li></ol><h2 id="todo">TODO</h2><p>Actually, after learned the MERN frame this semester, I am now aware of how naive this project is. However, I love Python, so it doesn't matter if i still regard Django as a hobbyğŸ¤¨. Who knows... I haven't even spent my time on that course project...</p><p>Now that the live demo is achieved, I might think of polishing this little project a bit.</p><ul><li>[ ] Fix bugs. Though i've already forgotten what those bugs are...</li><li>[ ] Replace the stupid code snippets...</li><li>[ ] Add the feature to compute time cost and typing speed. Also, save typing records.</li><li>[ ] Explicitly support other kinds of typing materials, and also support uploading customize materials.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ä¸‰æœˆçš„äººç”Ÿä¸»é¢˜æ˜¯å¤æ‚æ€§</title>
      <link href="/blog/complexity-again/"/>
      <url>/blog/complexity-again/</url>
      
        <content type="html"><![CDATA[<blockquote><p>æˆ‘æ”¾å¼ƒç†è§£å¾ˆå¤šä¸œè¥¿</p><p>æˆ‘å¼€å§‹æ‹¥æŠ±</p><p>æƒŠäººçš„å¤æ‚æ€§</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&amp;id=7326559292&amp;auto=1&amp;height=430"></iframe><p>è¿™ä¸¤å¤©åˆ—äº†ä¸€ä¸ªåå«ã€Œç²¾ç¥çŠ¶å†µã€çš„æ­Œå•ï¼Œé¡¾åæ€ä¹‰ï¼Œé´é€‰äº†å¤§æŠµæ˜¯æœ€èƒ½æœ‰æ•ˆè¡¨è¾¾æˆ‘è¿‘æ¥ç²¾ç¥çŠ¶å†µçš„9é¦–æ­Œï¼ˆä¸å‡ºæ‰€æ–™ä»¥æ‘‡æ»šä¸ºä¸»ï¼‰ã€‚å¥‡å¦™çš„æ˜¯ï¼Œä¸€æ–¹é¢æˆ‘è‡ªè¯©å¬æ­Œæ—¶å¯¹æ­Œè¯ç”šæ˜¯é‡è§†ï¼Œå¦ä¸€æ–¹é¢æœ€çˆ±çš„æ­Œç«Ÿç„¶æ­Œè¯å«é‡ä¸è¶³50%ã€‚æƒ³æƒ³ç°å®ï¼Œæˆ–è®¸æ˜¯æˆ‘æ”¾å¼ƒç†è§£å¾ˆå¤šä¸œè¥¿ï¼ˆç”¨ç†æ€§ï¼‰ï¼Œå¼€å§‹æ‹¥æŠ±å¤æ‚æ€§äº†å§ï¼ˆç”¨æ„Ÿæ€§ï¼Ÿï¼‰ã€‚</p><p><img src="https://s2.loli.net/2022/03/14/W5bxdr9w3egqRGF.jpg" style="zoom: 33%;" /></p><hr /><h3 id="æœ€è¿‘éšæƒ³">æœ€è¿‘éšæƒ³ï¼š</h3><ul><li>ã€Œæ´»ç€å¾ˆç´¯ï¼Œåœ¨æ¼«é•¿çš„ç”Ÿå‘½ä¸­ï¼Œç§¯ç´¯çš„ç—›è‹¦ä¸æŠ˜ç£¨ä¼šå˜å¾—ç»µé•¿ã€‚ã€æ‰€ä»¥äººé•¿å¤§äº†å¼€å§‹å–é…’ï¼Œä¸€ééå–é…’ï¼Œåªæ˜¯å› ä¸ºç§¯å‹çš„ç—›è‹¦æ— æ³•æ¶ˆè§£å§ã€‚ä¸è¿‡è¿™æ ·çš„å¥½å¤„å°±æ˜¯ä¼šæœ‰ä¸€å¤©ä¸å†åƒå¹´è½»æ—¶å€™é‚£æ ·æ€•æ­»äº†</li><li>æˆ‘å¾ˆæ‚²æˆšï¼Œè¿™ä¸ªä¸–ç•Œçš„æ‚²æˆšåº•è‰²è¿‘å¹´ä¹Ÿé€æ¸æ˜¾éœ²ï¼Œçœ‹å¾—æµ…çš„äººä¹Ÿèƒ½ä¸¾ç›®å°±çœ‹è§ç°é»‘è‰²äº†ã€‚å¾ˆæ— æœ›ï¼Œå¹´å¤ä¸€å¹´åŸ‹å¤´æ´»ç€ï¼Œä¸€æŠ¬å¤´å°±æä¸æ¸…æ¥šåœ¨ç›¼ä»€ä¹ˆã€‚æƒ³åˆ°çˆ¶æ¯æ¸è€ï¼Œå¤§å°æ¯›ç—…æ¥è¿ä¸æ–­ï¼›å°±è¿æˆ‘è‡ªå·±éƒ½å¼€å§‹æ˜¾éœ²ä¸€äº›èº«ä½“ä¸å¥½çš„è¿¹è±¡ï¼Œå°±å®åœ¨æ˜¯éš¾è¿‡ã€‚æƒ³åˆ°æˆ‘çš„ç”Ÿæ´»ï¼Œæµ·å¤–ç•™å­¦ï¼Œä¸“ä¸šå­¦æœ¯ï¼Œä¸çˆ¶æ¯çš„ç”Ÿæ´»ï¼ŒæŸ´ç±³æ²¹ç›ï¼Œå®¶é•¿é‡ŒçŸ­ï¼Œæˆ‘ä»¬çš„ä¸–ç•Œæ˜¯å‰²è£‚çš„ï¼Œæˆ‘æ•´ä¸ªäººä¹Ÿæ„Ÿåˆ°ä¸€ç§å‰²è£‚ï¼Œå¾…åœ¨çˆ¶æ¯èº«è¾¹æˆ–è€…è¿œæ–¹ï¼Œéƒ½å¾ˆæ‚²å“€ã€‚è¿™ç§æ‚²å“€ç”šè‡³åªæ˜¯ç¨çºµå³é€çš„ï¼Œå®ƒå“ªæ€•èƒ½é•¿å­˜ä¸€äº›ï¼Œæˆ‘ä¹Ÿèƒ½å¯¹ç”Ÿæ´»ç¨å¤šäº›æŠŠæ¡ã€‚åªæ˜¯æ—¶é—´æ€»ä¼šè¿‡å¾—å¾ˆå¿«ï¼Œå¯é¢„è§çš„æœªæ¥è¿˜ä¼šé¢ä¸´å˜æ›´ï¼Œä¼´éšæ›´æ·±ã€æ›´æ— åŠ›çš„æ‚²å“€ï¼Œä¾‹å¦‚ä½œä¸ºä¸€ä¸ªæˆå¹´äººéœ€æ‰¿æ‹…çš„ä¸€ä¸ªå®¶åº­çš„å‹åŠ›ï¼Œä¾‹å¦‚é‡è¦çš„äººçš„è¡°è€ã€ç–¾ç—…ã€æ­»äº¡ã€‚</li><li>æˆ‘æœ‰æ—¶å€™å¯¹äººç”Ÿå¾ˆéšä¾¿ï¼Œç³Ÿç³•å¢ƒé‡çš„å‘ç”Ÿä¼šè¢«é’æ„Ÿè€Œå¿½ç•¥ï¼Œæˆ–è€…å¾ˆå¿«è§†è‹¥æ— ç¹ï¼Œä¾‹å¦‚ç–«æƒ…å’Œç–«æƒ…åçš„ä¸–ç•Œã€‚åªæ˜¯æ¯ä¸€ä»¶äº‹ï¼Œä¹Ÿä¸è‡³äºå¯¹æˆ‘å…¨æ— å½±å“ï¼Œæˆ‘çš„è’è¯æ„Ÿéšç€å®ƒä»¬ä¸€å±‚ä¸€å±‚åŠ æ·±ã€‚</li><li>æ˜¨å¤©å»å¤–å…¬å¢“ä¸ŠæŒ‚ç¤¾äº†ï¼Œå’Œå¦ˆå¦ˆã€å¤–å©†ä¸€èµ·ã€‚å¤–å©†è·Ÿå¤–å…¬è¯´å¾ˆå¤šè¯ï¼Œé—®ä»–è¿™é‡Œé£æ™¯å¥½ä¸å¥½ï¼Œæœ‰æ²¡æœ‰å»å“ªé‡Œé’“é±¼ã€‚å¦ˆå¦ˆå·å·æŠ¹äº†å‡ æ¬¡çœ¼æ³ªã€‚æˆ‘æœ€æ²¡ç”¨ï¼Œçœ¼æ³ªå¤§é¢—å¤§é¢—æ»´åœ¨çº¸é’±ä¸Šï¼Œéƒ½ä¸å¥½çƒ§äº†ã€‚å¯æ˜¯æˆ‘çœ‹è§å¢“ç¢‘ä¸Šå¤–å…¬çš„åå­—ï¼Œä»–ç¬‘å®¹ç¿çƒ‚çš„å½©è‰²ç…§ç‰‡ï¼Œåªæ˜¯å¾ˆæƒ³å¿µå¾ˆæƒ³å¿µã€‚å¦ˆå¦ˆè®©æˆ‘ä½œæ–çš„æ—¶å€™å¯ä»¥å¿ƒé‡Œè·Ÿå¤–å…¬è¯´ä¸€äº›è¯ï¼Œå‘Šè¯‰ä»–ä¸è¦æ‹…å¿ƒã€è¯·ä»–ä¿ä½‘ã€‚å¯æ˜¯æˆ‘ä¸€è¾¹ä½œæ–ä¸€è¾¹å¿ƒé‡Œåªæœ‰ä¸€ä¸ªå£°éŸ³ï¼šå°•å…¬å•Š å°•å…¬è¯¶[æ³ª]</li></ul>]]></content>
      
      
      <categories>
          
          <category> thoughts </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>å¤æ‚æ€§</title>
      <link href="/blog/complexity/"/>
      <url>/blog/complexity/</url>
      
        <content type="html"><![CDATA[<p>æˆ‘æ”¾å¼ƒç†è§£å¾ˆå¤šä¸œè¥¿</p><p>æˆ‘å¼€å§‹æ‹¥æŠ±</p><p>æƒŠäººçš„å¤æ‚æ€§</p>]]></content>
      
      
      <categories>
          
          <category> poems </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Reading Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation</title>
      <link href="/blog/Reading-Pixel2meshPP/"/>
      <url>/blog/Reading-Pixel2meshPP/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1908.01491</p><p>ä½œè€…ï¼šChao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu</p><p>å‘è¡¨ï¼š ICCV 2019</p><p>é“¾æ¥ï¼š https://arxiv.org/abs/1908.01491</p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><h2 id="why">Whyï¼š</h2><ol type="1"><li>å•è§†è§’å›¾åƒ3Dé‡å»ºæ¨¡çš„æ•ˆæœä¸å¤Ÿå¥½ï¼Œå°¤å…¶æ˜¯èƒŒé¢ï¼Œè€Œä¸”æ³›åŒ–èƒ½åŠ›ä¹Ÿå·®ã€‚</li><li>æ‰€ä»¥å¢åŠ å¤šä¸ªè§†è§’çš„å›¾åƒï¼šæ›´å¤šè§†è§‰ä¿¡æ¯ï¼Œä¸”æœ‰å·²ç»å®šä¹‰å¾—å¾ˆå¥½çš„ä¼ ç»Ÿæ–¹æ³•ã€‚</li><li>ä½†æ˜¯ä¼ ç»Ÿæ–¹æ³•éœ€è¦æ›´å¤§é‡çš„å›¾åƒï¼›è¿™æ—¶å€™æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥éšå¼ç¼–ç è§†è§’é—´çš„å…³è”ï¼Œå°±æ´¾ä¸Šäº†ç”¨åœºã€‚</li><li>å¾ˆæœ‰ç”¨ï¼Œä½†æ˜¯æ¬ ç ”ç©¶ã€‚</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>åˆ©ç”¨å¤šè§†è§’å›¾åƒï¼Œå›ºå®šç›¸æœºposeå‚æ•°ï¼Œåˆ©ç”¨GCNï¼Œä»ç²—ç³™é€æ¸ç²¾ç»†åœ°å˜å½¢ï¼Œç”Ÿæˆ3D meshé‡å»ºæ¨¡ã€‚</li><li>é‡‡æ ·meshæ¨¡å‹é¡¶ç‚¹å‘¨å›´çš„åŒºåŸŸï¼Œåˆ©ç”¨perceptual featureæ¨ç†å‡ºå¯¹meshçš„å½¢å˜ã€‚</li><li>å¯¹äºä¸åŒç§ç±»çš„ç‰©ä½“æ³›åŒ–èƒ½åŠ›å¾ˆå¥½ã€‚</li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>ä¼¼ä¹å°±æ˜¯ç»™pixel2meshåŠ ä¸Šä¸€å±‚å£³ï¼Œåº”ç”¨åœ¨å¤šè§†è§’å›¾åƒä¸ŠğŸ¤”é‚£ä¹ˆè¿™é‡Œçš„åˆ›æ–°ç‚¹æœ¬è´¨åœ¨å“ªå„¿å‘¢ï¼Ÿ</li><li>GCNç”¨å¤„å¤§å—ï¼Œä¸ºä»€ä¹ˆç”¨å®ƒï¼Ÿ</li><li>é‡‡æ ·meshé¡¶ç‚¹å‘¨å›´çš„åŒºåŸŸï¼Œæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</li><li>å¤šè§†è§’3Dé‡å»ºï¼Œåº”ç”¨çœŸçš„å¹¿æ³›å—ï¼Ÿ</li></ol><h2 id="how">Howï¼š</h2><ol type="1"><li>å¤šè§†è§’å˜å½¢ç½‘ç»œ Multi-View Deformation Network (MDN)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> 3D Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Self-supervised Single-view 3D Reconstruction via Semantic Consistency</title>
      <link href="/blog/Reading-Self-supervised-Single-view-3D-Reconstruction-via-Semantic-Consistency/"/>
      <url>/blog/Reading-Self-supervised-Single-view-3D-Reconstruction-via-Semantic-Consistency/</url>
      
        <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2003.06473</p><p>ä½œè€…ï¼šXueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz</p><p>å‘è¡¨ï¼š ECCV 2020</p><p>é“¾æ¥ï¼š https://github.com/NVlabs/UMR</p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><h2 id="why">Whyï¼š</h2><ol type="1"><li>åœ¨3dé‡å»ºæ¨¡ä»»åŠ¡ä¸­ï¼ŒåŒæ—¶é¢„æµ‹å½¢çŠ¶ã€ç›¸æœºä½ç½®å’Œæè´¨æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒå†…åœ¨çš„ä¸ç¡®å®šæ€§ã€‚</li><li>ç°æœ‰æ–¹æ³•éƒ½éœ€è¦å€ŸåŠ©å„ç§æ‰‹æ®µï¼š3Då±‚é¢çš„ç›‘ç£ã€2Dè¯­ä¹‰å…³é”®ç‚¹ã€shadingï¼ˆè¿™æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ã€ç‰¹å®šç±»åˆ«çš„3D template ã€å¤šè§†è§’ç­‰ç­‰ã€‚è¿™äº›æ–¹æ³•éœ€è¦å¤§é‡äººåŠ›ï¼Œæ‰€ä»¥å¾ˆéš¾å¹¿æ³›åº”ç”¨ã€‚</li><li>äººç±»ä¼šç›´è§‰æ„ŸçŸ¥åˆ°ä¸€ä¸ªç‰©ä½“åŒ…æ‹¬å„ä¸ªéƒ¨åˆ†ï¼Œæ¯”å¦‚é¸Ÿæœ‰ä¸¤åªè…¿ã€ä¸¤ä¸ªç¿…è†€ã€ä¸€ä¸ªå¤´ï¼Œä»è€Œè¯†åˆ«ç‰©ä½“ã€‚ç±»ä¼¼çš„ï¼Œcvå—æ­¤å¯å‘ï¼Œä¹Ÿå¯ä»¥å°†ä¸€ä¸ªç‰©ä½“å®šä¹‰ä¸ºå¤šä¸ªå¯å˜å½¢çš„éƒ¨åˆ†çš„é›†åˆã€‚</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>ä»…éœ€è¦å•å¼ å›¾ç‰‡+è½®å»“maskï¼Œåˆ©ç”¨è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå®ç°è‡ªç›‘ç£3Dé‡å»ºæ¨¡</li><li>æ€è·¯ï¼š1. æ¯ä¸ªç‰©ä½“å¯ä»¥çœ‹ä½œç”±å¯å˜å½¢çš„éƒ¨åˆ†ç»„æˆï¼›2. å¯¹åŒä¸€ç±»å‹çš„ä¸åŒç‰©ä½“ï¼Œå®ƒä»¬çš„æ¯ä¸€éƒ¨åˆ†åœ¨è¯­ä¹‰ä¸Šéƒ½æ˜¯ä¸€è‡´çš„</li><li>é€šè¿‡è‡ªç›‘ç£å­¦ä¹ å¤§é‡åŒç±»çš„å›¾ç‰‡ï¼Œå¯ä»¥å»ºç«‹é‡å»ºçš„meshæ¨¡å‹ä¸å›¾ç‰‡ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™æ ·åœ¨åŒæ—¶é¢„æµ‹å½¢çŠ¶ã€ç›¸æœºä½ç½®å’Œæè´¨çš„æ—¶å€™ï¼Œå¯ä»¥é™ä½æ¨¡ç³Šæ€§ã€‚</li><li>ç¬¬ä¸€ä¸ªåšåˆ°ä¸éœ€è¦ç‰¹å®šç±»åˆ«çš„template meshæ¨¡å‹æˆ–è€…è¯­ä¹‰å…³é”®ç‚¹ï¼Œå°±å¯ä»¥ä»å•è§†è§’å›¾åƒä¸­å®ç°3dé‡å»ºæ¨¡ã€‚å› æ­¤ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å„ç§ç‰©ä½“ç±»åˆ«ï¼Œè€Œä¸éœ€è¦ç±»åˆ«çš„æ ‡ç­¾</li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><h2 id="how">Howï¼š</h2><h3 id="æ¨¡å‹">æ¨¡å‹</h3><figure><img src="https://s2.loli.net/2022/01/27/x2BKozeFVpgUPkE.png" alt="image-20220122145159190" /><figcaption>image-20220122145159190</figcaption></figure><ol type="1"><li>ï¼ˆaï¼‰æ˜¯åŸå§‹å›¾ç‰‡ã€‚éœ€è¦åŒä¸€ç±»åˆ«çš„å¤§é‡å›¾ç‰‡ä¸€èµ·ä½œä¸ºè¾“å…¥</li><li>ï¼ˆbï¼‰ç”¨SCOPSæ¨¡å‹ï¼ˆå¦ä¸€ç¯‡å·¥ä½œï¼‰ï¼Œå¯¹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„ç»“æœã€‚è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯è‡ªç›‘ç£çš„</li><li>ï¼ˆcï¼‰æ ‡å‡†è¯­ä¹‰ uv mapï¼ˆCanonical semantic uv mapï¼‰ï¼š<ol type="1"><li>ç†è®ºä¸Šï¼ŒåŒä¸€ç±»ç‰©ä½“çš„meshæ¨¡å‹ï¼Œå°½ç®¡å„è‡ªéƒ½æœ‰ä¸åŒçš„å½¢çŠ¶ï¼Œä½†æ¯ä¸ªç‚¹çš„è¯­ä¹‰å«ä¹‰éƒ½æ˜¯ä¸€è‡´çš„ã€‚</li><li>å› æ­¤ï¼Œæ ¹æ®å‰ä¸€æ­¥ç”Ÿæˆçš„å¤§é‡è¯­ä¹‰åˆ†å‰²ç»“æœï¼Œå¯ä»¥ç”Ÿæˆä¸€å¼ å¯¹åº”è¿™ä¸ªç±»åˆ«çš„Canonicalè¯­ä¹‰uv mapã€‚</li></ol></li><li>ï¼ˆdï¼‰ç”±å‰ä¸€æ­¥ç”Ÿæˆçš„Canonicalè¯­ä¹‰uv mapï¼Œå¯ä»¥å¾—åˆ°é‡å»ºçš„meshæ¨¡å‹è¡¨é¢çš„ç‚¹å¯¹åº”çš„è¯­ä¹‰æ ‡ç­¾</li><li>æ©˜è‰²ç®­å¤´ï¼šè¿™ä¸ªå°±æ˜¯è¯­ä¹‰ä¸€è‡´æ€§äº†ï¼Œå®ƒé¼“åŠ±2Då›¾åƒå’Œ3Dæ¨¡å‹ä¹‹é—´çš„è¯­ä¹‰æ ‡ç­¾ç›¸äº’ä¸€è‡´ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥è§£å†³å‰é¢æåˆ°è¿‡çš„åœ¨3Dé‡å»ºæ¨¡çš„æ—¶å€™çš„â€œç›¸æœº-å½¢çŠ¶ä¸ç¡®å®šæ€§â€è¿™ä¸ªéš¾é¢˜</li></ol><h3 id="å…·ä½“æ–¹æ³•">å…·ä½“æ–¹æ³•</h3><h4 id="cmræ˜¯baseline">CMRæ˜¯baseline</h4><ol type="1"><li>ç”¨ä¸‰ä¸ªdecoder <span class="math inline">\(D_{shape}\ D_{camera}\ D_{texture}\)</span> åŒæ—¶é¢„æµ‹meshæ¨¡å‹çš„å½¢çŠ¶ã€ç›¸æœºå’Œæè´¨<ol type="1"><li>å½¢çŠ¶ <span class="math inline">\(V=\tilde V + \Delta V\)</span>ï¼Œå…¶ä¸­ $V $ æ˜¯æŸç±»ç‰©ä½“çš„template meshæ¨¡å‹ï¼Œ<span class="math inline">\(\Delta V\)</span> æ˜¯é¢„æµ‹å‡ºæ¥çš„ç‚¹çš„åç§»é‡</li><li>ç›¸æœºpose <span class="math inline">\(\theta\)</span> æ˜¯ weak perspective transformation ï¼ˆï¼Ÿï¼‰</li><li>æè´¨ <span class="math inline">\(I_{flow}\)</span> æ˜¯ UV flowï¼Œæ˜¯å°†è¾“å…¥å›¾ç‰‡åˆ°UVç©ºé—´çš„æ˜ å°„ï¼Œç„¶åå®ƒå¯ä»¥è¢«ä¸€ä¸ªå·²ç»å®šä¹‰å¥½çš„å‡½æ•°<span class="math inline">\(\phi\)</span>æ˜ å°„åˆ°meshæ¨¡å‹çš„è¡¨é¢çš„æ¯ä¸€ä¸ªç‚¹</li></ol></li><li>ä½†æ˜¯CMRéœ€è¦äººå·¥æ ‡æ³¨çš„å…³é”®ç‚¹ä½œä¸ºè¾“å…¥ï¼Œè¿™ç¯‡è®ºæ–‡ä¸»è¦å°±æ˜¯æŠŠå®ƒå»æ‰äº†ã€‚å»æ‰ä¹‹åå‘¢ï¼Œä¼šå‡ºç°ç›¸æœº+å½¢çŠ¶åŒæ—¶é¢„æµ‹æ—¶Ambiguityçš„é—®é¢˜ï¼Œæ‰€ä»¥å°±æƒ³æ–¹è®¾æ³•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li></ol><figure><img src="https://s2.loli.net/2022/01/27/TkWsiJxMN38tKGu.png" alt="image-20220122160143711" /><figcaption>image-20220122160143711</figcaption></figure><h4 id="è¯­ä¹‰ä¸€è‡´æ€§è§£å†³ç›¸æœºå½¢çŠ¶åŒæ—¶é¢„æµ‹æ—¶çš„ambiguity">è¯­ä¹‰ä¸€è‡´æ€§ï¼šè§£å†³ç›¸æœº+å½¢çŠ¶åŒæ—¶é¢„æµ‹æ—¶çš„Ambiguity</h4><p>ä¹Ÿå°±æ˜¯Fig 3ä¸­çº¢è‰²æ¡†çš„éƒ¨åˆ†ã€‚</p><ol type="1"><li><p><em>è¯­ä¹‰éƒ¨ä»¶ä¸å˜æ€§ semantic part invarianceï¼š</em></p><ol type="1"><li>å¯¹äº2Då›¾åƒï¼Œç”¨SCOPSï¼ˆè‡ªç›‘ç£co-partè¯­ä¹‰åˆ†å‰²ï¼Œå¦ä¸€ç¯‡è®ºæ–‡çš„æ–¹æ³•ï¼‰å¯ä»¥å¾ˆå‡†ç¡®åœ°å¯¹ç‰©ä½“å„ä¸ªéƒ¨åˆ†è¿›è¡Œåˆ†å‰²</li><li>å¯¹äº3D meshï¼Œæ¯ä¸ªç‚¹çš„è¯­ä¹‰å«ä¹‰æ˜¯å›ºå®šä¸å˜çš„ï¼Œå°±ç®—æ¯ä¸ªç‰©ä½“ä¼šæœ‰å„è‡ªçš„å½¢å˜</li></ol></li><li><p><em>è¯­ä¹‰ä¸€è‡´æ€§</em>ï¼š</p><ol type="1"><li><p><img src="https://s2.loli.net/2022/01/27/TzCkKA462gRbOFa.png" alt="image-20220124121618081" style="zoom:50%;" /></p><p>ä»Fig 4 (i) å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæ²¡æœ‰è¯­ä¹‰ä¸€è‡´æ€§ï¼Œmeshæ¨¡å‹ä¸­åŸæœ¬å¯¹åº”å¤´çš„é¡¶ç‚¹è¢«å½“ä½œäº†ç¿…è†€å°–ï¼Œè¿™æ ·é”™è¯¯çš„å˜å½¢å¯¹åº”äº†é”™è¯¯çš„ç›¸æœºposeã€‚è¿™å°±æ˜¯ç›¸æœº+å½¢çŠ¶åŒæ—¶é¢„æµ‹æ—¶çš„Ambiguityã€‚</p></li><li><p>å‰é¢å·²ç»æåˆ°è¿‡ï¼Œå¯ä»¥ä¸ºæ¯ä¸ªå…·ä½“ç±»åˆ«ç”Ÿæˆä¸€å¼ æ ‡å‡†è¯­ä¹‰ uv mapï¼ˆCanonical semantic uv mapï¼‰ã€‚è¿™é‡Œï¼Œå°±å¯ä»¥è®© æ¯ä¸ªç‰©ä½“çš„2Dè¯­ä¹‰åˆ†å‰²ç»“æœ ä¸ æ ‡å‡†è¯­ä¹‰ uv map ä¿æŒä¸€è‡´æ€§ï¼Œä»è€Œè®©3Dæ¨¡å‹çš„æ¯ä¸ªè¯­ä¹‰éƒ¨ä»¶è·Ÿ2Då›¾åƒé‡Œç›¸åº”çš„ä½ç½®æœ‰å¯¹åº”å…³ç³»ã€‚è¿™æ ·å¯ä»¥å¾ˆå¥½åœ°è§£å†³ç›¸æœº-å½¢çŠ¶Ambiguityé—®é¢˜ã€‚</p></li></ol></li></ol><h5 id="é€šè¿‡scopså®ç°2då›¾åƒä¸­éƒ¨ä»¶çš„åˆ†å‰²">é€šè¿‡SCOPSå®ç°2Då›¾åƒä¸­éƒ¨ä»¶çš„åˆ†å‰²</h5><p><img src="https://s2.loli.net/2022/01/27/YFNpoKzfxkBRSbI.png" alt="image-20220124114332065" style="zoom:50%;" /></p><p>SCOPS æ˜¯è‡ªç›‘ç£çš„æ–¹æ³•ï¼Œä»ä¸€ç±»ç‰©ä½“çš„å¤§é‡å›¾ç‰‡ä¸­å‘æ˜å…±åŒçš„è¯­ä¹‰éƒ¨ä»¶ã€‚Fig 10ç¬¬äºŒè¡Œå°±æ˜¯å®ƒçš„ç»“æœã€‚åé¢è¿˜ä¼šæåˆ°ï¼Œé€šè¿‡æœ¬æ–‡çš„æ–¹æ³•ï¼Œè¿˜å¯ä»¥åè¿‡æ¥æå‡SCOPSçš„ç»“æœï¼šåˆ©ç”¨ç”Ÿæˆçš„æ ‡å‡†è¯­ä¹‰UV mapä½œä¸ºä¼ªæ ‡æ³¨åè¿‡æ¥è¿›è¡Œç›‘ç£ã€‚</p><h5 id="é€šè¿‡æ ‡å‡†è¯­ä¹‰uv-mapå®ç°3dæ¨¡å‹ä¸­éƒ¨ä»¶çš„åˆ†å‰²">é€šè¿‡æ ‡å‡†è¯­ä¹‰uv mapå®ç°3Dæ¨¡å‹ä¸­éƒ¨ä»¶çš„åˆ†å‰²</h5><ol type="1"><li><p>å·²ç»æœ‰äº†ï¼š</p><ol type="1"><li>æ¨¡å‹å­¦åˆ°çš„texture flow <span class="math inline">\(I_{flow}\)</span>å¯ä»¥å°†è¾“å…¥å›¾ç‰‡æ˜ å°„åˆ°UVç©ºé—´ï¼Œç„¶åå®ƒå¯ä»¥è¢«ä¸€ä¸ªå·²ç»å®šä¹‰å¥½çš„å‡½æ•°<span class="math inline">\(\phi\)</span>æ˜ å°„åˆ°meshæ¨¡å‹çš„è¡¨é¢çš„æ¯ä¸€ä¸ªç‚¹</li><li>é€šè¿‡SCOPSç”Ÿæˆçš„å›¾åƒ <span class="math inline">\(i\)</span> çš„è¯­ä¹‰åˆ†å‰²ç»“æœ <span class="math inline">\(P^i\in R^{H\times W\times N_p}\)</span>ï¼Œ å…¶ä¸­Hå’ŒWæ˜¯é•¿å’Œå®½ï¼Œ <span class="math inline">\(N_p\)</span> æ˜¯è¯­ä¹‰éƒ¨ä»¶æ•°é‡</li></ol></li><li><p>è¿™æ ·çš„è¯ï¼Œé€šè¿‡æ¨¡å‹çš„ <span class="math inline">\(I_{flow}\)</span> å°±å¯ä»¥æŠŠ2Dçš„è¯­ä¹‰åˆ†å‰²ç»“æœ <span class="math inline">\(P^i\)</span> æ˜ å°„åˆ° UV ç©ºé—´ï¼ŒæŠŠè¿™ä¸ªç§°ä¸º è¯­ä¹‰UV map</p></li><li><p>ç†è®ºä¸Šæ¥è¯´ï¼ŒåŒä¸€ç±»åˆ«çš„æ‰€æœ‰ç‰©ä½“éƒ½åº”è¯¥å¾—åˆ°åŒä¸€ä¸ªè¯­ä¹‰UV mapï¼Œå› ä¸º 1. æ ¹æ®è¯­ä¹‰éƒ¨ä»¶ä¸å˜æ€§ï¼Œmeshæ¨¡å‹çš„æ¯ä¸ªé¡¶ç‚¹å¯¹åº”çš„è¯­ä¹‰éƒ¨ä»¶éƒ½æ˜¯å›ºå®šä¸å˜çš„ 2. UV mapå’Œ3d mesh ä¸­çš„ç‚¹åˆæ˜¯é€šè¿‡<span class="math inline">\(\Phi\)</span>æ˜ å°„çš„å…³ç³»ï¼Œæ¯ä¸ªé¡¶ç‚¹å¯¹åº”çš„UV mapä¸Šçš„åæ ‡ä¹Ÿæ˜¯ä¸å˜çš„ã€‚</p></li><li><p>ä½†æ˜¯å› ä¸ºSCOPS + <span class="math inline">\(I_{flow}\)</span> çš„è¯¯å·®ï¼Œå„ä¸ªç‰©ä½“ç”Ÿæˆçš„è¯­ä¹‰UV mapäº‹å®ä¸Šå¾ˆä¸ä¸€æ ·ã€‚æ‰€ä»¥è¿™é‡Œæå‡ºäº†å¯¹ æ ‡å‡†è¯­ä¹‰UV map <span class="math inline">\(\bar P_{uv}\)</span> çš„ä¼°è®¡æ–¹æ³•ï¼š</p><ol type="1"><li><p>é€šè¿‡æŸç§æ–¹æ³•é€‰æ‹©å‡ºè®­ç»ƒé›†ä¸­æ•ˆæœæ¯”è¾ƒå¥½çš„å­é›† <span class="math inline">\(\mathcal{U}\)</span>ï¼Œå¯¹å®ƒä»¬çš„ç»“æœè¿›è¡ŒåŠ å’Œï¼Œ</p><p><strong>é€‰æ‹©æ ·æœ¬çš„æ–¹å¼</strong>ï¼š</p><ol type="1"><li>é¦–å…ˆé€‰æ‹©æœ€å¥½çš„é‚£ä¸€ä¸ªæ ·æœ¬ï¼Œå³ perceptual distanceï¼ˆ3DæŠ•å½±åˆ°2Dçš„å›¾åƒä¸åŸå§‹RGBå›¾åƒçš„çŸ¥è§‰è·ç¦»ï¼Ÿï¼‰æœ€å°çš„</li><li>ç„¶åé€‰æ‹©Kä¸ªè·Ÿè¿™ä¸ªæœ€å¥½çš„æ ·æœ¬æœ€æ¥è¿‘çš„æ ·æœ¬ï¼Œå³å®ƒä»¬çš„è¯­ä¹‰UV mapæœ€æ¥è¿‘</li></ol><p>å…¬å¼å¦‚ä¸‹ï¼š</p></li><li><p><span class="math display">\[\bar P_{uv}=\frac{1}{|\mathcal{U}|}\sum_{i\in \mathcal{U}}I^i_{flow}(P^i)\]</span></p><p>å…¶ä¸­ <span class="math inline">\(I^i_{flow}(P^i)\)</span> å°±æ˜¯é€šè¿‡ <span class="math inline">\(I_{flow}\)</span> æ˜ å°„è¯­ä¹‰åˆ†å‰²ç»“æœ <span class="math inline">\(P^i\)</span> å¾—åˆ°çš„ è¯­ä¹‰UV mapã€‚</p></li></ol></li></ol><h5 id="d-å’Œ-3d-é—´çš„è¯­ä¹‰ä¸€è‡´æ€§">2D å’Œ 3D é—´çš„è¯­ä¹‰ä¸€è‡´æ€§</h5><ol type="1"><li><p><em>åŸºäºæ¦‚ç‡çš„çº¦æŸ Probability-based constraint</em></p><ol type="1"><li><p><span class="math display">\[L_{sp}=||P^i-\mathcal{R}(\Phi (\bar P_{uv});\theta^i)||^2\]</span></p><p>æ ‡å‡†è¯­ä¹‰UV map <span class="math inline">\(\bar P_{uv}\)</span> ç”±é¢„å®šä¹‰å¥½çš„å‡½æ•° <span class="math inline">\(\Phi\)</span> æ˜ å°„åˆ° 3D meshè¡¨é¢ï¼Œç„¶åé‡‡ç”¨é¢„æµ‹å¥½çš„ç›¸æœºpose <span class="math inline">\(\theta^i\)</span> ï¼Œç”¨å¯å¾®åˆ†æ¸²æŸ“ <span class="math inline">\(\mathcal{R}\)</span> å°†3Dæ¨¡å‹æ¸²æŸ“åˆ°2Dï¼Œç„¶åå°†ç»“æœä¸å¯¹åº”çš„ç”±SCOPSç”Ÿæˆçš„éƒ¨ä»¶åˆ†å‰²æ¦‚ç‡å›¾ <span class="math inline">\(P^i\)</span> åšå‡æ–¹è¯¯å·®ã€‚</p></li><li><p>æ³¨ï¼šè¿™ä¸ªç”±SCOPSç”Ÿæˆçš„å›¾åƒåˆ†å‰²ç»“æœ <span class="math inline">\(P^i\)</span> æ˜¯æ¦‚ç‡æ•°å€¼çš„å½¢å¼</p></li><li><p>ç»éªŒæ€§åœ°é€‰æ‹©äº†é‡‡ç”¨å‡æ–¹è¯¯å·®MSEï¼Œæ¯” KullbackLeibler divergence æ•ˆæœå¥½</p></li></ol></li><li><p><em>åŸºäºé¡¶ç‚¹çš„çº¦æŸ Vertex-based constraint</em></p><ol type="1"><li><p>è®©3Dæ¨¡å‹æŠ•å½±å›2Dä¹‹åï¼Œè¢«åˆ†ç±»åˆ°æŸä¸ªè¯­ä¹‰partçš„é¡¶ç‚¹ä»ç„¶å¤„åœ¨å›¾åƒä¸­è¯¥partå¯¹åº”çš„åŒºåŸŸ</p></li><li><p><span class="math display">\[L_{sv}=\sum^{N_p}_{p=1} \frac{1}{|\bar V_p|}Chamfer(\mathcal{R}(\bar V_p;\theta^i),Y_p^i)\]</span></p><p>å…¶ä¸­ï¼Œ<span class="math inline">\(\bar V_p\)</span> æ˜¯å·²ç»å­¦å¥½çš„æŸç±»ç‰©ä½“çš„template meshä¸­å±äºéƒ¨ä»¶pçš„é‚£éƒ¨åˆ†ï¼Œ<span class="math inline">\(Y_p^i\)</span>æ˜¯åŸå§‹2Då›¾åƒä¸­å±äºéƒ¨ä»¶pçš„é‚£éƒ¨åˆ†ï¼Œ<span class="math inline">\(N_p\)</span> æ˜¯è¯­ä¹‰éƒ¨ä»¶æ•°é‡ã€‚</p></li><li><p>ç”¨Chamfer distanceæ˜¯å› ä¸ºæŠ•å½±åçš„é¡¶ç‚¹å’ŒåŸå§‹çš„åƒç´ ç‚¹å¹¶ä¸æ˜¯ä¸¥æ ¼ä¸€å¯¹ä¸€å¯¹åº”çš„å…³ç³»</p></li><li><p>ç”¨æŸç±»ç‰©ä½“çš„template meshï¼Œå°±å¯ä»¥è®©ç½‘ç»œå­¦ç›¸æœºposeï¼›åä¹‹ï¼Œå‡å¦‚ç”¨å•ä¸ªå…·ä½“ç‰©ä½“çš„meshçš„è¯ï¼Œç½‘ç»œå°±ä»…ä»…ä¼šå¯¹3Dç‰©ä½“çš„å½¢çŠ¶è¿›è¡Œæ‰­æ›²ï¼Œä¸ä¼šå­¦åˆ°æ­£ç¡®çš„ç›¸æœºposeäº†ã€æˆ‘æœ‰ç‚¹ä¸ç†è§£ä¸ºå•¥ã€‘</p></li></ol></li></ol><h3 id="æ¸è¿›çš„è®­ç»ƒæ–¹æ³•em">æ¸è¿›çš„è®­ç»ƒæ–¹æ³•EM</h3><ol type="1"><li><p>ä¹‹æ‰€ä»¥è¦ç”¨æ¸è¿›å¼è®­ç»ƒï¼Œæ˜¯å› ä¸º</p><ol type="1"><li>éœ€è¦3Dé‡å»ºæ¨¡ç½‘ç»œé¦–å…ˆå­¦ä¼šä¸€ä¸ªå¤§ä½“ä¸Šå¯ç”¨çš„texture encoder <span class="math inline">\(I_{flow}\)</span>ï¼Œç„¶åæ‰èƒ½ç”Ÿæˆæ ‡å‡†è¯­ä¹‰UV mapï¼Œ</li><li>è¿™æ ·è¿˜èƒ½å…ˆç”Ÿæˆå¯¹åº”å…·ä½“ç±»åˆ«çš„template meshï¼Œä¸€æ–¹é¢åŠ é€Ÿç½‘ç»œçš„æ”¶æ•›ï¼Œä¸€æ–¹é¢å¯ä»¥ç”¨åœ¨å‰é¢æåˆ°çš„<em>åŸºäºé¡¶ç‚¹çš„çº¦æŸ</em>ä¸­ã€‚</li></ol></li><li><p>ä½†æ˜¯ï¼Œå¦‚æœç›´æ¥æŠŠtemplate meshå’Œé‡å»ºæ¨¡æ¨¡å‹å…¨éƒ½ä¸€èµ·å­¦ä¹ çš„è¯ï¼Œæ•ˆæœä¸å¥½ï¼›æ‰€ä»¥å°±æå‡ºäº†ï¼šEMè®­ç»ƒæ­¥éª¤ï¼ˆexpectation-maximizationæœŸæœ›æœ€å¤§åŒ–ï¼Ÿï¼‰ï¼Œå°±æ˜¯å…ˆå›ºå®šä¸€éƒ¨åˆ†å­¦ä¹ å¦ä¸€éƒ¨åˆ†ã€‚</p><ol type="1"><li><p><strong>E</strong>ï¼šå›ºå®šæ ‡å‡†è¯­ä¹‰UV mapå’Œtemplateï¼ˆåˆå§‹æ˜¯çƒä½“ï¼‰ï¼Œ<strong>è®­ç»ƒé‡å»ºæ¨¡ç½‘ç»œ</strong>ã€‚200è½®ã€‚</p><p>lossåŒ…æ‹¬ï¼š</p><ol type="1"><li><p>3DæŠ•å½±åˆ°2Dçš„å›¾åƒä¸gtå‰ªå½±çš„ IoU âœ–ï¸ -1</p></li><li><p>3DæŠ•å½±åˆ°2Dçš„å›¾åƒä¸åŸå§‹RGBå›¾åƒçš„ perceptual distanceï¼ˆçŸ¥è§‰è·ç¦»ï¼Ÿï¼‰</p></li><li><p>å‰é¢æåˆ°çš„åŸºäºæ¦‚ç‡çš„çº¦æŸå’ŒåŸºäºé¡¶ç‚¹çš„çº¦æŸ</p></li><li><p>æè´¨å¾ªç¯ä¸€è‡´æ€§ Texture cycle consistencyï¼š</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/01/27/SU3aFdKZrOtAiws.png" alt="image-20220124182432533" /><figcaption>image-20220124182432533</figcaption></figure><p>å­¦ä¹ texture flowçš„æ—¶å€™æœ€å¤§çš„é—®é¢˜ï¼šé¢œè‰²ç›¸ä¼¼çš„3D meshçš„é¢ä¼šè¢«å¯¹åº”åˆ°é”™è¯¯çš„2Då›¾åƒçš„åƒç´ ç‚¹ä¸Š</p></li><li><p>è¿™æ˜¯ä¸€ä¸ªcycleï¼šå¼ºåˆ¶é¢„æµ‹å‡ºæ¥çš„texture flowï¼ˆ2D to 3Dï¼‰å’Œç›¸æœºæŠ•å½±ï¼ˆ3D to 2Dï¼‰äºŒè€…ä¸€è‡´ã€‚</p></li><li><p>é¦–å…ˆå®šä¹‰äº†<span class="math inline">\(\mathcal{C}_{in}^j\)</span>ã€ <span class="math inline">\(\mathcal{C}_{out}^j\)</span>åˆ†åˆ«æ˜¯è¾“å…¥å›¾åƒä¸­è¢«æ˜ å°„åˆ°ä¸‰è§’å½¢é¢<span class="math inline">\(j\)</span>çš„ä¸€å®šæ•°é‡åƒç´ ç‚¹çš„å‡ ä½•ä¸­å¿ƒï¼Œå’Œä»ä¸‰è§’å½¢é¢<span class="math inline">\(j\)</span>æ¸²æŸ“å›2Då›¾åƒæ—¶å¯¹åº”çš„ä¸€å®šæ•°é‡åƒç´ ç‚¹çš„å‡ ä½•ä¸­å¿ƒã€‚å…¬å¼å¦‚ä¸‹ï¼š <span class="math display">\[\mathcal{C}_{in}^j = \frac{1}{N_c}\sum^{N_c}_{m=1}\Phi(I_{flow}(\mathcal{G}^m))_j;\\ \mathcal{C}_{out}^j = \frac{\sum^{H\times W}_{m=1}\mathcal{W}_j^m\times \mathcal{G}^m}{\sum^{H\times W}_{m=1}\mathcal{W}_j^m}\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(\mathcal{G}^m\)</span>æ˜¯æŠ•å½±å›¾åƒçš„æ ‡å‡†åæ ‡ç½‘æ ¼ï¼ˆåŒ…å«äº†åƒç´ çš„åæ ‡<span class="math inline">\((u,v)\)</span>å€¼ï¼‰ï¼Œ<span class="math inline">\(\Phi\)</span>æ˜¯UV mapï¼Œ<span class="math inline">\(I_{flow}\)</span>æŠŠåƒç´ æ˜ å°„åˆ°3D meshçš„é¢<span class="math inline">\(j\)</span>ä¸Šï¼›<span class="math inline">\(N_c\)</span>æ˜¯å¯¹åº”åˆ°é¢<span class="math inline">\(j\)</span>çš„åƒç´ ç‚¹çš„æ•°é‡ï¼›<span class="math inline">\(\mathcal{W}\)</span>æ˜¯å¯å¾®åˆ†æ¸²æŸ“æ—¶ç”Ÿæˆçš„æ¦‚ç‡mapï¼Œæ¯ä¸ª<span class="math inline">\(\mathcal{W}_j^m\)</span>è¡¨ç¤ºé¢ j è¢«æŠ•å½±åˆ°åƒç´  m ä¸Šçš„æ¦‚ç‡ã€‚</p><ul><li>æŠŠé‡å»ºæ¨¡meshæ¨¡å‹æ¸²æŸ“æˆ2Då›¾åƒï¼Œç”¨çš„æ˜¯ Soft Rasterizerï¼Œè€Œä¸æ˜¯CMRä¸­ç”¨çš„ Neural Mesh Rendererï¼Œå› ä¸ºå‰è€…å¯ä»¥æä¾›æ¦‚ç‡mapï¼Œä¾›texture cycle consistencyä½¿ç”¨</li></ul></li><li><p>é‚£ä¹ˆï¼Œæè´¨å¾ªç¯ä¸€è‡´æ€§å°±æ˜¯è®©<span class="math inline">\(\mathcal{C}_{in}^j\)</span>æ¥è¿‘ <span class="math inline">\(\mathcal{C}_{out}^j\)</span>ï¼š <span class="math display">\[L_{tcyc} = \frac{1}{|F|}\sum^{|F|}_{j=1}||\mathcal{C}_{in}^j-\mathcal{C}_{out}^j||^2_F\]</span></p></li></ol></li><li><p>è¿˜æœ‰å†™åœ¨é™„å½•é‡Œçš„ä¸¤ä¸ªlossï¼š</p><ol type="1"><li>graph Laplacian constraint æ¥é¼“åŠ±meshè¡¨é¢å¹³æ»‘ã€ä»pixel-meshä¸­æ¥çš„ã€‘</li><li>edge regularization æ¥æƒ©ç½šå¤§å°ä¸è§„åˆ™çš„é¢ ä»£ç é‡Œä¼¼ä¹æ˜¯flatten loss</li></ol></li><li><p>è¿˜æœ‰å†™åœ¨é™„å½•é‡Œçš„å¯¹æŠ—è®­ç»ƒloss</p></li></ol></li><li><p><strong>M</strong>ï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„é‡å»ºæ¨¡ç½‘ç»œï¼Œæ›´æ–°templateï¼ˆä»çƒä½“å¼€å§‹ï¼‰å’Œæ ‡å‡†è¯­ä¹‰UV mapã€‚</p><ol type="1"><li><p>templateä¸€å¼€å§‹æ˜¯çƒä½“ï¼Œç„¶åæ¯è®­ç»ƒKè½®ï¼Œå¯¹å®ƒè¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼š <span class="math display">\[\bar V_t=\bar V_{t-1} + D_{shape}(\frac{1}{|\mathcal{Q}|}\sum_{i\in \mathcal{Q}}E(I^i))\]</span> <span class="math inline">\(V_{t}\)</span>å’Œ <span class="math inline">\(V_{t-1}\)</span>æ˜¯æ›´æ–°å‰åçš„templateï¼ŒIæ˜¯è¾“å…¥çš„å›¾ç‰‡ï¼Œç»è¿‡Eç”Ÿæˆ3Då±æ€§ï¼ŒDæ˜¯å½¢çŠ¶ encoderã€‚Qæ˜¯ç»è¿‡æŸç§æ–¹å¼é€‰æ‹©å‡ºæ¥çš„éƒ¨åˆ†æ ·æœ¬ã€‚</p><p><strong>é€‰æ‹©æ ·æœ¬çš„æ–¹å¼</strong>ï¼š</p><ol type="1"><li>é¦–å…ˆé€‰æ‹©æœ€å¥½çš„é‚£ä¸€ä¸ªæ ·æœ¬ï¼Œå³ä¸ground truthè½®å»“çš„IoUæœ€å°çš„</li><li>ç„¶åé€‰æ‹©Kä¸ªè·Ÿè¿™ä¸ªæœ€å¥½çš„æ ·æœ¬æœ€æ¥è¿‘çš„æ ·æœ¬ï¼Œå³è¿™äº›æ ·æœ¬çš„gtè½®å»“ä¸æœ€å¥½çš„æ ·æœ¬çš„gtè½®å»“çš„IoUè¶Šå°åˆ™è¶Šæ¥è¿‘</li></ol></li><li><p>è¿™æ ·çš„è¯ï¼Œtemplate <span class="math inline">\(V_t\)</span> å°±æ˜¯é€‰å‡ºæ¥çš„æ ·æœ¬çš„å¹³å‡å½¢çŠ¶</p></li></ol></li><li><p>æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¼šåŒ…æ‹¬ä¸¤è½®ï¼Œæ¯è½®éƒ½åŒ…æ‹¬ä¸€ä¸ªEå’Œä¸€ä¸ªMã€‚ï¼ˆä¸¤è½®åˆ†åˆ«å°±æ˜¯ä»£ç ä¸­çš„<code>train_s1</code> <code>train_s2</code>ã€‚ï¼‰åœ¨Eä¸­ï¼Œè®­ç»ƒ200 epoch é‡å»ºæ¨¡ç½‘ç»œï¼Œç„¶ååœ¨Mä¸­ç”¨è®­ç»ƒå¥½çš„ç½‘ç»œæ›´æ–°templateå’Œæ ‡å‡†è¯­ä¹‰UV mapã€‚æ³¨æ„åœ¨ç¬¬ä¸€è½®ä¸­ï¼ˆä¸€è½®åŒ…æ‹¬ä¸€ä¸ªEå’ŒMï¼‰ï¼Œåªè®­ç»ƒé‡å»ºæ¨¡ç½‘ç»œï¼Œè€Œæ²¡æœ‰è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸã€‚</p></li></ol></li></ol><h3 id="å®éªŒ">å®éªŒ</h3><ol type="1"><li>æ•°æ®é›†ï¼šPASCAL3D+ä¸­çš„è½¦å’Œæ‘©æ‰˜è½¦ã€CUB-200-2011ä¸­çš„é¸Ÿã€ImageNetä¸­çš„é©¬ æ–‘é©¬ ç‰›ã€OpenImagesä¸­çš„ä¼é¹…</li><li>å±€é™æ€§ï¼š<ol type="1"><li>ä¾èµ–äºSCOPSæä¾›è¯­ä¹‰åˆ†å‰²ï¼Œæœ‰æ—¶å€™è¯­ä¹‰åˆ†å‰²ä¸å‡†ç¡®çš„è¯ç»“æœå°±ä¸å¥½</li><li>æ¯”è¾ƒå°‘è§çš„ç›¸æœºposeå¾ˆéš¾</li><li>ç»†èŠ‚æ€§çš„åœ°æ–¹æ•ˆæœä¸å¥½ï¼Œæ¯”å¦‚æ­£åœ¨é£çš„é¸Ÿçš„ä¸¤ä¸ªç¿…è†€ã€æ–‘é©¬çš„è…¿ç­‰</li></ol></li></ol><h1 id="questions">Questions</h1><ol type="1"><li><p>ä¸ºä»€ä¹ˆè¦stage2 ï¼Ÿ</p><ol type="1"><li>è¿™ä¸¤ä¸ª stageçš„ä¸»è¦åŒºåˆ«å°±æ˜¯ï¼šstage1çš„æ—¶å€™æ²¡æœ‰ç”¨è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸï¼Œåœ¨stage2æ‰åŠ ä¸Šã€‚å› ä¸ºä¸€å¼€å§‹texture flow encoderæ•ˆæœå¹¶ä¸å¥½ï¼Œavg_uvä¹Ÿä¸å‡†ç¡®ï¼Œæ‰€ä»¥å¹²è„†å…ˆä¸ç”¨ã€‚æ‰€ä»¥åˆ†æˆs1å’Œs2ï¼Œæœ€ä¸»è¦çš„å°±æ˜¯å› ä¸ºåœ¨s1è®­ç»ƒå®Œä¹‹åï¼Œé‡å»ºæ¨¡ç½‘ç»œå·²ç»å¤§ä½“å¯ä»¥ç”¨äº†ï¼Œè¿™æ—¶å€™å°±å¯ä»¥è°ƒç”¨<code>avg_uv.py</code>æ¥ç”Ÿæˆæ ‡å‡†è¯­ä¹‰UV mapï¼Œä¾›s2çš„æ—¶å€™è¯­ä¹‰ä¸€è‡´æ€§ç”¨ã€‚</li><li>é™„å½•é‡Œè¯´ï¼Œä»æ•ˆæœä¸Šæ¥çœ‹ï¼Œ2ä¸ªstageæ¯”1ä¸ªæ•ˆæœè¦å¥½ï¼Œä¸”å·²ç»è¶³å¤Ÿå¥½äº†ï¼Œæœ‰è¿™å¼ å›¾å¯¹æ¯”äº†ä¸€ä¸‹ï¼š<img src="https://s2.loli.net/2022/01/27/Q26BbRypnrKj7AU.png" alt="image-20220127171646865" /></li></ol></li><li><p>avg_uv å°±æ˜¯å­¦ seg map -&gt; uv mapçš„ä¹ˆï¼Ÿ</p><ol type="1"><li>seg map -&gt; uv mapè¿™ä¸ªè¿‡ç¨‹æ˜¯é‡å»ºæ¨¡ç½‘ç»œä¸­texture flowè¿™ä¸ªéƒ¨åˆ†åšçš„äº‹æƒ…</li><li>avg_uvå°±æ˜¯è®ºæ–‡é‡Œè¯´çš„ æ ‡å‡†è¯­ä¹‰ uv mapï¼ˆCanonical semantic uv mapï¼‰ï¼š<ol type="1"><li>ç†è®ºä¸Šï¼ŒåŒä¸€ç±»ç‰©ä½“çš„meshæ¨¡å‹ï¼Œå°½ç®¡å„è‡ªéƒ½æœ‰ä¸åŒçš„å½¢çŠ¶ï¼Œä½†æ¯ä¸ªç‚¹çš„è¯­ä¹‰å«ä¹‰éƒ½æ˜¯ä¸€è‡´çš„</li><li>å› æ­¤ï¼Œå¯¹äºæŸä¸€ç±»ç‰©ä½“çš„å¤§é‡å›¾åƒæ•°æ®é›†ï¼ˆæ¯”å¦‚é¸Ÿï¼‰ï¼Œå¯ä»¥ç”Ÿæˆä¸€å¼ å¯¹åº”è¿™æ•´ä¸ªç±»åˆ«çš„avg_uv</li><li>åˆ©ç”¨è¿™ä¸ªavg_uvï¼Œç›¸å½“äºæ˜¯ç»™æ•´ä¸ªç±»åˆ«çš„templateæ‰“ä¸Šäº†è¯­ä¹‰æ ‡ç­¾ï¼Œåç»­è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸçš„æ—¶å€™å¯ä»¥ç”¨ã€‚</li><li>è¿™ä¸ªavg_uvçš„è®¡ç®—è¿‡ç¨‹ï¼š<ol type="1"><li>é¦–å…ˆç”¨SCOPSï¼ˆå¦ä¸€ç¯‡å·¥ä½œï¼Œæ— ç›‘ç£çš„ï¼‰ç”Ÿæˆæ‰€æœ‰å›¾åƒçš„è¯­ä¹‰åˆ†å‰²ç»“æœseg mapï¼Œç„¶åç”¨é‡å»ºæ¨¡ç½‘ç»œå­¦åˆ°çš„texture flowæ˜ å°„æˆuv map</li><li>é€‰æ‹©æ•ˆæœæœ€å¥½çš„ä¸€éƒ¨åˆ†instancesï¼Œå¯¹å®ƒä»¬çš„uv mapå–å¹³å‡ï¼Œå¾—åˆ°avg_uv</li></ol></li></ol></li></ol></li><li><p>paper é‡Œé¢ æœ‰è¯´å›ºå®šcamera å­¦shapeï¼Ÿ é‚£ä»£ç é‡Œæœ‰fix cameraé¢„æµ‹ä¹ˆï¼Ÿ</p><p>æˆ‘å¥½åƒæ²¡æœ‰è¯»åˆ°paperé‡Œæœ‰å…·ä½“è¯´åˆ°å›ºå®šcameraå­¦shapeè€¶â€¦â€¦</p><p>è®ºæ–‡é‡Œæåˆ°è¦è§£å†³camera-shapeä¸€èµ·å­¦æ—¶çš„ambiguityçš„é—®é¢˜ï¼Œä½†ä¸æ˜¯å›ºå®šä¸€ä¸ªå­¦å¦ä¸€ä¸ªï¼Œè€Œæ˜¯åˆ©ç”¨avg_uvæ¥å®ç°è¯­ä¹‰ä¸€è‡´æ€§ï¼šè®© æ¯ä¸ªç‰©ä½“çš„2D seg uv map ä¸ avg_uv ä¿æŒä¸€è‡´æ€§ï¼Œä»è€Œè®©3Dæ¨¡å‹çš„æ¯ä¸ªè¯­ä¹‰éƒ¨ä»¶è·Ÿ2Då›¾åƒé‡Œç›¸åº”çš„ä½ç½®æœ‰å¯¹åº”å…³ç³»ã€‚</p><p><img src="https://s2.loli.net/2022/01/27/TzCkKA462gRbOFa.png" alt="image-20220124121618081" style="zoom:50%;" /></p><p>ä»è¿™å¼ å›¾é‡Œå¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæ²¡æœ‰è¯­ä¹‰ä¸€è‡´æ€§ï¼Œmeshæ¨¡å‹ä¸­åŸæœ¬å¯¹åº”å¤´çš„é¡¶ç‚¹è¢«å½“ä½œäº†ç¿…è†€å°–ï¼Œè¿™æ ·çš„cameraå°±æ˜¯é”™è¯¯çš„ï¼Œé”™è¯¯çš„cameraåˆé€ æˆäº†é”™è¯¯çš„shapeã€‚è€Œæœ‰äº†è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå°±èƒ½åˆ©ç”¨è¯­ä¹‰è®©camera æ›´å‡†ç¡®ï¼Œè¿™æ ·å°±èƒ½è·Ÿç€æå‡shape</p></li><li><p>æŒ‰ç…§ä»–çš„è¯´æ³•ï¼Œ å…ˆæ˜¯feature avg ç„¶å decode å‡º average shapeã€‚é‚£ä¹ˆè¿™ä¸ªfeatureå°±è¦å­¦å¥½ä¸€ç‚¹ï¼Œå¦åˆ™å¹³å‡å®¹æ˜“æˆçƒå½¢ã€‚é‚£ä¹ˆè¿™ä¸ªfeature è¿˜æœ‰å…¶ä»–lossåœ¨ä¸Šé¢ä¹ˆï¼Ÿæ¯”å¦‚æˆ‘ä»¬smr ä¸Šè¿˜æœ‰ consistency loss ä½†æ˜¯åŠ åœ¨ delta_verticeä¸Šï¼Ÿä»–æœ‰åŠ åœ¨featureä¸Šä¹ˆï¼Ÿå¦åˆ™ä¸èƒ½ç¡®ä¿è¿™ä¸ª feature avg äº†ä»¥å è¿˜æœ‰æ„ä¹‰</p><p>æˆ‘å¯èƒ½æ²¡æœ‰æ‡‚è¿™ä¸ªé—®é¢˜è€¶â€¦â€¦å­¦é•¿è¯´çš„æ˜¯ä¸æ˜¯è®¡ç®—category level çš„ templateè¿™ä¸ªæ­¥éª¤å‘¢ï¼Ÿæˆ‘è§‰å¾—è¿™ä¸ªæ­¥éª¤é‡Œé¢ä¿è¯æ•ˆæœå¥½çš„æ–¹å¼æœ‰è¿™å‡ ç‚¹æ¯”è¾ƒå…³é”®ï¼š</p><ol type="1"><li>æ›´æ–°category level çš„ templateæ˜¯ä»Mæ­¥éª¤æ‰å¼€å§‹è¿›è¡Œçš„ï¼›åœ¨æ­¤ä¹‹å‰ï¼ŒEæ­¥éª¤ä¸­ä¼šåœ¨å›ºå®štemplateçš„å‰æä¸‹ï¼Œå•ç‹¬è®­ç»ƒé‡å»ºæ¨¡ç½‘ç»œ200è½®ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­çš„lossè¿˜æ˜¯æŒºå¤šçš„ï¼Œé™¤äº†è¯­ä¹‰ä¸€è‡´æ€§æ²¡æœ‰ç”¨ä»¥å¤–ï¼Œå…¶ä»–çš„losséƒ½ç”¨äº†ï¼ŒåŒ…æ‹¬è®ºæ–‡é‡Œæå‡ºçš„texture cycle consistencyï¼Œè¿˜æœ‰é™„å½•é‡Œæåˆ°çš„graph Laplacian constraintã€edge constraintç­‰ç­‰</li><li>è®¡ç®—average templateçš„æ—¶å€™ï¼Œå¹¶ä¸æ˜¯ç”¨äº†æ‰€æœ‰æ•°æ®ï¼Œè€Œæ˜¯é€‰æ‹©äº†æœ€å¥½çš„ä¸€éƒ¨åˆ†instancesï¼š<ol type="1"><li>é¦–å…ˆé€‰æ‹©æœ€å¥½çš„é‚£ä¸€ä¸ªinstanceï¼Œå³ä¸ground truth maskçš„IoUæœ€å°çš„</li><li>ç„¶åé€‰æ‹©Kä¸ªè·Ÿè¿™ä¸ªæœ€å¥½çš„æ ·æœ¬æœ€æ¥è¿‘çš„æ ·æœ¬ï¼Œå³è¿™äº›æ ·æœ¬çš„gt maskä¸æœ€å¥½çš„æ ·æœ¬çš„gt maskçš„IoUè¶Šå