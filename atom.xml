<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tianke Youke</title>
  
  <subtitle>A sanctuary for secreting and rushing at night.</subtitle>
  <link href="https://jyzhu.top/blog/atom.xml" rel="self"/>
  
  <link href="https://jyzhu.top/blog/"/>
  <updated>2024-05-29T18:26:34.814Z</updated>
  <id>https://jyzhu.top/blog/</id>
  
  <author>
    <name>Jiayin Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>look at my new keyboard!!</title>
    <link href="https://jyzhu.top/blog/look-at-my-new-keyboard/"/>
    <id>https://jyzhu.top/blog/look-at-my-new-keyboard/</id>
    <published>2024-05-29T18:14:32.000Z</published>
    <updated>2024-05-29T18:26:34.814Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://s2.loli.net/2024/05/30/RzloOHq4KsZtJyT.jpg" alt="Rainy 75 + corn cob keycaps" /><figcaption>Rainy 75 + corn cob keycaps</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2024/05/30/RzloOHq4KsZtJyT.jpg&quot; alt=&quot;Rainy 75 + corn cob keycaps&quot; /&gt;&lt;figcaption&gt;Rainy 75 + corn cob k</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jyzhu.top/blog/hello%20i%20try%20to%20use%20this%20keyboard/"/>
    <id>https://jyzhu.top/blog/hello%20i%20try%20to%20use%20this%20keyboard/</id>
    <published>2024-05-25T15:39:24.890Z</published>
    <updated>2024-05-25T15:39:24.890Z</updated>
    
    <content type="html"><![CDATA[<p>hello i try to use this keyboard.</p><p>it sounds ok.</p><p>but it is really slight</p><p>i mean i cannot type in a strong way otherwise it will be i don't know.</p><p>it sounds ok and feels ok to me</p><p>well all the keys are correct no?</p><p>qwertyuiop[];'zxcvbnm,./A</p><p>The right shift is just right to me</p><p>then I have the arrow keys</p><p>it is good</p><p>then there're several useful commonly used function keys</p><p>such as delete home end pgup pgdn</p><p>这个键盘打字挺开心说真的</p><p>还可以的就是打字好像不是特别好</p><p>我是说我自己打字不是很好</p><p>不过还行</p><p>不知道在说什么</p><p>没事</p><p>不重要</p><p>准备读一下论文？</p><p>angela让读的</p><p>这个噼里啪啦的声音 但是说实话真的在认真打字工作的时候并不会注意到它</p><p>不过一旦注意到还是很开心的</p><p>hello 老婆你好我是小霖，为了让你的输入法认识我，我决定多打几遍小霖小霖是小霖，嘿嘿。弟弟在外面怎么突然没声音了咧，妈妈咋突然不说话了？话说既然永远都玩不到四个人一起玩的游戏，你是不是把你的手柄可以拿回来了因为不想把你的手柄弄脏了</p><p>那我等你现在去拿 不可以远程吗</p><p>哭哭哭哭、、、、、、、、</p><p>很糟糕、、、、、、、、</p><p>哭哭哭哭哭哭、、、、、、 算了！</p><p>看狼姐吧！</p><p>​ 就是就是计算机加思索 你你你你你您寄几啊就是你的苏你你你你你你你好我是朱珈印你是谁你是小霖我的小霖宝</p><p>哈哈哈哈咕咕咕咕嘻嘻写成</p><p>我多打几个不同的字</p><p>但是我有点不知道说什么</p><p>哈哈哈哈</p><p>床前明月光</p><p>疑是地上霜</p><p>举头望明月</p><p>低头思故乡</p><p>飞流直下三千尺</p><p>疑似银河落九天</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;hello i try to use this keyboard.&lt;/p&gt;
&lt;p&gt;it sounds ok.&lt;/p&gt;
&lt;p&gt;but it is really slight&lt;/p&gt;
&lt;p&gt;i mean i cannot type in a strong way otherwise it will be i don&#39;t know.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Borges and AI 读后感</title>
    <link href="https://jyzhu.top/blog/Reading-Borges-and-AI/"/>
    <id>https://jyzhu.top/blog/Reading-Borges-and-AI/</id>
    <published>2024-04-27T10:57:45.000Z</published>
    <updated>2024-04-27T11:01:20.966Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2310.01425" class="uri">https://arxiv.org/abs/2310.01425</a></p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bottou,+L">Léon Bottou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schölkopf,+B">Bernhard Schölkopf</a></p><p>发表： Arxiv</p><hr /><p>首先我们可以用博尔赫斯的“小径分叉的花园”的隐喻，来类比大语言模型。人们做出一个选择的时候，所付出的代价是放弃了其他所有可能的选择。那如果把所有可能的选择完全考虑进来，就能得到一个充满无数可能性分叉的花园。这个花园，就可以类比“完美语言模型”。它包含了所有可能的人类语言的组合。当它在聊天框输出一串文字的时候，就像是从分叉的小径中选择其中一条一样，或者是人类做出一个选择的时候一样。未被选择的词语构成了其他的分叉。“语言模型”这个全包的概念，是完美的，包含了所有语言的可能性。</p><p>实际上，“完美语言模型”就像一本预言书，里面包含了所有我们想要听到的话、可能听到的话。唯一能影响它输出的内容的，就是和它展开对话的那个人。我们用prompt作为引子来引导语言模型的生成，这其实就是对无限的可能性施加限制，prompt完全决定了语言模型的输出。</p><p>我们可以先从“幻觉”这个所谓的问题讨论起。其实这不是“幻觉”，而是一种“虚构”，是从人类语言中所有有可能的分叉中，适当地挑选出可能性高的那些，借用了一些合理的逻辑，虚构了另一套合理的故事。“完美的语言模型”，倒不如说是一个“虚构小说机器”。</p><p>一类人认为有一些大逆不道的话永远不该存在，所以要求审查LLM；更多的另一类人是希望LLM真的像智能一样为人服务、创造价值，所以要在必要的场合说必要的话。两者为了各自的目的，都希望对LLM这种本身不包含任何是非对错评判偏差的“完美语言模型”进行剪枝，去掉不想要的部分。所用的手段就是用人工精挑细选的语料来微调，或者叫human feedback。</p><p>实际上这种审查有很多的问题。最大的问题是，有了“虚构小说机器”之后，它对我们人类文化的影响比想象中大，我们会依赖它来塑造我们的知识和对未来的想象，因此它会影响整个人类的文化。当来到这个层次后，“审查”，或者说“净化”，就变得危险了，因为谁都想把自己的那一套强加在LLM之上，可是谁的标准才是真的标准呢，或者我们真的能有真正的“净化”标准吗？</p><p>“在未来，几乎每个人都使用语言模型来丰富他们的思维，对语言模型所写内容的控制权将成为对我们所思考内容的控制权。如此强大的力量能存在而不被滥用吗？</p><p>“有些人担心小说机器是一种无所不知的人工智能，可能会比我们活得更久；然而，更黑暗的诱惑是让我们的思想屈服于这个现代的皮提亚，不受真理和意图的影响，但却可以被他人操纵。如果我们一直把小说机器误认为是可以减轻我们思考负担的人工智能，那么语言模型无休止的喋喋不休会让我们像苦苦挣扎的图书馆员一样疯狂。然而，作为小说机器，它们的故事可以丰富我们的生活，帮助我们重温过去，了解现在，甚至瞥见未来。”</p><p>作者最后说，人们发明的这种“机器”，不仅能写故事，而且可以写故事的所有变体，这是人类历史上一个重要的里程碑，堪比印刷机的发明。或者，甚至可以比作早在印刷或书写被发明之前的，在洞穴壁画之前就出现的，一种塑造人类的艺术：讲故事的艺术。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2310.01425&quot; class=&quot;uri&quot;&gt;https://arxiv.org/abs/2310.01425&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bottou,+L&quot;&gt;Léon Bottou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schölkopf,+B&quot;&gt;Bernhard Schölkopf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： Arxiv&lt;/p&gt;
&lt;hr&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Literature" scheme="https://jyzhu.top/blog/tags/Literature/"/>
    
    <category term="AI" scheme="https://jyzhu.top/blog/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Score-based generative model</title>
    <link href="https://jyzhu.top/blog/Score-based-generative-model/"/>
    <id>https://jyzhu.top/blog/Score-based-generative-model/</id>
    <published>2024-04-09T20:08:34.000Z</published>
    <updated>2024-04-12T13:08:47.664Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>references:</p><p><a href="https://yang-song.net/blog/2021/score/" class="uri">https://yang-song.net/blog/2021/score/</a></p><p><a href="https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html#" class="uri">https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html#</a></p><p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" class="uri">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></p></blockquote><h1 id="score-based-models-v.s.-diffusion-models">Score-based models v.s. diffusion models</h1><ul><li>一开始两者独立发展，所以有不同的理论依据和术语</li><li>最后两者殊途同归：得到一样的模型</li></ul><h1 id="score-function">Score function</h1><ol type="1"><li>生成模型可以归类为两类：<ol type="1"><li>显式的，基于likelihood的模型，比如autoregressive，normalizing flow，VAE。这些方法通过近似最大似然来建模概率密度分布。问题是，为了计算likelihood，需要一个normalizing constant这个量，它要么是未知的，要么要想尽办法通过其他的限制来估计或者消除。所以这类模型比较复杂。</li><li>隐式的，直接建模一个概率分布的sample process，而不是概率分布本身。比如GAN。但是它需要对抗学习，这比较不稳定，而且可能有模式坍塌的问题。</li></ol></li><li>Score function和第一类相关，它也是建模likelihood——但不是它本身，而是跟它相关的一个量，即 Stein score function。这样的一个神奇好处是，可以直接消除normalizing constant这个量。 <img src="https://s2.loli.net/2024/04/12/rGvERP3W1gXV964.png" /> <em>（图中，等高线表示一个概率分布，箭头表示它的分数场。score-based model就是建模这些分数场)</em></li><li>具体一点来说：<ol type="1"><li>假设给定一个数据集 <span class="math inline">\({x_1,x_2,...,x_N}\)</span>，每个数据都是从一个未知的数据分布 <span class="math inline">\(p(x)\)</span> 采样。我们用一个生成模型来生成新的数据，这些数据都是从这个分布中采样的</li><li>我们要想办法表示这个概率分布。前面说的基于likelihood的模型是这么做的：直接建模这个概率密度函数： <span class="math display">\[ p_\theta(x)=\frac{e^{-f_\theta(x)}}{Z_\theta} \]</span> 其中这个 <span class="math inline">\(Z_\theta&gt;0\)</span> 就是依赖于 <span class="math inline">\(\theta\)</span> 的那个normalizing constant了。 这个 <span class="math inline">\(p_\theta(x)\)</span> 训练的目标函数就是最大log-likelihood： <span class="math display">\[  \max_\theta\sum^N_{i=1}\log p_\theta(x_i) \]</span></li><li>前面说了，问题就是 <span class="math inline">\(Z_\theta\)</span> 很难估计。为了避开估计它，我们的神经网络不再直接估计概率分布，而是估计它的分数（概率密度函数的log的梯度）： <span class="math display">\[s_\theta(x)=\nabla_x\log p(x)=-\nabla_x f_\theta(x)-\nabla_x\log Z_\theta = -\nabla_x f_\theta(x)\]</span> 去掉了 <span class="math inline">\(Z_\theta\)</span>！</li><li>训练的目标函数是 <strong>Fisher divergence</strong>： <span class="math display">\[ \mathbb E_{p(x)}[\|\nabla_x\log p(x)-s_\theta(x)\|^2_2] \]</span></li><li>现在的问题就是，上式中的 <span class="math inline">\(\nabla_x\log p(x)\)</span>未知。但是很好解决，一种叫<strong>score matching</strong>的方法可以直接最小化 <strong>Fisher divergence</strong>，不需要知道真实的score。</li><li>最后的问题就是得到了这个 score function 之后，怎么从中采样新的数据了。<strong>Langevin dynamics</strong> 提出了一种迭代式的采样方法，就是和diffusion的步骤一模一样的。相当于从空间中任意一个位置初始化，然后顺着score function往高概率密度的方向优化，足够多的步骤之后就到了峰值处。</li></ol></li></ol><h1 id="关-noise-什么事">关 noise 什么事？</h1><p>是这样的，前面提到的方法已经讲清楚了神经网络建模和目标函数。但是假如直接拿着数据集（比如图像）让网络学习的话，效果并不好。因为这个score function在低概率密度的区域样本很少，学得也很不好。</p><p><img src="https://s2.loli.net/2024/04/12/6cuXBCoHShsMiaO.png" /></p><p>为了解决这个问题，我们才往数据中加 noise，在被噪声扰动后的数据集上训练网络。这些扰动后的数据点极大地扩充了数据集，最主要的是能填充那些低概率密度的分布区域。</p><p>大的噪声破坏数据分布，小的噪声不够填充低概率密度区域。所以就用多尺度的噪声。也就是diffusion model的前向过程了。</p><p>总体来说，score-based model就是在这些噪声扰动后的数据集上训练的。训练的时候，噪声的尺度当然也可以作为一个已知量输入，也就是noise conditional <span class="math inline">\(s_\theta(x,i)\)</span></p><h1 id="diffusion-models">diffusion models</h1><p>to read：<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" class="uri">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></p><p>从diffusion models的角度解释整个模型</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;references:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://yang-song.net/blog/2021/score/&quot; class=&quot;uri&quot;&gt;https://yang-song.net/blog/2021/score/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html#&quot; class=&quot;uri&quot;&gt;https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html#&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&quot; class=&quot;uri&quot;&gt;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;score-based-models-v.s.-diffusion-models&quot;&gt;Score-based models v.s. diffusion models&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;一开始两者独立发展，所以有不同的理论依据和术语&lt;/li&gt;
&lt;li&gt;最后两者殊途同归：得到一样的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;score-function&quot;&gt;Score function&lt;/h1&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="generative model" scheme="https://jyzhu.top/blog/tags/generative-model/"/>
    
    <category term="diffusion model" scheme="https://jyzhu.top/blog/tags/diffusion-model/"/>
    
  </entry>
  
  <entry>
    <title>Reading Learning Locally Editable Virtual Humans</title>
    <link href="https://jyzhu.top/blog/Reading-Learning-Locally-Editable-Virtual-Humans/"/>
    <id>https://jyzhu.top/blog/Reading-Learning-Locally-Editable-Virtual-Humans/</id>
    <published>2023-12-12T09:02:56.000Z</published>
    <updated>2024-04-12T13:01:05.871Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://files.ait.ethz.ch/projects/custom-humans/paper.pdf" class="uri">https://files.ait.ethz.ch/projects/custom-humans/paper.pdf</a></p><p>作者：Hsuan-I Ho, Lixin Xue, Jie Song, Otmar Hilliges。ETH</p><p>发表： CVPR23</p><p>链接： <a href="https://custom-humans.github.io/" class="uri">https://custom-humans.github.io/</a></p><hr /><h2 id="why">Why：</h2><ol type="1"><li>3D数字人现在很热门</li><li>传统方法中，重建好一个3D数字人之后，不知道怎么编辑和定制它</li><li>所以想搞一个方法，既能创建3D数字人，又能在创建完成后对它进行定制化</li></ol><h2 id="what">What：</h2><ol type="1"><li>这个方法允许在不同的3D人之间部分地迁移几何和外貌细节，还能在改变人的姿势的时候保留一致的局部细节</li><li>具体一点说，是把NeRF和LBS-articulated（铰接）的mesh模型的优点结合起来：NeRF很灵活，有很强的建模能力；mesh模型可以变形，并且可以被完全显式地控制</li><li>再具体一点，是提出了一个混合的3D人体representation，允许跨不同主体进行局部编辑；然后提出了一整套的生成这种3D数字人的流程，既能拟合没见过的3D扫描人数据，也能随机采样生成新的个体；另外还提出一个大尺度高质量3D扫描人体数据集</li></ol><h2 id="how">How：</h2><p><img src="https://s2.loli.net/2023/12/12/3MfkXdqwn1rpeF8.png" /></p><ol type="1"><li><p>混合的3D人体representation：</p><ol type="1"><li><p>有一个可学习的特征codebook，包含<span class="math inline">\(M\times 2F\)</span>个特征，其中<span class="math inline">\(F\)</span>个是几何特征，另外<span class="math inline">\(F\)</span>个是外貌特征。给定一个human mesh，mesh有M个顶点（M很大，一万多），每个顶点跟codebook中的特征显式地一一对应。</p></li><li><p>当NeRF渲染时，给定空间中一个query点，提取局部的特征：</p><figure><img src="https://s2.loli.net/2023/12/12/RL3ehZ6nuGq12B4.png" alt="image-20231212173121173" /><figcaption>image-20231212173121173</figcaption></figure><ol type="1"><li>找出mesh顶点中离它最近的3个顶点，对这3个顶点对应的特征用barycentric interpolation（重心插值法？）得到插值后的特征</li><li>还需要把全局坐标转换成局部坐标，这么做是为了让局部与全局解耦，方便在后续更改人体pose的时候不影响局部的细节。转换方式：还是根据mesh顶点中离它最近的3个顶点组成的三角形，直接用<span class="math inline">\((u,v)\)</span>表示query点投影到三角形平面上的点在三角形中的位置，然后再加上一个<span class="math inline">\(d\)</span>表示query点距离三角形平面的距离，以及一个<span class="math inline">\(\textbf n\)</span>表示距离的方向</li><li>decoder，或者说renderer，不是单独一个NeRF，而是分成了两个独立的，一个是SDF field，一个是rgb field。论文里说这样能方便用3D loss显式地监督这两个网络。后面的实验证明如果不解耦这两个网络的话，效果会差很多</li><li>那么最后给到这两个neural fields的输入，就是<span class="math inline">\(\textbf f_s/\textbf f_c,(u,v,d),\textbf n\)</span>，分别是几何特征或者颜色特征、局部坐标、方向</li></ol></li></ol></li><li><p>采样个体样本时，有两种方式：</p><ol type="1"><li>直接采样已有的个体样本。训练的时候，针对每个个体样本，都是单独学习一个codebook。假如有N个ground truth人体，那codebook实际上是有<span class="math inline">\(N\times M\times 2F\)</span>这么多维度。想要采样其中一个个体，直接从N个样本中取一个entry就好了</li><li>生成全新的样本？用PCA。具体说，是创建一个新的D维的codebook（<span class="math inline">\(D\times 2MF\)</span>），这D个维度是对N个人体样本拟合出的PCA系数。生成随机新人体样本的时候，只要简单随机生成D个PCA参数，然后乘以利用这个新codebook算好的特征向量就好了</li></ol></li><li><p>训练过程</p><ol type="1"><li>给定一个扫描的3D人体（高质量、很细节的mesh），用其他现有工具把一个SMPL mesh和它对齐，得到相应的pose和shape参数。</li><li>用M个扫描的3D人体，分别训练特征codebook的M个entries。</li><li>三方面的loss：<ol type="1"><li>3D loss，包括rgb loss和sdf loss；</li><li>2D adversarial loss，这里是用前面提到的PCA采样方法，得到随机生成的人体，渲染成2D图像；然后用相同的相机视角用任意gt 3D人体扫描得到2D图像，然后用StyleGAN辨别这两张真假图像。这里的用意是，不需要严格相同的gt监督，就可以对PCA得到的生成样本进行监督，这样的监督又能传播到全部训练样本上，应该是能很好地提升模型的泛化性能。实验结果看这个loss还挺重要的。</li><li>简单的对特征的正则项，让特征符合高斯分布</li></ol></li></ol></li><li><p>一些编辑方式</p><figure><img src="https://s2.loli.net/2023/12/12/Xh6aIN3q7OgzGrl.png" alt="image-20231212175622543" /><figcaption>image-20231212175622543</figcaption></figure><ol type="1"><li><p>初始化：采样一个3D数字人样本。也就是前面提到的两种采样方式，可以采样已有的，也可以采样全新生成的</p></li><li><p>optimize特征，拟合一个3D扫描人体。用到的是3D loss</p></li><li><p>跨个体的特征编辑：简单粗暴，先用Blender选择人体局部对应的顶点，然后把想要的样本的codebook中的那部分特征交换到目标的codebook来</p></li><li><p>绘制材质：拿到渲染后的2D图像后，可以直接对2D图像进行一些绘制，然后再用它监督训练codebook。后面的实验结果看上去效果还不错，但limitation说不能很好地拟合过于高精度的细节</p><figure><img src="https://s2.loli.net/2023/12/12/b3nCjQsoRNuwcTg.png" alt="image-20231212180508779" /><figcaption>image-20231212180508779</figcaption></figure></li><li><p>更换人体姿势：这个representation本身是由一个SMPL mesh加一个codebook组成的；只要对SMPL mesh的参数进行修改，就能直接改变姿势了</p></li></ol></li><li><p>实验</p><ol type="1"><li>用了自己提出的CustomHumans以及THuman2.0数据集来训练模型；用SIZER数据集来测试对没见过的人体扫描的拟合性能</li><li>用chamfer distance、normal consistency、f-score来衡量拟合性能；结果挺好</li><li>展示了编辑性能，看起来挺好</li><li>ablation study：值得深究<img src="https://s2.loli.net/2023/12/12/gwQCR6b2dvLpAz8.png" alt="image-20231212175727556" /></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://files.ait.ethz.ch/projects/custom-humans/paper.pdf&quot; class=&quot;uri&quot;&gt;https://files.ait.ethz.ch/projects/custom-humans/paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Hsuan-I Ho, Lixin Xue, Jie Song, Otmar Hilliges。ETH&lt;/p&gt;
&lt;p&gt;发表： CVPR23&lt;/p&gt;
&lt;p&gt;链接： &lt;a href=&quot;https://custom-humans.github.io/&quot; class=&quot;uri&quot;&gt;https://custom-humans.github.io/&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="3D Reconstruction" scheme="https://jyzhu.top/blog/tags/3D-Reconstruction/"/>
    
    <category term="3D Human" scheme="https://jyzhu.top/blog/tags/3D-Human/"/>
    
  </entry>
  
  <entry>
    <title>Reading Neural Capture of Animatable 3D Human from Monocular Video</title>
    <link href="https://jyzhu.top/blog/Reading-Neural-Capture-of-Animatable-3D-Human-from-Monocular-Video/"/>
    <id>https://jyzhu.top/blog/Reading-Neural-Capture-of-Animatable-3D-Human-from-Monocular-Video/</id>
    <published>2023-11-23T20:46:40.000Z</published>
    <updated>2023-11-23T21:43:37.425Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：</p><p>作者：Gusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, and Yan Lu</p><p>发表： ECCV 2022</p><p>链接： <a href="https://arxiv.org/abs/2208.08728" class="uri">https://arxiv.org/abs/2208.08728</a></p><hr /><h2 id="why">Why：</h2><ol type="1"><li>之前的3D人体重建工作一般需要多视角视频，或者额外的3D几何信息。这篇工作以单视角视频为输入</li><li>之前的工作建模出来的3D人体很难泛化到新的pose</li><li>之前的工作都只能解决一部分的问题：基于参数化人体模型的方法对appearance的表示精度有限；基于NeRF的方法的appearance效果好，但是要么只关注于NeRF场本身的构建，要么需要精确的3D mesh作为先验。</li></ol><h2 id="what">What：</h2><ol type="1"><li>提出一个从单视角视频重建animatable的3D人体的方法</li><li>表示方法是把 dynamic NeRF 和一个 human mesh （SMPL）相结合。这个 dynamic NeRF 的输入是一些嵌入到mesh 顶点的局部信息，这样，当需要表现一个人不同的姿势的时候，本质上是对这个 canonical space 的静态NeRF进行deformation。<strong>这里的关键问题是如何设计这个局部信息，来让查询observation space中的任意一点的时候，都能够良好地deform到canonical space，从而找到静态NeRF里正确的点</strong></li><li>在优化过程中，首先借用别的工具初始化一个mesh pose，然后逐帧地同时 finetune mesh pose 和 NeRF</li></ol><h2 id="how">How：</h2><h3 id="query-embedding-for-nerf">Query embedding for NeRF</h3><figure><img src="https://s2.loli.net/2023/11/24/IFWcdEZDfquSVNJ.png" alt="image-20231124051120340" /><figcaption>image-20231124051120340</figcaption></figure><p>精髓就在图里了：</p><ol type="1"><li><p>人体的表现形式是我们熟悉的：首先有一个由pose参数<span class="math inline">\(\theta\)</span>驱动的SMPL mesh，以及一个mesh-guided NeRF，后者的输入是对应query ray上的3D points的embedding</p></li><li><p>Query embedding的具体构成：</p><ol type="1"><li>最直观的 Latent Code：存储在每一个mesh顶点上，表示的是appearance信息。对于一个query point，会找到mesh顶点中K nearest neighbors所对应的latent codes</li><li>被称为 Deformation Guidance：其实是在<strong>canonical space</strong>中，刚刚用到的那些KNN顶点的坐标（用inverse LBS得到），以及query point相对于mesh表面投影点的方向。这个信息能够指导deformation field，所以叫guidance</li><li>另一方面还有 Deformation Priors：是在<strong>observation space</strong>中，query point相对于刚刚用到的那些KNN顶点的距离。文中说这是用来防止deformation field落入local minima的，所以叫做priors</li></ol></li><li><figure><img src="https://s2.loli.net/2023/11/24/2d4HiJucZTotBxP.png" alt="image-20231124052407094" /><figcaption>image-20231124052407094</figcaption></figure><p>文中特意用上图强调了这里需要用到K近邻顶点，而不是单个最近的顶点。因为如果只用单个最近的顶点（图a），就不能提供不同的deformation pattern 的信息；而（b）里加上了K-NN distance之后，就能有这个deformation pattern信息了。（我有点疑惑什么是deformation pattern，就是这个表面的凹凸性吗？）</p></li></ol><h3 id="训练过程">训练过程</h3><ol type="1"><li>首先借用别的工具初始化一个mesh pose。但是这个pose不够精准，还需要finetune。文中提到这里直接是finetune per-frame pose parameter，而不用per-vertex offset，因为后者可能容易过拟合到local minima（？有点疑惑，我以为用pose parameter或许是有利于在后续用temporal consistency之类的，但是好像并没有用到；那pose param相比之下就是一个更不精准而已？）</li><li>训练时逐帧地同时 finetune mesh pose 和 NeRF。loss很直观：<ol type="1"><li>NeRF渲染图和原视频帧的L2 loss</li><li>正则 <span class="math inline">\(\|\theta-\theta^0\|^2_2\)</span>，是为了让每帧的pose finetune不至于太偏离初始估计</li></ol></li></ol><h3 id="实验">实验</h3><ol type="1"><li>训练要在v100上60小时；数据集用到People-Snapshot、DoubleFusion、ZJU-MoCap、Human3.6M；指标用PSNR和SSIM</li><li>在2022，没有什么能直接对比的其他工作，跟需要多视角视频输入的AniNeRF、底层架构很不同的A-NeRF、mesh-based 的方法VideoAvatar比了三下，比他们都好</li></ol><figure><img src="https://s2.loli.net/2023/11/24/r4V68kqj9HBdaxD.png" alt="image-20231124054314933" /><figcaption>image-20231124054314933</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;/p&gt;
&lt;p&gt;作者：Gusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, and Yan Lu&lt;/p&gt;
&lt;p&gt;发表： ECCV 2022&lt;/p&gt;
&lt;p&gt;链接： &lt;a href=&quot;https://arxiv.org/abs/2208.08728&quot; class=&quot;uri&quot;&gt;https://arxiv.org/abs/2208.08728&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="Human Reconstruction" scheme="https://jyzhu.top/blog/tags/Human-Reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>KL散度 （Kullback-Leibler Divergence，相对熵）</title>
    <link href="https://jyzhu.top/blog/KL%E6%95%A3%E5%BA%A6-%EF%BC%88Kullback-Leibler-Divergence,%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%89/"/>
    <id>https://jyzhu.top/blog/KL%E6%95%A3%E5%BA%A6-%EF%BC%88Kullback-Leibler-Divergence,%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%89/</id>
    <published>2023-10-11T20:04:11.000Z</published>
    <updated>2024-04-12T13:07:00.554Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>References:</p><p><a href="https://zhuanlan.zhihu.com/p/45131536" class="uri">https://zhuanlan.zhihu.com/p/45131536</a></p><p><a href="https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/" class="uri">https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/</a></p><p><a href="https://www.jiqizhixin.com/articles/2018-05-29-2" class="uri">https://www.jiqizhixin.com/articles/2018-05-29-2</a></p></blockquote><h1 id="含义">含义</h1><ul><li>一句话：KL散度可以用来<strong>衡量两个分布之间的差异/匹配程度</strong>。但它并不是一个真正的度量或者距离，因为它不具有对称性。</li><li>广义的散度指的是一类运算，它将矢量空间上的一个矢量场对应到一个标量场上，通俗的讲，就是输入一组矢量，返回一个标量。</li><li>在统计学意义上来说，KL散度可以用来衡量两个分布之间的差异程度。若两者差异越小，KL散度越小，反之亦反。当两分布一致时，其KL散度为0。正是因为其可以衡量两个分布之间的差异，所以在VAE、EM、GAN中均有使用到KL散度。</li><li>在信息论中，其可理解为编码系统对信息进行编码时所需要的平均附加信息量。</li></ul><h1 id="定义">定义</h1><p><img src="https://s2.loli.net/2024/04/12/yDOmFUz2IMvjCbh.png" /></p><h3 id="理解熵">理解熵</h3><ul><li>熵是用来表示信息量。</li><li>首先，我们考虑一个离散的随机变量 <span class="math inline">\(x\)</span> 。当我们观察到这个变量的一个具体值的时候，我们接收到了多少信息呢？</li><li>信息量可以被看成在学习 <span class="math inline">\(x\)</span> 的值的时候的“惊讶程度”。如果有人告诉我们一个相当不可能的事件发生了，我们收到的信息要多于我们被告知某个很可能发生的事件发生时收到的信息。如果我们知道某件事情一定会发生，那么我们就不会接收到信息。于是，我们对于信息内容的度量将依赖于概率分布<span class="math inline">\(p(x)\)</span> ，因此我们想要寻找一个函数<span class="math inline">\(h(x)\)</span> ，它是概率<span class="math inline">\(p(x)\)</span>的单调递增函数，表达了信息的内容。 <span class="math inline">\(h(.)\)</span> 的形式可以这样寻找：如果我们有两个不相关的事件 x 和 y ，那么我们观察到两个事件同时发生时获得的信息应该等于观察到事件各自发生时获得的信息之和，即 <span class="math inline">\(h(x)+h(y)=h(x,y)\)</span> 。两个不相关事件是统计独立的，因此<span class="math inline">\(p(x)p(y)=p(x,y)\)</span>。根据这两个关系，很容易看出<span class="math inline">\(h(.)\)</span>一定与<span class="math inline">\(p(.)\)</span> 的对数有关。因此，我们有： <span class="math display">\[ h(x)=-\log p(x)\]</span> 其中，负号确保了信息一定是正数或者是零。注意，低概率事件<span class="math inline">\(x\)</span> 对应于高的信息量。</li><li>现在假设一个发送者想传输一个<strong>随机变量</strong>的值给接收者。这个过程中，他们传输的<strong>平均信息量</strong>通可以通过求上式关于概率分布 p(x) 的期望得到。这个期望值就是熵Entropy： <span class="math display">\[  H[x]=-\sum_x p(x)\log p(x)\]</span></li><li>它是这个随机变量的平均信息量。</li></ul><h2 id="kl散度的性质">KL散度的性质</h2><ol type="1"><li><span class="math inline">\(D_{KL}(p||q)&gt;=0\)</span>， 当且仅当<span class="math inline">\(p(x)=q(x)\)</span>时取等号</li><li>不满足对称性，即<span class="math inline">\(D_KL(p||q)\neq D_KL(q||p)\)</span></li></ol><h1 id="讨论">讨论</h1><ul><li>KL散度是不对称的。因此不能作为一个距离度量，在使用时往往有一些问题。</li><li>那么就可以用<span class="math inline">\(\alpha\)</span>-散度。KL散度是它的一个特殊化。根据这个，还能算出一个对称的Hellinger距离，它的平方根是一个合法的距离度量</li><li>还可以推广到F散度：把KL散度公式中的<span class="math inline">\(\log\)</span>函数替换为任意的函数f，只要f满足这两个条件：<ol type="1"><li>f是一个凸函数</li><li><span class="math inline">\(f(1)=0\)</span> 此时F散度的表达式为： <span class="math display">\[  D_f(p\|q)=\int q(X)f(\frac{p(X)}{q(X)})dX\]</span> 当<span class="math inline">\(f(X)=X\log X\)</span>时，就是KL散度了。</li></ol></li><li>Bregman散度：这是从另一个角度来思考“距离”。最常见的均方欧氏距离，推广到任意维度的函数之间的距离，同样只是需要一个凸函数就能表达了。这个凸函数的取值，可以表示一大片不同的散度，都属于Bregman散度的特例</li><li>Wasserstein距离。这是用来解决一个问题的：如果两个分布离得太远，完全没有重叠，那么KL散度的值会失去意义。这在深度学习中意味着这一点梯度为0——梯度消失！ Wasserstein距离可以解决这种问题，也叫做Earth-Mover（推土机）距离：当我们希望把一堆土推移成另一堆土的形状和位置，推土代价定义为<strong>移动土的量*土移动的距离</strong>，这个代价就是两个分布的Wasserstein距离。 Wessertein距离相比KL散度和JS散度的<strong>优势</strong>在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。WGAN就是Wasserstein距离比较经典的应用之一。</li></ul><h1 id="信息的压缩">信息的压缩</h1><ol type="1"><li>假如我们有一组样本，每个样本分别有不同的值。可以直接记下每个样本的信息。</li><li>也可以用这个样本的分布来表示同样的信息量。（每个取值的概率）</li><li>还可以用一个已知的分布来表示这个分布（比如均匀分布、二项分布、正态分布），只需要记下具体分布的参数。</li></ol>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45131536&quot; class=&quot;uri&quot;&gt;https://zhuanlan.zhihu.com/p/45131536&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/&quot; class=&quot;uri&quot;&gt;https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2018-05-29-2&quot; class=&quot;uri&quot;&gt;https://www.jiqizhixin.com/articles/2018-05-29-2&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;含义&quot;&gt;含义&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;一句话：KL散度可以用来&lt;strong&gt;衡量两个分布之间的差异/匹配程度&lt;/strong&gt;。但它并不是一个真正的度量或者距离，因为它不具有对称性。&lt;/li&gt;
&lt;li&gt;广义的散度指的是一类运算，它将矢量空间上的一个矢量场对应到一个标量场上，通俗的讲，就是输入一组矢量，返回一个标量。&lt;/li&gt;
&lt;li&gt;在统计学意义上来说，KL散度可以用来衡量两个分布之间的差异程度。若两者差异越小，KL散度越小，反之亦反。当两分布一致时，其KL散度为0。正是因为其可以衡量两个分布之间的差异，所以在VAE、EM、GAN中均有使用到KL散度。&lt;/li&gt;
&lt;li&gt;在信息论中，其可理解为编码系统对信息进行编码时所需要的平均附加信息量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;定义&quot;&gt;定义&lt;/h1&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Reading Handy: Towards a high fidelity 3D hand shape and appearance model</title>
    <link href="https://jyzhu.top/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/"/>
    <id>https://jyzhu.top/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/</id>
    <published>2023-07-17T11:32:07.000Z</published>
    <updated>2023-07-21T11:41:56.456Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://rolpotamias.github.io/Handy/" class="uri">https://rolpotamias.github.io/Handy/</a></p><p>作者：Rolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou. From Imperial College London and Cosmos.</p><p>发表： CVPR23</p><p>链接： <a href="https://github.com/rolpotamias/handy" class="uri">https://github.com/rolpotamias/handy</a></p><hr /><figure><img src="https://github.com/rolpotamias/handy/raw/main/figures/teaser_fig.png" alt="handy" /><figcaption>handy</figcaption></figure><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><p>Q：我感觉这个任务听起来还挺直观的，就是用GAN去训练外观，定义一些更多vertices的mesh template，用超级大量的样本去训练堆效果嘛？hand model的定义会有什么新意吗？我倒是想不出来。</p><p>A：确实很直观，hand model的定义没什么太大区别。贡献点主要在于：1. 很大很好很variant的新数据集，造成了很好的Handy 2. 用StyleGAN来学texture，而不是传统的PCA，得到的texture更高频细节，更好。</p><h2 id="why">Why：</h2><ol type="1"><li>VR AR发展，对人手的建模、追踪和重建的研究变得流行，因为手是一个重要的显示人的行为的东西</li><li>大部分工作基于MANO，只有很粗糙的low polygon count，而且只基于31个样本构建，distribution不够宽</li><li>大部分工作都忽略了材质的构建</li></ol><h2 id="what">What：</h2><ol type="1"><li>提出一个large-scale的hand model，包含了形状和外观，用超过1200个人类样本训练，样本有large diversity</li><li>构建Synthetic dataset，训练一个hand pose estimation网络，从单张图像中重建手</li><li>提出一个基于GAN的有高频细节的手的外观+形状重建方法，即使是in-the-wild的单视角图像作为输入</li></ol><p>读前疑问：</p><ol type="1"><li>看上去作者是用NeRF做了一个high fidelity的hand model。我不太清楚技术细节如何实现，尤其是nerf如何跟parametric model结合，如果训练一个nerf layer，让它可以根据单张输入图像infer一个新手。不知道我哪里来的误解，总之不是用的nerf诶……</li><li>fig 1 看上去效果有点假……似乎是皮肤反光率的问题，用的什么lighting representation呢？没什么representation，纯粹用PCA去掉了阴影成分</li><li>居然连皱纹、血管、指甲油也能出来，确实是高频细节了。有针对这些东西做特别的优化吗？还是全是那个style-based GAN的功劳，或者大样本量的功劳呢？真是大力出奇迹呀。还真就是GAN的功劳……？</li></ol><h2 id="how">How：</h2><h3 id="收集large-scale数据集">1. 收集large-scale数据集</h3><p>raw scan：3000 vertices meshes。1208个人，包括关于他们的meta data，比如性别，年龄，身高，种族等。这些人的diversity比较大</p><h3 id="形状重建">2. 形状重建</h3><ol type="1"><li>对齐3D scans 和 mesh template<ol type="1"><li>用了两组template，一个是低分辨率的MANO，它可以直接用进SMPL人体模型中，有778个顶点；一个是高分辨的template，有8407个顶点</li><li>获得稠密的correspondence的方法是：<ol type="1"><li>从多视角渲染这些raw scans，用MediaPipe来检测2D关键点</li><li>用linear triangulation来把2D关键点转换到3D；利用手指骨架到表面尖端的投影来检测指尖。</li><li>用3D关键点来把template和3D scans的表面对齐</li><li>用Non-rigid Iterative Closest Point algorithm (NICP)来registration，寻找稠密的顶点对应关系</li></ol></li></ol></li><li>转换成规范的张开手掌的姿势<ol type="1"><li>用PCA构建一个手部形状模型。</li><li>公式和MANO几乎一样，<span class="math inline">\(\beta\)</span> <span class="math inline">\(\theta\)</span> 两个参数，分别是形状和姿势参数。</li></ol></li></ol><h3 id="高分辨率外观模型">3. 高分辨率外观模型</h3><ol type="1"><li>叫一个图像学艺术家（😳）设计了一个UV template，把scans给unwrap成那样了</li><li>对UV textures进行预处理，去掉阴影和光照：用PCA来识别描述阴影的因素，然后把这些因素去掉。（PCA居然这么好用？！）</li><li>用一个图像处理步骤，将手部纹理映射到更自然的颜色，包括增加亮度，伽玛校正，以及调整色调。</li><li>训练过程：不像其他方法那样直接把外观空间映射到一个低频PCA域，而是用GAN来建模材质。学习率较小，0.001；一个正则权重50也很有效。（啊？这个GAN就这么一句带过吗？直接用的StyleGANv3？）</li></ol><h3 id="实验">实验</h3><ol type="1"><li>和MANO比hand model：<ol type="1"><li>更紧致，5个主成分表现90% variance，mano需要9个才行</li><li>泛化到数据集外的手的能力更强</li><li>特异性误差（specificity error）？衡量生成的手和ground truth的误差</li></ol></li><li>重建小孩的手，效果更好</li><li>从单张图像进行3D重建：<ol type="1"><li>生成数据集：用自己训的GAN模型生成30000张图像，为了更真实，渲染的手跟ShapeNet中的物体有交互，以及是和用SMPL表示的人放在一起的</li><li>模型直接参考3，14，16；加了一个预测材质参数的分支</li><li>loss：L2 between estimated and gt shape parameter， pose parameter，and 3D vertices； L1 between estimated and gt UV map；L1 between estimated and gt 2D image；LPIPS loss on two images</li><li>另外设计了in-the-wild数据集，用预训练的模型预测handy 姿势、形状和材质参数，然后只优化材质参数w来拟合材质。</li><li>优化函数包括L1 and LPIPS loss on two images，以及一个对w的L2正则。得到了改进的材质参数w‘之后，finetune回归网络。</li><li>为了定量评估所提出方法的纹理重建，我们向网络提供数据中使用的扫描设备的图像。gt UV map用的是之前registration后得到的。（我不理解诶，这样真的能跟HTML公平比较吗？一方面你的handy就是从这些数据中来的，当然能对in-distribution的东西拟合得更好啊？另一方面HTML生成的UV map和你的定义是一样的吗？这个gt UV map对它来说有用吗？）</li><li>结论是：handy+GAN能得到高频细节，甚至皱纹、戒指、纹身、指甲油、白癜风之类的；handy+PCA会过渡平滑，甚至对肤色的重建失败；HTML更不行。</li></ol></li><li>Test on FreiHand 刷新了指标，7.8 MPVPE and MPJPE……</li><li>从点云重建形状和姿势。降维打击了MANO和LISA，即使用Hand+MANO+10个PCA Components，也比其他方法好很多……</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://rolpotamias.github.io/Handy/&quot; class=&quot;uri&quot;&gt;https://rolpotamias.github.io/Handy/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Rolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou. From Imperial College London and Cosmos.&lt;/p&gt;
&lt;p&gt;发表： CVPR23&lt;/p&gt;
&lt;p&gt;链接： &lt;a href=&quot;https://github.com/rolpotamias/handy&quot; class=&quot;uri&quot;&gt;https://github.com/rolpotamias/handy&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
    <category term="Hand" scheme="https://jyzhu.top/blog/tags/Hand/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT Applications to be explored</title>
    <link href="https://jyzhu.top/blog/ChatGPT-Applications-to-be-explored/"/>
    <id>https://jyzhu.top/blog/ChatGPT-Applications-to-be-explored/</id>
    <published>2023-07-16T17:57:48.000Z</published>
    <updated>2023-07-16T18:01:58.015Z</updated>
    
    <content type="html"><![CDATA[<p>今天逛 github，发现了一些很 amazing 的chatgpt applications，摘录一些感兴趣的精华在此。真是感慨：LLM 以来天天风云变幻，弄潮儿在前面兴风作浪，我在后面望其项背……</p><ol type="1"><li><p>(Useful) egoist / openai-proxy</p><p>用 Vercel 开一个小的 Proxy server，转发 gpt API，这样可以绕开有些国家地区的 IP 限制</p></li><li><p>BuilderIO / ai-shell</p><p>在命令行里使用 chatgpt，把自然语言转化成 Linux commands，命令是 <code>ai [texts]</code></p></li><li><p>eli64s / readme-ai</p><p>一个轻量的 script，根据 repository 生成酷炫的 readme 文件</p></li><li><p>efJerryYang / chatgpt-cli</p><p>命令行 chatgpt client</p></li><li><p>yufeikang / ai-cli</p><p>另一个命令行 chatgpt client（实测的时候再对比一下这俩）</p></li><li><p>mukulpatnaik / researchgpt</p><p>输入论文 PDF 文件，然后和 gpt 聊论文。一个用 Flask 开发的 web client 貌似，可以再仔细看一下咋实现的，挺有意思</p></li><li><p>(⭐️ Amazing) AntonOsika / gpt-engineer</p><p>很方便安装，pip install 就好了！直接通过描述 + AI 追问 + 补充细节，生成一个代码项目</p></li><li><p>(⭐️ Amazing) Yidadaa / ChatGPT-Next-Web</p><p>好像很实用的 web GUI！一键部署到 Vercel。我找这玩意主要是为了直接用 API 访问 GPT-4，就不用订阅每个月的 ChatGPT Plus 了，后者太贵了，也用不了那么多</p></li></ol><p><img src="https://s2.loli.net/2023/07/17/QU2X5ckaCmyJeGv.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天逛 github，发现了一些很 amazing 的chatgpt applications，摘录一些感兴趣的精华在此。真是感慨：LLM 以来天天风云变幻，弄潮儿在前面兴风作浪，我在后面望其项背……&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;(Useful) eg</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="LLM" scheme="https://jyzhu.top/blog/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Back to Homeland</title>
    <link href="https://jyzhu.top/blog/Back-to-Homeland/"/>
    <id>https://jyzhu.top/blog/Back-to-Homeland/</id>
    <published>2023-07-05T12:44:49.000Z</published>
    <updated>2023-07-16T18:19:38.272Z</updated>
    
    <content type="html"><![CDATA[<p>今天，恩施的傍晚似乎没有夕阳。天空是深蓝色，深山上有几只黑鸟掠过。厚重的狗叫声。我闻到一种使我感到悲戚的气味，或许是和过去某种悲戚的回忆联系起来。具体回忆内容倒也记不清。明天我要走了，我说要吃点辣的，去了南边又没有了。结果还是去吃了潮汕牛肉。妈妈今天比较易怒，我看得到她的伤感。</p><p>回国，这个词对我来说不算什么，身处国内的时候，自己是没有概念的。只有在国外的时候，才对国内有概念。这一点真是讽刺。国内的时间匆匆而过，以前会为了在国外多待几年而争取，争取到了又雀跃。现在真要待那么多年了，才感到自己正坐在什么飞驰而远去的列车上。</p><p>人生在世，奔头这个词，我是想解构它的。人活着不为了什么，活就活了。可是我分明在努力什么，在抓住些什么。嘴硬罢了，谁能超凡脱俗，没点惦念的东西？亲人，向往的某种生活，成就感，这就是我的奔头。只是：停在原地原来是一种幸运，也是一种特权。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天，恩施的傍晚似乎没有夕阳。天空是深蓝色，深山上有几只黑鸟掠过。厚重的狗叫声。我闻到一种使我感到悲戚的气味，或许是和过去某种悲戚的回忆联系起来。具体回忆内容倒也记不清。明天我要走了，我说要吃点辣的，去了南边又没有了。结果还是去吃了潮汕牛肉。妈妈今天比较易怒，我看得到她的伤</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>奶奶</title>
    <link href="https://jyzhu.top/blog/Grandma/"/>
    <id>https://jyzhu.top/blog/Grandma/</id>
    <published>2023-07-03T18:08:23.000Z</published>
    <updated>2023-07-03T18:11:44.610Z</updated>
    
    <content type="html"><![CDATA[<p>奶奶身体还好，但心态不好。健忘、固执，自闭，悲观。她几乎没有什么盼头了。她觉得活得失败，活得不好。她觉得如今脸上无光。她说：</p><p>我最痛恨别人，半熟不熟的人见面，跟你打招呼，问你家里近况怎么样。我很生气，简直想骂回去。可是你骂了吧，人家觉得你是神经病。可是我要怎么回答呢？我这两个儿子，一个工作都丢了，一个身体又那样不好。</p><p>我命苦啊。我从小家里穷，也没人管我。13岁就出去工作养活自己了。跟了你爷爷，过的都是苦日子。他一个月三十四块五角钱，二十块钱给他爸妈，雷打不动的。家里饭都吃不起了，也要给。剩下十四块五，十块钱给我，他留四块五。抽最差的烟，走得那么早。</p><p>我要管家里所有事。他什么也不管啊，一日三餐，他妈，两个儿子，去河边洗衣服。我还要上班。我每天都好累。</p><p>好不容易，日子稍微好一点了。他又走了！我那时简直恨他。</p><p>别看我现在这样，现在是我最轻松的日子。共产党给我发钱。所以我说，感谢共产党。我拿自己的钱，过自己的日子！就是记性不好。人活到连自理能力也没有了，还有什么意思。跟你爸说好了，到时候我死在这间屋子了，就火化。我都看得开。</p><p>死！我听到就眼泪直湍湍地流。奶奶说，不说这个了。她也抹眼泪。</p><p>奶奶一个月领三千多退休工资，自己省吃俭用，只花得了一千多。剩的，存在那，每年寒暑假我和妹妹去看她，她发给我们。</p><p>我说爸爸，你帮我把钱退给她。</p><p>爸爸说你拿着。她自己花不完，这钱不给你们，她拿着有什么用了？给了她安心。</p><p>我走的时候，她送出屋子，送到楼梯口。</p><p>我走远了，爸爸说，你回头再给奶奶挥挥手，你看她在阳台上看你呢。</p><p>我转头看到一簇花白的头发，远远的，很小。</p><p>能不能不要有离别呢？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;奶奶身体还好，但心态不好。健忘、固执，自闭，悲观。她几乎没有什么盼头了。她觉得活得失败，活得不好。她觉得如今脸上无光。她说：&lt;/p&gt;
&lt;p&gt;我最痛恨别人，半熟不熟的人见面，跟你打招呼，问你家里近况怎么样。我很生气，简直想骂回去。可是你骂了吧，人家觉得你是神经病。可是我要怎么回答呢？我这两个儿子，一个工作都丢了，一个身体又那样不好。&lt;/p&gt;
&lt;p&gt;我命苦啊。我从小家里穷，也没人管我。13岁就出去工作养活自己了。跟了你爷爷，过的都是苦日子。他一个月三十四块五角钱，二十块钱给他爸妈，雷打不动的。家里饭都吃不起了，也要给。剩下十四块五，十块钱给我，他留四块五。抽最差的烟，走得那么早。&lt;/p&gt;
&lt;p&gt;我要管家里所有事。他什么也不管啊，一日三餐，他妈，两个儿子，去河边洗衣服。我还要上班。我每天都好累。&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>Solutions to Common Problems in Pytorch3D Rendering</title>
    <link href="https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/"/>
    <id>https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/</id>
    <published>2023-04-23T06:22:24.000Z</published>
    <updated>2023-04-23T06:42:33.231Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch3d-rendering-的一些疑难杂症">Pytorch3D Rendering 的一些疑难杂症</h1><p>有哪些？</p><ol type="1"><li>有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？</li><li>图像的黄蓝色反了？</li><li>render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？</li><li>皮肤表面反光太强，光滑得像镜面一样，怎样更自然？</li><li>怎么物体只剩半截，更远的部分似乎被截掉了？</li><li>没解决的问题：PBR（physical based rendering）</li></ol><span id="more"></span><h3 id="有了相机内参-k而render又需要ndc坐标系那要怎么定义相机">1. 有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？</h3><p>这里的坑在于，camera本身支持任意坐标系，比如Freihand提供的是screen是224*224的相机坐标系。但是，render是默认NDC坐标系的！也就是normalized coordinate system，x和y是normalized到[-1,1]的。</p><p>一开始我直接把相机内参传给<code>PerspectiveCameras</code>，并且定义我的相机screen是224*224，像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>完全不报错，就是有问题：render 过后没东西在画面上。</p><h4 id="解决">解决：</h4><p>我最后在<a href="https://pytorch3d.org/docs/cameras">官方文档</a>找到不起眼的一句：</p><blockquote><p>The PyTorch3D renderer for both meshes and point clouds assumes that the camera transformed points, meaning the points passed as input to the rasterizer, are in PyTorch3D's NDC space.</p></blockquote><figure><img src="https://user-images.githubusercontent.com/669761/145090051-67b506d7-6d73-4826-a677-5873b7cb92ba.png" alt="（世界坐标系 -&gt; 相机坐标系 -&gt; ndc坐标系 -&gt; 图像坐标系）" /><figcaption>（世界坐标系 -&gt; 相机坐标系 -&gt; ndc坐标系 -&gt; 图像坐标系）</figcaption></figure><p>我一看，原来默认PerspectiveCameras是ndc坐标系的，<code>in_ndc = False</code> by default！</p><p>所以解决方法就是：</p><blockquote><p>Screen space camera parameters are common and for that case the user needs to set <code>in_ndc</code> to <code>False</code> and also provide the <code>image_size=(height, width)</code> of the screen, aka the image.</p></blockquote><p>那么加一个参数就好了，可是谁知道这问题困扰了我整整两三天：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, in_ndc=<span class="literal">False</span>, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>另外，我还找到了如下这个等价方法，是先把内参转到NDC坐标系，再传给<code>PerspectiveCameras</code>。（至于为什么探索到这个方法，在后面问题 3 里可以找到原因…）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ndc_fcl_prp</span>(<span class="params">Ks</span>):</span></span><br><span class="line">        ndc_fx = Ks[:, <span class="number">0</span>, <span class="number">0</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_fy = Ks[:, <span class="number">1</span>, <span class="number">1</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_px = - (Ks[:, <span class="number">0</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_py = - (Ks[:, <span class="number">1</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        focal_length = torch.stack([ndc_fx, ndc_fy], dim=-<span class="number">1</span>)</span><br><span class="line">        principal_point = torch.stack([ndc_px, ndc_py], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> focal_length, principal_point</span><br><span class="line"></span><br><span class="line">fcl, prp = get_ndc_fcl_prp(Ks)</span><br><span class="line">cameras = PerspectiveCameras(focal_length=-fcl, principal_point=prp)</span><br></pre></td></tr></table></figure><p>注意<code>focal_length=-fcl</code>，这个负号是为什么呢？这是另一个坑了哈哈哈哈。</p><p>答案是：pytorch3d坐标系的convention和我的相机不一样，它是+X指向左，+Y指向上，+Z指向图像平面外。这其中有个上下左右镜像的关系。</p><h3 id="图像的黄蓝色反了">2. 图像的黄蓝色反了？</h3><p>cv2的图像是BGR（老生常谈了），pytorch3d的是RGB。如果图像的黄蓝色相反了，基本就是这个问题，需要翻转一下，可以用torch的<code>clip(dim=(2,))</code></p><h3 id="render-完的图像锯齿很严重怎么抗锯齿antialiasing">3. render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？</h3><p>锯齿就是说像下图这样，物体的边缘很尖锐，像素点粒粒分明！</p><figure><img src="https://s2.loli.net/2023/04/23/6gx5DEXJK9uMGwS.png" alt="rand_4_skin_rendered_bad" /><figcaption>rand_4_skin_rendered_bad</figcaption></figure><p>下面是我抗锯齿处理后的效果，可以看见边缘柔和了很多：</p><figure><img src="https://s2.loli.net/2023/04/23/LAchPHVfImRtkDs.png" alt="rand_4_skin_rendered" /><figcaption>rand_4_skin_rendered</figcaption></figure><p>（我真的搞了一周这个问题……看看我的心路历程：</p><ol type="1"><li>是不是 camera 没有用 NDC，而是直接用224x224的坐标系，导致投影过程有损失？所以我试了先转换成 NDC 坐标系的相机，再render。答案是，没有影响。</li><li>是不是 Shader 的参数设置得不对，比如 <code>blur_radius</code> 和 <code>faces_per_pixel</code> 应该调大一些？这其实是一个很直观的想法了，甚至一个有经验的学长看了之后都告诉我应该是这个问题。可是当我疯狂调大这两个参数，发现并没有改变这个问题。blur_radius 只会让物体内部的材质更模糊，但是边缘的锯齿完全没改变。faces_per_pixel更是无益，几乎不影响效果。</li><li>是不是图像尺寸太小了（224x224），只能达到这么个效果？我首先测试了调大图像尺寸，到<code>1024x1024</code>，发现锯齿边缘的确是不明显了！可是我又看了相机拍摄的原始图像，虽然是有点模糊，但是不至于这么大的锯齿呀，肯定还有别的问题。）</li></ol><h4 id="解决-1">解决：</h4><p>终于，在这个issue里找到同样的问题：https://github.com/facebookresearch/pytorch3d/issues/399</p><p>解决方案是：</p><blockquote><p>render at a higher resolution and then use average pooling to reduce back to the target resolution</p></blockquote><p>居然这么暴力……不过issue里面有很详细的解释，也能理解，这就是render原理之外需要考虑的事情，甚至算不上什么bug。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">aa_factor = <span class="number">3</span> <span class="comment"># Anti-aliasing factor</span></span><br><span class="line">raster_settings_soft = RasterizationSettings(</span><br><span class="line">        image_size=<span class="number">224</span> * aa_factor, </span><br><span class="line">    )</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">images = renderer(mesh)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># NHWC -&gt; NCHW</span></span><br><span class="line">images = F.avg_pool2d(images, kernel_size=aa_factor, stride=aa_factor)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; NHWC</span></span><br></pre></td></tr></table></figure><h3 id="皮肤表面反光太强光滑得像镜面一样怎样更自然">4. 皮肤表面反光太强，光滑得像镜面一样，怎样更自然？</h3><p>一开始，皮肤 render 出来像这样，跟陶瓷似的，像话吗：</p><figure><img src="https://s2.loli.net/2023/04/23/8NWzr7txYKyqRk6.png" alt="rand_4_skin_rendered_bad2" /><figcaption>rand_4_skin_rendered_bad2</figcaption></figure><p>改进后，效果这样，自然多了：</p><figure><img src="https://s2.loli.net/2023/04/23/4qHNOs58ZhEI6Ry.png" alt="rand_4_skin_rendered_big" /><figcaption>rand_4_skin_rendered_big</figcaption></figure><h4 id="解决-2">解决：</h4><p>其实搞清楚材质相关的一些参数就好了。主要来说，这个反光是由这两个量决定的：</p><ol type="1"><li><code>specular_color</code>: specular reflectivity of the material，指定镜面反射颜色，在表面有光泽和镜面般的地方看到的颜色。</li><li><code>shininess</code>：定义材质中镜面反射高光的焦点。 值通常介于 0 到 1000 之间，较高的值会产生紧密、集中的高光。</li></ol><p>注意这里是改物体material的这些参数。虽然lighting也有这些参数定义，但这是关于光源的，和这个反光没有关系。</p><p>所以修改很简单：定义materials类，调整<code>specular_color</code>。默认是<code>1,1,1</code>，就是纯白色；调成<code>0.2,0.2,0.2</code>比较适合人的皮肤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch3d.renderer <span class="keyword">import</span> Materials</span><br><span class="line"></span><br><span class="line">materials = Materials(</span><br><span class="line">    specular_color=((<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),), <span class="comment"># 默认是1,1,1，就是纯白色；测试发现调成0.2,0.2,0.2比较适合人的皮肤。</span></span><br><span class="line">    shininess=<span class="number">30</span>, <span class="comment"># 默认值是 64，看上去高光稍微有点聚集了，改成30的话略自然，差别不太明显</span></span><br><span class="line">)</span><br><span class="line"> renderer_p3d = MeshRenderer(</span><br><span class="line">    rasterizer=MeshRasterizer(),</span><br><span class="line">    shader=HardPhongShader(</span><br><span class="line">        materials=materials,</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="怎么物体只剩半截更远的部分似乎被截掉了">5. 怎么物体只剩半截，更远的部分似乎被截掉了？</h3><p>还是一只手的模型，render 出来居然只有半个手背，距离相机更远的部分像是被截断了：</p><figure><img src="https://s2.loli.net/2023/04/23/imzqovVyWfnK3Fs.png" alt="rand_1_skin_rendered_half" /><figcaption>rand_1_skin_rendered_half</figcaption></figure><p>改进后，正常的效果应该是这样才对：</p><figure><img src="https://s2.loli.net/2023/04/23/gSJ5wm74RUjHODY.png" alt="rand_1_skin_rendered_full" /><figcaption>rand_1_skin_rendered_full</figcaption></figure><p>所以问题出在哪呢？的确是“更远的部分被截掉了”。我找到了<code>RasterizationSettings</code>里有这么一个相关的参数：</p><ul><li>z_clip_value: if not None, then triangles will be clipped (and possibly subdivided into smaller triangles) such that z &gt;= z_clip_value. This avoids camera projections that go to infinity as z-&gt;0. Default is None as clipping affects rasterization speed and should only be turned on if explicitly needed. See clip.py for all the extra computation that is required.</li></ul><p>可是问题不在这个参数上，因为它的默认值就是None，应该在后续都没有影响。</p><h4 id="解决-3">解决：</h4><p>经过仔细看源码，我发现问题出在<code>SoftPhongShader</code>……具体来说，在<code>shader.py</code> 第138-139行，<code>SoftPhongShader</code>的<code>forward</code>函数里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">znear = kwargs.get(<span class="string">&quot;znear&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;znear&quot;</span>, <span class="number">1.0</span>))</span><br><span class="line">zfar = kwargs.get(<span class="string">&quot;zfar&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;zfar&quot;</span>, <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p>居然有一个默认的z范围[1,100]……………………所以其实是我的mesh的scale太大了，再加上相机的dist比较大，整个深度就超过zfar了。所以有两种方法，要么缩小一下mesh的尺度；要么不想改变原数据的话，在render的时候，把<code>znear</code> <code>zfar</code>参数额外传入，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">images = renderer(mesh, ..., znear=-<span class="number">2.0</span>, zfar=<span class="number">1000.0</span>)</span><br></pre></td></tr></table></figure><h3 id="没解决的问题pbrphysical-based-rendering">6. 没解决的问题：PBR（physical based rendering）</h3><p>我的数据中3D mesh的材质用了PBR（physical based rendering）。它提供三张贴图图像：diffuse map，specular map和normal map。</p><p>但是pytorch3d目前并不支持PBR inspired shading（see <a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>）。</p><p>所以目前我只能把diffuse map作为一般意义上的texture map，而忽略了specular map和normal map这两张图。</p><p>我不确定能不能自己实现这部分功能，比如自定义 <code>phong_shading</code>函数（参考<a href="https://github.com/facebookresearch/pytorch3d/issues/865">issue</a>）。但这有点超出我的能力范围和精力范围，所以暂时搁置了。如果能实现的话，PyTorch3D 似乎是欢迎contribution的（<a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>）</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;pytorch3d-rendering-的一些疑难杂症&quot;&gt;Pytorch3D Rendering 的一些疑难杂症&lt;/h1&gt;
&lt;p&gt;有哪些？&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？&lt;/li&gt;
&lt;li&gt;图像的黄蓝色反了？&lt;/li&gt;
&lt;li&gt;render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？&lt;/li&gt;
&lt;li&gt;皮肤表面反光太强，光滑得像镜面一样，怎样更自然？&lt;/li&gt;
&lt;li&gt;怎么物体只剩半截，更远的部分似乎被截掉了？&lt;/li&gt;
&lt;li&gt;没解决的问题：PBR（physical based rendering）&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="Pytorch3D" scheme="https://jyzhu.top/blog/tags/Pytorch3D/"/>
    
  </entry>
  
  <entry>
    <title>Camera projection with the pinhole model</title>
    <link href="https://jyzhu.top/blog/Camera-Model-Notes/"/>
    <id>https://jyzhu.top/blog/Camera-Model-Notes/</id>
    <published>2023-04-02T13:44:43.000Z</published>
    <updated>2023-04-02T15:47:48.447Z</updated>
    
    <content type="html"><![CDATA[<p>A camera is a mapping between the 3D world (object space) and a 2D image.</p><p>In general, the camera projection matrix P has 11 degrees of freedom: <span class="math display">\[P=K[R\ \ \ t]\]</span></p><table><thead><tr class="header"><th>Component</th><th># DOF</th><th>Elements</th><th>Known As</th></tr></thead><tbody><tr class="odd"><td>K</td><td>5</td><td><span class="math inline">\(f_x, f_y, s,p_x, p_y\)</span></td><td>Intrinsic Parameters; camera calibration matrix</td></tr><tr class="even"><td>R</td><td>3</td><td><span class="math inline">\(\alpha,\beta,\gamma\)</span></td><td>Extrinsic Parameters</td></tr><tr class="odd"><td>t (or <span class="math inline">\(\tilde{C}\)</span>)</td><td>3</td><td><span class="math inline">\((t_x,t_y,t_z)\)</span></td><td>Extrinsic Parameters</td></tr></tbody></table><p>3D world frame ----- R, t ----&gt; 3D camera frame ------ K -----&gt; 2D image</p><p>Explanation:</p><ul><li><p>P: Projective camera, maps 3D world points to 2D image points.</p></li><li><p>K: Camera calibration matrix, 3 x 3, <span class="math inline">\(x=K[I|0]X_{cam}\)</span>, given 3D points in camera coordinate frame <span class="math inline">\(X_{cam}\)</span>, we can project it into 2D points on image <span class="math inline">\(x\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/FEe9lM5t3TJKgyL.png" alt="K" style="zoom:50%;" /></p></li><li><p>R and t: Camera Rotation and Translation, rigid transformation. <span class="math inline">\(X_{cam}=( X,Y,Z,1)^T\)</span> is expressed in the camera coordinate frame. In general, 3D points are expressed in a different Euclidean coordinate frame, known as the <strong>world coordinate frame</strong>. The two frames are related via a rigid transformation (R, t).</p></li></ul><h3 id="some-other-terms-you-may-see">Some other terms you may see</h3><ul><li><p><strong>P</strong>: 3x4, homogeneous, camera projection matrix, <span class="math inline">\(P=diag(f,f,1)[I|0]\)</span>. P is K without considering <span class="math inline">\((x_{cam},y_{cam})\)</span> in the image. (In other words, it simplify <span class="math inline">\((p_x, p_y)=(0,0)\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/TlcW7QFhOAXK6sY.png" alt="P" style="zoom:50%;" /></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;A camera is a mapping between the 3D world (object space) and a 2D image.&lt;/p&gt;
&lt;p&gt;In general, the camera projection matrix P has 11 degrees of freedom: &lt;span class=&quot;math display&quot;&gt;&#92;[
P=K[R&#92; &#92; &#92; t]
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;# DOF&lt;/th&gt;
&lt;th&gt;Elements&lt;/th&gt;
&lt;th&gt;Known As&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;K&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(f_x, f_y, s,p_x, p_y&#92;)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Intrinsic Parameters; camera calibration matrix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;alpha,&#92;beta,&#92;gamma&#92;)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;t (or &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;tilde{C}&#92;)&lt;/span&gt;)&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;((t_x,t_y,t_z)&#92;)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3D world frame ----- R, t ----&amp;gt; 3D camera frame ------ K -----&amp;gt; 2D image&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Configure Academic Page in Jekyll + Blog in Hexo together</title>
    <link href="https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/"/>
    <id>https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/</id>
    <published>2023-03-14T06:24:17.000Z</published>
    <updated>2023-04-02T16:12:37.901Z</updated>
    
    <content type="html"><![CDATA[<h2 id="difficult-situations">Difficult situations:</h2><ol type="1"><li>The Academic page is powered by <a href="http://jekyllrb.com/">Jekyll</a>, while the blog website is powered by Hexo.</li><li>And they are maintained in two separated repositories on Github.</li><li>Besides <code>[username].github.io</code>, I have a domain <code>jyzhu.top</code>, and want to use my custom domain.</li><li>All in all, I hope to visit the academic page is at <a href="jyzhu.top" class="uri">jyzhu.top</a>, while visit the blog is at <a href="jyzhu.top/blog" class="uri">jyzhu.top/blog</a>.</li></ol><h2 id="now-lets-configure.">Now let's configure.</h2><ol type="1"><li><p>Rename the blog repo as <code>blog</code>; rename the academic page repo as <code>[username].github.io</code>.</p></li><li><p>Edit the blog's Hexo config file:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="string">https://jyzhu.top/blog</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/blog/</span></span><br></pre></td></tr></table></figure><p><em>While no need to move all the files into a subfolder <code>blog</code> of your repo.</em></p><p>The Jekyll config is simple. Nothing needs to specify.</p></li><li><p>Edit the Github repo settings. Set the academic repo's <strong>custom domain</strong> as <code>jyzhu.top</code>. A <code>CNAME</code> file will be automatically added in the root. Now obviously, the <code>jyzhu.top</code> successfully refers to the academic page.</p><p>Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of <code>[username].github.io</code> by Github. Then coz <code>[username].github.io</code> is mapped to <code>[url]</code>, everything will be there, including <code>[url]/blog</code> for the <code>blog</code> repo.</p></li></ol><h2 id="todo">TODO</h2><p>The <code>hexo-douban</code> plugin cannot render styles now. Need to fix.</p><p><em>Update</em>:</p><ol type="1"><li>updated hexo-douban to the latest version</li><li>edit the file path of <code>loading.gif</code> in the <code>index.js</code> of this plugin</li></ol><p>then it seems ok now.</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;difficult-situations&quot;&gt;Difficult situations:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The Academic page is powered by &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;, while the blog website is powered by Hexo.&lt;/li&gt;
&lt;li&gt;And they are maintained in two separated repositories on Github.&lt;/li&gt;
&lt;li&gt;Besides &lt;code&gt;[username].github.io&lt;/code&gt;, I have a domain &lt;code&gt;jyzhu.top&lt;/code&gt;, and want to use my custom domain.&lt;/li&gt;
&lt;li&gt;All in all, I hope to visit the academic page is at &lt;a href=&quot;jyzhu.top&quot; class=&quot;uri&quot;&gt;jyzhu.top&lt;/a&gt;, while visit the blog is at &lt;a href=&quot;jyzhu.top/blog&quot; class=&quot;uri&quot;&gt;jyzhu.top/blog&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;now-lets-configure.&quot;&gt;Now let&#39;s configure.&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Rename the blog repo as &lt;code&gt;blog&lt;/code&gt;; rename the academic page repo as &lt;code&gt;[username].github.io&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the blog&#39;s Hexo config file:&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;url:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://jyzhu.top/blog&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;root:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/blog/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;While no need to move all the files into a subfolder &lt;code&gt;blog&lt;/code&gt; of your repo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Jekyll config is simple. Nothing needs to specify.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the Github repo settings. Set the academic repo&#39;s &lt;strong&gt;custom domain&lt;/strong&gt; as &lt;code&gt;jyzhu.top&lt;/code&gt;. A &lt;code&gt;CNAME&lt;/code&gt; file will be automatically added in the root. Now obviously, the &lt;code&gt;jyzhu.top&lt;/code&gt; successfully refers to the academic page.&lt;/p&gt;
&lt;p&gt;Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of &lt;code&gt;[username].github.io&lt;/code&gt; by Github. Then coz &lt;code&gt;[username].github.io&lt;/code&gt; is mapped to &lt;code&gt;[url]&lt;/code&gt;, everything will be there, including &lt;code&gt;[url]/blog&lt;/code&gt; for the &lt;code&gt;blog&lt;/code&gt; repo.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Blog" scheme="https://jyzhu.top/blog/tags/Blog/"/>
    
    <category term="Hexo" scheme="https://jyzhu.top/blog/tags/Hexo/"/>
    
    <category term="Jekyll" scheme="https://jyzhu.top/blog/tags/Jekyll/"/>
    
  </entry>
  
  <entry>
    <title>齐次坐标系</title>
    <link href="https://jyzhu.top/blog/homogeneous-coordinates/"/>
    <id>https://jyzhu.top/blog/homogeneous-coordinates/</id>
    <published>2023-01-26T10:49:19.000Z</published>
    <updated>2023-01-26T10:58:06.245Z</updated>
    
    <content type="html"><![CDATA[<h1 id="齐次坐标系">齐次坐标系</h1><p>之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了</p><h2 id="齐次坐标系的现实意义">齐次坐标系的现实意义</h2><p>就是用来表示现实世界中我们眼睛看到的样子：两条平行线在无限远处能相交。 <embed src="https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp" /></p><h2 id="齐次坐标系的本质">齐次坐标系的本质：</h2><p>就是用N+1维来代表N维坐标。</p><p>也就是说，原本二维空间的点<span class="math inline">\((X,Y)\)</span>，增加一个维度，用<span class="math inline">\((x,y,w)\)</span>来表示。把齐次坐标转换成笛卡尔坐标是很简单的，对前两个维度分别除以最后一个维度的值，就好了，即 <span class="math display">\[X=\frac x w,\  Y=\frac y w\\ (X,Y)=(\frac x w,\frac y w)\]</span> <embed src="https://pic2.zhimg.com/80/v2-da28eed57fda0fc6a06b7122be5f2a1d_1440w.webp" /></p><p>这样做就可以表示两条平行线在远处能相交了！why？</p><p>要解释这个，需要先解释一个齐次坐标系的特点：规模不变性（也是叫homogeneous这个名字的原因）。也就是说，对任意非零的k，<span class="math inline">\((x,y,w)\)</span>和<span class="math inline">\((kx,ky,kw)\)</span>都表示二维空间中同一个点<span class="math inline">\((\frac x w,\frac y w)\)</span>。（因为<span class="math inline">\(\frac{kx}{kw}=\frac xw\)</span>嘛。）</p><p>首先，用原本笛卡尔坐标系中的表示方法，无限远处的点会被表示成<span class="math inline">\((\infty,\infty)\)</span>，从而失去意义。但是我们发现用齐次坐标，我们就有了一个方法明确表示无限远处的任意点，即，<span class="math inline">\((x,y,0)\)</span>。（为什么？因为把它转换回笛卡尔坐标，会得到<span class="math inline">\((\frac x 0,\frac y 0)=(\infty,\infty)\)</span>）。</p><p>现在，用初中所学，联立两条直线的方程，得到的解是两条直线的交点。假如有两条平行线<span class="math inline">\(Ax+By+C=0\)</span>和<span class="math inline">\(Ax+By+D=0\)</span>，求交点，则 <span class="math display">\[\left\{\matrix{Ax+By+C=0 \\Ax+By+D=0}\right.\]</span> 在笛卡尔坐标系中，可知唯一解是<span class="math inline">\(C=D\)</span>，即两条线为同一条直线。</p><p>但是，如果把它换成齐次坐标，得到 <span class="math display">\[\left\{\matrix{A\frac x w+B\frac y w+C=0\\ A\frac x w + B\frac y w +D=0}\right.\]</span></p><p><span class="math display">\[\left\{\matrix{Ax+By+Cw=0\\Ax+By+Dw=0}\right.\]</span></p><p>当<span class="math inline">\(w=0\)</span>，上式变成<span class="math inline">\(Ax+By=0\)</span>，得到解<span class="math inline">\((x,-\frac {A}Bx,0)\)</span>。其实这里的x和y是什么不重要，重要的是w=0，意味着这是个无限远处的点。也就是说，两条平行线在无限远处相交了！甚至能明确求出交点！</p><blockquote><p>Reference:</p><p>http://www.songho.ca/math/homogeneous/homogeneous.html</p><p>https://zhuanlan.zhihu.com/p/373969867</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;齐次坐标系&quot;&gt;齐次坐标系&lt;/h1&gt;
&lt;p&gt;之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了&lt;/p&gt;
&lt;h2 id=&quot;齐次坐标系的现实意义&quot;&gt;齐次坐标系的现实意义&lt;/h2&gt;
&lt;p&gt;就是用来表示现实世界中我们眼睛看到的样子：两条平行线在无限远处能相交。 &lt;embed src=&quot;https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to NeRF</title>
    <link href="https://jyzhu.top/blog/Introduction-to-NeRF/"/>
    <id>https://jyzhu.top/blog/Introduction-to-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-07-28T10:41:26.239Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h1 id="introduction-to-nerf">1. Introduction to NeRF</h1><h2 id="what-is-nerf">What is NeRF</h2><blockquote><p>Reference: Original NeRF paper; an online ariticle</p></blockquote><p>在已知视角下对场景进行一系列的捕获 (包括拍摄到的图像，以及每张图像对应的内外参)，合成新视角下的图像。</p><p>NeRF 想做这样一件事，不需要中间三维重建的过程，仅根据位姿内参和图像，直接合成新视角下的图像。为此 NeRF 引入了辐射场的概念，这在图形学中是非常重要的概念，在此我们给出渲染方程的定义：</p><p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p><p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p><p>那么辐射和颜色是什么关系呢？简单讲就是，光就是电磁辐射，或者说是振荡的电磁场，光又有波长和频率，<span class="math inline">\(波长\times 频率=光速\)</span>，光的颜色是由频率决定的，大多数光是不可见的，人眼可见的光谱称为可见光谱，对应的频率就是我们认为的颜色：</p><p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p><p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p><h3 id="implementation">Implementation</h3><h4 id="mlp-structure">MLP Structure</h4><ol type="1"><li>The net is constrained to be multi-view consistent by restricting the predicting of <span class="math inline">\(\sigma\)</span> to be independent of viewing direction</li><li>While the color <span class="math inline">\(\bold c\)</span> depends on both viewing direction and in-scene coordinate.</li></ol><p>How is this implemented?</p><p>The MLP is designed to be two-stages:</p><ol type="1"><li><span class="math inline">\(F_{\theta_1}(\bold x) = (\sigma, \text{&lt;256 dim features&gt;})\)</span></li><li><span class="math inline">\(F_{\theta_2}(\text{&lt;256 dim features&gt;}, \bold d)=\bold c\)</span></li></ol><h4 id="novel-view-synthesis">Novel view synthesis</h4><p>For each pixel, sample points along the camera ray through this pixel;</p><p>For each sampling point, compute local color and density;</p><p>Use volume rendering, an integral along the camera ray through pixels is used: <span class="math display">\[C(\bold r)=\int_{t_1}^{t_2} T(t)\cdot \sigma (\bold r(t))\cdot \bold c(\bold r(t),\bold d)\cdot dt \\T(t)=\exp (-\int_{t_1}^t \sigma(\bold r(u))\cdot du)\]</span> We can get the color C of the pixel.</p><p>This can be implemented by sampling approaches.</p><p>Now everything can be approximated: <span class="math display">\[\hat C(\bold r)=\sum_{i=1}^N \alpha_iT_i\bold c_i \\T_i=\exp (-\sum_{j=1}^{i-1}\sigma_i\delta_j) \\\alpha_i=1-\exp(\sigma_i\delta_i)\\\delta_i=\text{distance between sampling point i and i+1}\]</span></p><ul><li>Loss is just L2 on color of the pixels:</li></ul><p><span class="math display">\[L=\sum_{r\in R}\| \hat C(\bold r)-C_{gt}(\bold r)\|^2_2\]</span></p><h4 id="depth-regularization">Depth regularization</h4><p>Similar to the above formulas, expected depth can also be calculated, and can be used to regularize the depth smoothness.</p><h4 id="positional-encoding">Positional encoding</h4><p>It is required to greatly improve the fine detail results.</p><p>There are many other positional encoding techs, including trainable parametric, integral, and hierarchical variants</p><h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3><p>SDF是一种计算图形学中定义距离的函数。SDF定义了空间中的点到隐式曲面的距离，该点在曲面内外决定了其SDF的正负性。</p><p>相较于其他像点云（point cloud）、体素（voxel）、面云（mesh）那样的经典3D模型表示方法，SDF有固定的数学方程，更关注物体的表面信息，具有可控的计算成本。</p><h2 id="features-of-nerf">Features of NeRF</h2><ul><li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.<ul><li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li></ul></li><li>Neural Field has advantages:<ol type="1"><li>Compactness 紧致:</li><li>Regularization: nn itself as inductive bias makes it easy to learn</li><li>Domain Agonostic: cheap to add a dimension</li></ol></li><li>also problems<ul><li>Editability / Manipulability</li><li>Computational Complexity</li><li>Spectral Bias</li></ul></li></ul><h2 id="problem-formulation">Problem Formulation</h2><ul><li>Input: multiview images</li><li>Output: 3D Geometry and appearance</li><li>Objective:</li></ul><p><span class="math display">\[\arg \min_x\|y-F(x)\|+\lambda P(x)\]</span></p><p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p><p>F can be differentiable, then you can supervise this.</p><ul><li>nn本身就是某种constraints，你就不需要加太多handicraft constraints</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;introduction-to-nerf&quot;&gt;1. Introduction to NeRF&lt;/h1&gt;
&lt;h2 id=&quot;what-is-nerf&quot;&gt;What is NeRF&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: Original NeRF paper; an online ariticle&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Learning NeRF</title>
    <link href="https://jyzhu.top/blog/Learning-NeRF/"/>
    <id>https://jyzhu.top/blog/Learning-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:37.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-nerf">Learning NeRF</h1><p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="reading-list">Reading List</h2><h3 id="classical">Classical</h3><ul><li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p><p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li><li><p><a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li></ul><h3 id="survey">Survey</h3><ul><li><a href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li></ul><h3 id="cvpr">2021CVPR</h3><p>2021年CVPR还有许多相关的精彩工作发表。例如，提升网络的泛化性：</p><ul><li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>：将每个像素的特征向量而非像素本身作为输入，允许网络在不同场景的多视图图像上进行训练，学习场景先验，然后测试时直接接收一个或几个视图为输入合成新视图。</li><li><a href="https://ibrnet.github.io/">IBRNet</a>：学习一个适用于多种场景的通用视图插值函数，从而不用为每个新的场景都新学习一个模型才能渲染；且网络结构上用了另一个时髦的东西 Transformer。</li><li><a href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>：训练一个具有泛化性能的先验网络，在推理的时候只用3张输入图片就重建一个新的场景。</li></ul><p>针对动态场景的NeRF:</p><ul><li><a href="https://nerfies.github.io/">Nerfies</a>：多使用了一个多层感知机来拟合形变的SE(3) field，从而建模帧间场景形变。Nerfies: Deformable Neural Radiance Fields</li><li><a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>：多使用了一个多层感知机来拟合场景形变的displacement。</li><li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>：多提出了一个scene flow fields来描述时序的场景形变。</li></ul><p>其他创新点：</p><ul><li><a href="https://kai-46.github.io/PhySG-website/">PhySG</a>：用球状高斯函数模拟BRDF（高级着色的上古神器）和环境光照，针对更复杂的光照环境，能处理非朗伯表面的反射。</li><li><a href="https://nex-mpi.github.io/">NeX</a>：用MPI（Multi-Plane Image ）代替NeRF的RGBσ作为网络的输出。</li></ul><h3 id="cvpr-1">2022 CVPR</h3><p><a href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p><h2 id="useful-references"><strong>Useful References:</strong></h2><blockquote><p><a href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p><p><a href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p><p><a href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p><p><a href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p></blockquote><p>Bigger to learn:</p><ul><li>[ ] Above NeRF: neural rendering</li><li>[ ] Related theories in graphics and computer vision</li><li>[ ] NeRF的一作Ben Mildenhall在SIGGRAPH 2021 Course <a href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>中从概率的角度推导了NeRF的体渲染公式。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;learning-nerf&quot;&gt;Learning NeRF&lt;/h1&gt;
&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;reading-list&quot;&gt;Reading List&lt;/h2&gt;
&lt;h3 id=&quot;classical&quot;&gt;Classical&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Manipulate Neural Fields</title>
    <link href="https://jyzhu.top/blog/Manipulate-Neural-Fields/"/>
    <id>https://jyzhu.top/blog/Manipulate-Neural-Fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:11:38.197Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2><p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p><figure><img src="https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption></figure><p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p><p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p><figure><img src="https://s2.loli.net/2023/01/12/S7HWcQPh1FtdJaw.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption></figure><p>The following examples 落在不同的象限。</p><h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3><ul><li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li><li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li><li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p><figure><img src="https://s2.loli.net/2023/01/12/y4bGulHpfOwWkqN.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption></figure><ul><li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p><figure><img src="https://s2.loli.net/2023/01/12/SBzGy3rnUaqLFI8.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption></figure></li></ul></li><li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p><figure><img src="https://s2.loli.net/2023/01/12/UbFu74iCQ15mK3B.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption></figure></li></ul><h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/zxFElDIuSnPioJ7.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption></figure><p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p><p>We need to define the consistency here, so that the network can learn through forward and backward:</p><figure><img src="https://s2.loli.net/2023/01/12/VS1K3rQxHXYPRIg.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption></figure><h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3><p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p><figure><img src="https://s2.loli.net/2023/01/12/w2XEyYn5qbh41OB.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption></figure><ul><li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li></ul><figure><img src="https://s2.loli.net/2023/01/12/ruER6MJPpljSZiX.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption></figure><ul><li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/HL61itcqsIaEThX.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/y7eCcKDmdUY4VOu.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption></figure><ul><li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/2lTvenQixfRm8Po.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption></figure><ul><li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li></ul><h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/5Ohb7ExW4osc1n2.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption></figure><ul><li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li><li>Beyond geometry, we can also manipulate <strong>color</strong></li></ul><figure><img src="https://s2.loli.net/2023/01/12/vM1QwkT4BJqGNIR.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption></figure><p>It decomposes the network into shape and color networks, and we can edit each independently.</p><figure><img src="https://s2.loli.net/2023/01/12/38HNsE9GnF1pydQ.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption></figure><ul><li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;manipulate-neural-fields&quot;&gt;2.5. Manipulate Neural Fields&lt;/h2&gt;
&lt;p&gt;Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png&quot; alt=&quot;image-20221212211525928&quot;&gt;&lt;figcaption&gt;image-20221212211525928&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Differentiable Forward Maps</title>
    <link href="https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/"/>
    <id>https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:34.505Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2><figure><img src="https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption></figure><h3 id="differentiable-rendering">Differentiable rendering</h3><figure><img src="https://s2.loli.net/2023/01/12/1Ng8wz2KP4oTiVH.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption></figure><p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p><h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3><ul><li>BRDF Shading? details later.</li></ul><h3 id="differentiation-itself">Differentiation itself</h3><p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p><figure><img src="https://s2.loli.net/2023/01/12/Gi6IaAkhvlBxpoe.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption></figure><p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p><p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p><h3 id="special-identity-operator">Special: Identity Operator</h3><p><span class="math display">\[\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\\text{Reconstruction} == \text{Sensor domain}\]</span></p><p>Q&amp;A:</p><ul><li>Can we obtain a neural network in just one forward, without optimization?</li><li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;differentiable-forward-maps&quot;&gt;2.3. Differentiable Forward Maps&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png&quot; alt=&quot;image-20221208175453557&quot;&gt;&lt;figcaption&gt;image-20221208175453557&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;differentiable-rendering&quot;&gt;Differentiable rendering&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Hybrid representations</title>
    <link href="https://jyzhu.top/blog/NeRF-Hybrid-representations/"/>
    <id>https://jyzhu.top/blog/NeRF-Hybrid-representations/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:55.420Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="hybrid-representations">2.2. Hybrid representations</h2><h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3><figure><img src="https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/sDg8FHQcjrGRW1b.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption></figure><p>You may choose one proper representation depending on your own application</p><h3 id="grid">1. Grid</h3><figure><img src="https://s2.loli.net/2023/01/12/ZpYEMbXvRdeOqiy.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption></figure><p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p><figure><img src="https://s2.loli.net/2023/01/12/lMi3tK9NPgcWUaI.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption></figure><p>NeRFusion CVPR22: online!</p><h3 id="point-cloud">2. point cloud</h3><figure><img src="https://s2.loli.net/2023/01/12/zhYHQnFsgxBLCfM.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption></figure><p>Cons:</p><ol type="1"><li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li><li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li></ol><p><img src="https://s2.loli.net/2023/01/12/37EbFsANOCc9voX.png" alt="image-20221208163050867" style="zoom:50%;" /></p><h3 id="mesh">3. Mesh</h3><p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p><figure><img src="https://s2.loli.net/2023/01/12/Dwir9hsm3VgZjQk.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/nPMw7T3hqRAU2iv.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption></figure><h3 id="multiplanar-images">4. Multiplanar Images</h3><p>Something like project a 3D grid into an axis to get levels of planes.</p><figure><img src="https://s2.loli.net/2023/01/12/CmhFDT5NoiMAOnv.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption></figure><p>Pros:</p><ol type="1"><li>Compact</li><li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li></ol><p>Cons:</p><ol type="1"><li>Resolution bias on plane axis: coz it is discrete betweens planes.</li></ol><p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p><p><img src="https://s2.loli.net/2023/01/12/UDO6HlWAp3y7qF1.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p><h3 id="multiresolution-grids">5. Multiresolution grids</h3><figure><img src="https://s2.loli.net/2023/01/12/TlmkK1NDj2dAtUp.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption></figure><p>Pros:</p><ol start="2" type="1"><li>Stable coz you indeed need both low and high resolution info</li></ol><h3 id="hash-grids">6. Hash grids</h3><p><img src="https://s2.loli.net/2023/01/12/NZtH7wfxpSPVkGe.png" alt="image-20221208170131069" /> <span class="math display">\[[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}\]</span> Pros:</p><ol type="1"><li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li><li>Can be online!</li></ol><p>Cons:</p><ol type="1"><li>May still need large codebooks</li><li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li></ol><h3 id="codebook-grids">7. Codebook grids</h3><figure><img src="https://s2.loli.net/2023/01/12/pnBJGxM6jNHD2v5.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption></figure><p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p><p>cons:</p><ol type="1"><li>To make the indexing operation differentiable, the computing complexity rises here.</li><li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li></ol><h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3><figure><img src="https://s2.loli.net/2023/01/12/LQ6fzh21OltTJxq.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption></figure><p>Commonly used method in computer graphics</p><h3 id="others-voxel">9. Others (voxel)</h3><figure><img src="https://s2.loli.net/2023/01/12/5sE1MYpOLB4mPCF.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption></figure><ul><li>For dynamic nerfs, is there any better hybrid representation? Sure.</li><li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;hybrid-representations&quot;&gt;2.2. Hybrid representations&lt;/h2&gt;
&lt;h3 id=&quot;tradeoffs-of-choosing-a-proper-representation&quot;&gt;Tradeoffs of choosing a proper representation&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png&quot; alt=&quot;image-20221208172055153&quot;&gt;&lt;figcaption&gt;image-20221208172055153&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
</feed>
