<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tianke Youke</title>
  
  <subtitle>A sanctuary for secreting and rushing at night.</subtitle>
  <link href="https://jyzhu.top/blog/atom.xml" rel="self"/>
  
  <link href="https://jyzhu.top/blog/"/>
  <updated>2023-07-21T11:29:22.901Z</updated>
  <id>https://jyzhu.top/blog/</id>
  
  <author>
    <name>Jiayin Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reading Handy: Towards a high fidelity 3D hand shape and appearance model</title>
    <link href="https://jyzhu.top/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/"/>
    <id>https://jyzhu.top/blog/Reading-Handy-Towards-a-high-fidelity-3D-hand-shape-and-appearance-model/</id>
    <published>2023-07-17T11:32:07.000Z</published>
    <updated>2023-07-21T11:29:22.901Z</updated>
    
    <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://rolpotamias.github.io/Handy/</p><p>ä½œè€…ï¼šRolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou. From Imperial College London and Cosmos.</p><p>å‘è¡¨ï¼š CVPR23</p><p>é“¾æ¥ï¼š <a href="https://github.com/rolpotamias/handy" class="uri">https://github.com/rolpotamias/handy</a></p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><p>Qï¼šæˆ‘æ„Ÿè§‰è¿™ä¸ªä»»åŠ¡å¬èµ·æ¥è¿˜æŒºç›´è§‚çš„ï¼Œå°±æ˜¯ç”¨GANå»è®­ç»ƒå¤–è§‚ï¼Œå®šä¹‰ä¸€äº›æ›´å¤šverticesçš„mesh templateï¼Œç”¨è¶…çº§å¤§é‡çš„æ ·æœ¬å»è®­ç»ƒå †æ•ˆæœå˜›ï¼Ÿhand modelçš„å®šä¹‰ä¼šæœ‰ä»€ä¹ˆæ–°æ„å—ï¼Ÿæˆ‘å€’æ˜¯æƒ³ä¸å‡ºæ¥ã€‚</p><p>Aï¼šç¡®å®å¾ˆç›´è§‚ï¼Œhand modelçš„å®šä¹‰æ²¡ä»€ä¹ˆå¤ªå¤§åŒºåˆ«ã€‚è´¡çŒ®ç‚¹ä¸»è¦åœ¨äºï¼š1. å¾ˆå¤§å¾ˆå¥½å¾ˆvariantçš„æ–°æ•°æ®é›†ï¼Œé€ æˆäº†å¾ˆå¥½çš„Handy 2. ç”¨StyleGANæ¥å­¦textureï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„PCAï¼Œå¾—åˆ°çš„textureæ›´é«˜é¢‘ç»†èŠ‚ï¼Œæ›´å¥½ã€‚</p><h2 id="why">Whyï¼š</h2><ol type="1"><li>VR ARå‘å±•ï¼Œå¯¹äººæ‰‹çš„å»ºæ¨¡ã€è¿½è¸ªå’Œé‡å»ºçš„ç ”ç©¶å˜å¾—æµè¡Œï¼Œå› ä¸ºæ‰‹æ˜¯ä¸€ä¸ªé‡è¦çš„æ˜¾ç¤ºäººçš„è¡Œä¸ºçš„ä¸œè¥¿</li><li>å¤§éƒ¨åˆ†å·¥ä½œåŸºäºMANOï¼Œåªæœ‰å¾ˆç²—ç³™çš„low polygon countï¼Œè€Œä¸”åªåŸºäº31ä¸ªæ ·æœ¬æ„å»ºï¼Œdistributionä¸å¤Ÿå®½</li><li>å¤§éƒ¨åˆ†å·¥ä½œéƒ½å¿½ç•¥äº†æè´¨çš„æ„å»º</li></ol><h2 id="what">Whatï¼š</h2><ol type="1"><li>æå‡ºä¸€ä¸ªlarge-scaleçš„hand modelï¼ŒåŒ…å«äº†å½¢çŠ¶å’Œå¤–è§‚ï¼Œç”¨è¶…è¿‡1200ä¸ªäººç±»æ ·æœ¬è®­ç»ƒï¼Œæ ·æœ¬æœ‰large diversity</li><li>æ„å»ºSynthetic datasetï¼Œè®­ç»ƒä¸€ä¸ªhand pose estimationç½‘ç»œï¼Œä»å•å¼ å›¾åƒä¸­é‡å»ºæ‰‹</li><li>æå‡ºä¸€ä¸ªåŸºäºGANçš„æœ‰é«˜é¢‘ç»†èŠ‚çš„æ‰‹çš„å¤–è§‚+å½¢çŠ¶é‡å»ºæ–¹æ³•ï¼Œå³ä½¿æ˜¯in-the-wildçš„å•è§†è§’å›¾åƒä½œä¸ºè¾“å…¥</li></ol><p>è¯»å‰ç–‘é—®ï¼š</p><ol type="1"><li>çœ‹ä¸Šå»ä½œè€…æ˜¯ç”¨NeRFåšäº†ä¸€ä¸ªhigh fidelityçš„hand modelã€‚æˆ‘ä¸å¤ªæ¸…æ¥šæŠ€æœ¯ç»†èŠ‚å¦‚ä½•å®ç°ï¼Œå°¤å…¶æ˜¯nerfå¦‚ä½•è·Ÿparametric modelç»“åˆï¼Œå¦‚æœè®­ç»ƒä¸€ä¸ªnerf layerï¼Œè®©å®ƒå¯ä»¥æ ¹æ®å•å¼ è¾“å…¥å›¾åƒinferä¸€ä¸ªæ–°æ‰‹ã€‚ä¸çŸ¥é“æˆ‘å“ªé‡Œæ¥çš„è¯¯è§£ï¼Œæ€»ä¹‹ä¸æ˜¯ç”¨çš„nerfè¯¶â€¦â€¦</li><li>fig 1 çœ‹ä¸Šå»æ•ˆæœæœ‰ç‚¹å‡â€¦â€¦ä¼¼ä¹æ˜¯çš®è‚¤åå…‰ç‡çš„é—®é¢˜ï¼Œç”¨çš„ä»€ä¹ˆlighting representationå‘¢ï¼Ÿæ²¡ä»€ä¹ˆrepresentationï¼Œçº¯ç²¹ç”¨PCAå»æ‰äº†é˜´å½±æˆåˆ†</li><li>å±…ç„¶è¿çš±çº¹ã€è¡€ç®¡ã€æŒ‡ç”²æ²¹ä¹Ÿèƒ½å‡ºæ¥ï¼Œç¡®å®æ˜¯é«˜é¢‘ç»†èŠ‚äº†ã€‚æœ‰é’ˆå¯¹è¿™äº›ä¸œè¥¿åšç‰¹åˆ«çš„ä¼˜åŒ–å—ï¼Ÿè¿˜æ˜¯å…¨æ˜¯é‚£ä¸ªstyle-based GANçš„åŠŸåŠ³ï¼Œæˆ–è€…å¤§æ ·æœ¬é‡çš„åŠŸåŠ³å‘¢ï¼ŸçœŸæ˜¯å¤§åŠ›å‡ºå¥‡è¿¹å‘€ã€‚è¿˜çœŸå°±æ˜¯GANçš„åŠŸåŠ³â€¦â€¦ï¼Ÿ</li></ol><h2 id="how">Howï¼š</h2><h3 id="æ”¶é›†large-scaleæ•°æ®é›†">1. æ”¶é›†large-scaleæ•°æ®é›†</h3><p>raw scanï¼š3000 vertices meshesã€‚1208ä¸ªäººï¼ŒåŒ…æ‹¬å…³äºä»–ä»¬çš„meta dataï¼Œæ¯”å¦‚æ€§åˆ«ï¼Œå¹´é¾„ï¼Œèº«é«˜ï¼Œç§æ—ç­‰ã€‚è¿™äº›äººçš„diversityæ¯”è¾ƒå¤§</p><h3 id="å½¢çŠ¶é‡å»º">2. å½¢çŠ¶é‡å»º</h3><ol type="1"><li>å¯¹é½3D scans å’Œ mesh template<ol type="1"><li>ç”¨äº†ä¸¤ç»„templateï¼Œä¸€ä¸ªæ˜¯ä½åˆ†è¾¨ç‡çš„MANOï¼Œå®ƒå¯ä»¥ç›´æ¥ç”¨è¿›SMPLäººä½“æ¨¡å‹ä¸­ï¼Œæœ‰778ä¸ªé¡¶ç‚¹ï¼›ä¸€ä¸ªæ˜¯é«˜åˆ†è¾¨çš„templateï¼Œæœ‰8407ä¸ªé¡¶ç‚¹</li><li>è·å¾—ç¨ å¯†çš„correspondenceçš„æ–¹æ³•æ˜¯ï¼š<ol type="1"><li>ä»å¤šè§†è§’æ¸²æŸ“è¿™äº›raw scansï¼Œç”¨MediaPipeæ¥æ£€æµ‹2Då…³é”®ç‚¹</li><li>ç”¨linear triangulationæ¥æŠŠ2Då…³é”®ç‚¹è½¬æ¢åˆ°3Dï¼›åˆ©ç”¨æ‰‹æŒ‡éª¨æ¶åˆ°è¡¨é¢å°–ç«¯çš„æŠ•å½±æ¥æ£€æµ‹æŒ‡å°–ã€‚</li><li>ç”¨3Då…³é”®ç‚¹æ¥æŠŠtemplateå’Œ3D scansçš„è¡¨é¢å¯¹é½</li><li>ç”¨Non-rigid Iterative Closest Point algorithm (NICP)æ¥registrationï¼Œå¯»æ‰¾ç¨ å¯†çš„é¡¶ç‚¹å¯¹åº”å…³ç³»</li></ol></li></ol></li><li>è½¬æ¢æˆè§„èŒƒçš„å¼ å¼€æ‰‹æŒçš„å§¿åŠ¿<ol type="1"><li>ç”¨PCAæ„å»ºä¸€ä¸ªæ‰‹éƒ¨å½¢çŠ¶æ¨¡å‹ã€‚</li><li>å…¬å¼å’ŒMANOå‡ ä¹ä¸€æ ·ï¼Œ<span class="math inline">\(\beta\)</span> <span class="math inline">\(\theta\)</span> ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯å½¢çŠ¶å’Œå§¿åŠ¿å‚æ•°ã€‚</li></ol></li></ol><h3 id="é«˜åˆ†è¾¨ç‡å¤–è§‚æ¨¡å‹">3. é«˜åˆ†è¾¨ç‡å¤–è§‚æ¨¡å‹</h3><ol type="1"><li>å«ä¸€ä¸ªå›¾åƒå­¦è‰ºæœ¯å®¶ï¼ˆğŸ˜³ï¼‰è®¾è®¡äº†ä¸€ä¸ªUV templateï¼ŒæŠŠscansç»™unwrapæˆé‚£æ ·äº†</li><li>å¯¹UV texturesè¿›è¡Œé¢„å¤„ç†ï¼Œå»æ‰é˜´å½±å’Œå…‰ç…§ï¼šç”¨PCAæ¥è¯†åˆ«æè¿°é˜´å½±çš„å› ç´ ï¼Œç„¶åæŠŠè¿™äº›å› ç´ å»æ‰ã€‚ï¼ˆPCAå±…ç„¶è¿™ä¹ˆå¥½ç”¨ï¼Ÿï¼ï¼‰</li><li>ç”¨ä¸€ä¸ªå›¾åƒå¤„ç†æ­¥éª¤ï¼Œå°†æ‰‹éƒ¨çº¹ç†æ˜ å°„åˆ°æ›´è‡ªç„¶çš„é¢œè‰²ï¼ŒåŒ…æ‹¬å¢åŠ äº®åº¦ï¼Œä¼½ç›æ ¡æ­£ï¼Œä»¥åŠè°ƒæ•´è‰²è°ƒã€‚</li><li>è®­ç»ƒè¿‡ç¨‹ï¼šä¸åƒå…¶ä»–æ–¹æ³•é‚£æ ·ç›´æ¥æŠŠå¤–è§‚ç©ºé—´æ˜ å°„åˆ°ä¸€ä¸ªä½é¢‘PCAåŸŸï¼Œè€Œæ˜¯ç”¨GANæ¥å»ºæ¨¡æè´¨ã€‚å­¦ä¹ ç‡è¾ƒå°ï¼Œ0.001ï¼›ä¸€ä¸ªæ­£åˆ™æƒé‡50ä¹Ÿå¾ˆæœ‰æ•ˆã€‚ï¼ˆå•Šï¼Ÿè¿™ä¸ªGANå°±è¿™ä¹ˆä¸€å¥å¸¦è¿‡å—ï¼Ÿç›´æ¥ç”¨çš„StyleGANv3ï¼Ÿï¼‰</li></ol><h3 id="å®éªŒ">å®éªŒ</h3><ol type="1"><li>å’ŒMANOæ¯”hand modelï¼š<ol type="1"><li>æ›´ç´§è‡´ï¼Œ5ä¸ªä¸»æˆåˆ†è¡¨ç°90% varianceï¼Œmanoéœ€è¦9ä¸ªæ‰è¡Œ</li><li>æ³›åŒ–åˆ°æ•°æ®é›†å¤–çš„æ‰‹çš„èƒ½åŠ›æ›´å¼º</li><li>ç‰¹å¼‚æ€§è¯¯å·®ï¼ˆspecificity errorï¼‰ï¼Ÿè¡¡é‡ç”Ÿæˆçš„æ‰‹å’Œground truthçš„è¯¯å·®</li></ol></li><li>é‡å»ºå°å­©çš„æ‰‹ï¼Œæ•ˆæœæ›´å¥½</li><li>ä»å•å¼ å›¾åƒè¿›è¡Œ3Dé‡å»ºï¼š<ol type="1"><li>ç”Ÿæˆæ•°æ®é›†ï¼šç”¨è‡ªå·±è®­çš„GANæ¨¡å‹ç”Ÿæˆ30000å¼ å›¾åƒï¼Œä¸ºäº†æ›´çœŸå®ï¼Œæ¸²æŸ“çš„æ‰‹è·ŸShapeNetä¸­çš„ç‰©ä½“æœ‰äº¤äº’ï¼Œä»¥åŠæ˜¯å’Œç”¨SMPLè¡¨ç¤ºçš„äººæ”¾åœ¨ä¸€èµ·çš„</li><li>æ¨¡å‹ç›´æ¥å‚è€ƒ3ï¼Œ14ï¼Œ16ï¼›åŠ äº†ä¸€ä¸ªé¢„æµ‹æè´¨å‚æ•°çš„åˆ†æ”¯</li><li>lossï¼šL2 between estimated and gt shape parameterï¼Œ pose parameterï¼Œand 3D verticesï¼› L1 between estimated and gt UV mapï¼›L1 between estimated and gt 2D imageï¼›LPIPS loss on two images</li><li>å¦å¤–è®¾è®¡äº†in-the-wildæ•°æ®é›†ï¼Œç”¨é¢„è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹handy å§¿åŠ¿ã€å½¢çŠ¶å’Œæè´¨å‚æ•°ï¼Œç„¶ååªä¼˜åŒ–æè´¨å‚æ•°wæ¥æ‹Ÿåˆæè´¨ã€‚</li><li>ä¼˜åŒ–å‡½æ•°åŒ…æ‹¬L1 and LPIPS loss on two imagesï¼Œä»¥åŠä¸€ä¸ªå¯¹wçš„L2æ­£åˆ™ã€‚å¾—åˆ°äº†æ”¹è¿›çš„æè´¨å‚æ•°wâ€˜ä¹‹åï¼Œfinetuneå›å½’ç½‘ç»œã€‚</li><li>ä¸ºäº†å®šé‡è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„çº¹ç†é‡å»ºï¼Œæˆ‘ä»¬å‘ç½‘ç»œæä¾›æ•°æ®ä¸­ä½¿ç”¨çš„æ‰«æè®¾å¤‡çš„å›¾åƒã€‚gt UV mapç”¨çš„æ˜¯ä¹‹å‰registrationåå¾—åˆ°çš„ã€‚ï¼ˆæˆ‘ä¸ç†è§£è¯¶ï¼Œè¿™æ ·çœŸçš„èƒ½è·ŸHTMLå…¬å¹³æ¯”è¾ƒå—ï¼Ÿä¸€æ–¹é¢ä½ çš„handyå°±æ˜¯ä»è¿™äº›æ•°æ®ä¸­æ¥çš„ï¼Œå½“ç„¶èƒ½å¯¹in-distributionçš„ä¸œè¥¿æ‹Ÿåˆå¾—æ›´å¥½å•Šï¼Ÿå¦ä¸€æ–¹é¢HTMLç”Ÿæˆçš„UV mapå’Œä½ çš„å®šä¹‰æ˜¯ä¸€æ ·çš„å—ï¼Ÿè¿™ä¸ªgt UV mapå¯¹å®ƒæ¥è¯´æœ‰ç”¨å—ï¼Ÿï¼‰</li><li>ç»“è®ºæ˜¯ï¼šhandy+GANèƒ½å¾—åˆ°é«˜é¢‘ç»†èŠ‚ï¼Œç”šè‡³çš±çº¹ã€æˆ’æŒ‡ã€çº¹èº«ã€æŒ‡ç”²æ²¹ã€ç™½ç™œé£ä¹‹ç±»çš„ï¼›handy+PCAä¼šè¿‡æ¸¡å¹³æ»‘ï¼Œç”šè‡³å¯¹è‚¤è‰²çš„é‡å»ºå¤±è´¥ï¼›HTMLæ›´ä¸è¡Œã€‚</li></ol></li><li>Test on FreiHand åˆ·æ–°äº†æŒ‡æ ‡ï¼Œ7.8 MPVPE and MPJPEâ€¦â€¦</li><li>ä»ç‚¹äº‘é‡å»ºå½¢çŠ¶å’Œå§¿åŠ¿ã€‚é™ç»´æ‰“å‡»äº†MANOå’ŒLISAï¼Œå³ä½¿ç”¨Hand+MANO+10ä¸ªPCA Componentsï¼Œä¹Ÿæ¯”å…¶ä»–æ–¹æ³•å¥½å¾ˆå¤šâ€¦â€¦</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;è®ºæ–‡åœ°å€ï¼šhttps://rolpotamias.github.io/Handy/&lt;/p&gt;
&lt;p&gt;ä½œè€…ï¼šRolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, Stefanos Zafeiriou. From Imperial College London and Cosmos.&lt;/p&gt;
&lt;p&gt;å‘è¡¨ï¼š CVPR23&lt;/p&gt;
&lt;p&gt;é“¾æ¥ï¼š &lt;a href=&quot;https://github.com/rolpotamias/handy&quot; class=&quot;uri&quot;&gt;https://github.com/rolpotamias/handy&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
    <category term="Hand" scheme="https://jyzhu.top/blog/tags/Hand/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT Applications to be explored</title>
    <link href="https://jyzhu.top/blog/ChatGPT-Applications-to-be-explored/"/>
    <id>https://jyzhu.top/blog/ChatGPT-Applications-to-be-explored/</id>
    <published>2023-07-16T17:57:48.000Z</published>
    <updated>2023-07-16T18:01:58.015Z</updated>
    
    <content type="html"><![CDATA[<p>ä»Šå¤©é€› githubï¼Œå‘ç°äº†ä¸€äº›å¾ˆ amazing çš„chatgpt applicationsï¼Œæ‘˜å½•ä¸€äº›æ„Ÿå…´è¶£çš„ç²¾ååœ¨æ­¤ã€‚çœŸæ˜¯æ„Ÿæ…¨ï¼šLLM ä»¥æ¥å¤©å¤©é£äº‘å˜å¹»ï¼Œå¼„æ½®å„¿åœ¨å‰é¢å…´é£ä½œæµªï¼Œæˆ‘åœ¨åé¢æœ›å…¶é¡¹èƒŒâ€¦â€¦</p><ol type="1"><li><p>(Useful) egoist / openai-proxy</p><p>ç”¨ Vercel å¼€ä¸€ä¸ªå°çš„ Proxy serverï¼Œè½¬å‘ gpt APIï¼Œè¿™æ ·å¯ä»¥ç»•å¼€æœ‰äº›å›½å®¶åœ°åŒºçš„ IP é™åˆ¶</p></li><li><p>BuilderIO / ai-shell</p><p>åœ¨å‘½ä»¤è¡Œé‡Œä½¿ç”¨ chatgptï¼ŒæŠŠè‡ªç„¶è¯­è¨€è½¬åŒ–æˆ Linux commandsï¼Œå‘½ä»¤æ˜¯ <code>ai [texts]</code></p></li><li><p>eli64s / readme-ai</p><p>ä¸€ä¸ªè½»é‡çš„ scriptï¼Œæ ¹æ® repository ç”Ÿæˆé…·ç‚«çš„ readme æ–‡ä»¶</p></li><li><p>efJerryYang / chatgpt-cli</p><p>å‘½ä»¤è¡Œ chatgpt client</p></li><li><p>yufeikang / ai-cli</p><p>å¦ä¸€ä¸ªå‘½ä»¤è¡Œ chatgpt clientï¼ˆå®æµ‹çš„æ—¶å€™å†å¯¹æ¯”ä¸€ä¸‹è¿™ä¿©ï¼‰</p></li><li><p>mukulpatnaik / researchgpt</p><p>è¾“å…¥è®ºæ–‡ PDF æ–‡ä»¶ï¼Œç„¶åå’Œ gpt èŠè®ºæ–‡ã€‚ä¸€ä¸ªç”¨ Flask å¼€å‘çš„ web client è²Œä¼¼ï¼Œå¯ä»¥å†ä»”ç»†çœ‹ä¸€ä¸‹å’‹å®ç°çš„ï¼ŒæŒºæœ‰æ„æ€</p></li><li><p>(â­ï¸ Amazing) AntonOsika / gpt-engineer</p><p>å¾ˆæ–¹ä¾¿å®‰è£…ï¼Œpip install å°±å¥½äº†ï¼ç›´æ¥é€šè¿‡æè¿° + AI è¿½é—® + è¡¥å……ç»†èŠ‚ï¼Œç”Ÿæˆä¸€ä¸ªä»£ç é¡¹ç›®</p></li><li><p>(â­ï¸ Amazing) Yidadaa / ChatGPT-Next-Web</p><p>å¥½åƒå¾ˆå®ç”¨çš„ web GUIï¼ä¸€é”®éƒ¨ç½²åˆ° Vercelã€‚æˆ‘æ‰¾è¿™ç©æ„ä¸»è¦æ˜¯ä¸ºäº†ç›´æ¥ç”¨ API è®¿é—® GPT-4ï¼Œå°±ä¸ç”¨è®¢é˜…æ¯ä¸ªæœˆçš„ ChatGPT Plus äº†ï¼Œåè€…å¤ªè´µäº†ï¼Œä¹Ÿç”¨ä¸äº†é‚£ä¹ˆå¤š</p></li></ol><p><img src="https://s2.loli.net/2023/07/17/QU2X5ckaCmyJeGv.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ä»Šå¤©é€› githubï¼Œå‘ç°äº†ä¸€äº›å¾ˆ amazing çš„chatgpt applicationsï¼Œæ‘˜å½•ä¸€äº›æ„Ÿå…´è¶£çš„ç²¾ååœ¨æ­¤ã€‚çœŸæ˜¯æ„Ÿæ…¨ï¼šLLM ä»¥æ¥å¤©å¤©é£äº‘å˜å¹»ï¼Œå¼„æ½®å„¿åœ¨å‰é¢å…´é£ä½œæµªï¼Œæˆ‘åœ¨åé¢æœ›å…¶é¡¹èƒŒâ€¦â€¦&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;(Useful) eg</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="LLM" scheme="https://jyzhu.top/blog/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Back to Homeland</title>
    <link href="https://jyzhu.top/blog/Back-to-Homeland/"/>
    <id>https://jyzhu.top/blog/Back-to-Homeland/</id>
    <published>2023-07-05T12:44:49.000Z</published>
    <updated>2023-07-16T18:19:38.272Z</updated>
    
    <content type="html"><![CDATA[<p>ä»Šå¤©ï¼Œæ©æ–½çš„å‚æ™šä¼¼ä¹æ²¡æœ‰å¤•é˜³ã€‚å¤©ç©ºæ˜¯æ·±è“è‰²ï¼Œæ·±å±±ä¸Šæœ‰å‡ åªé»‘é¸Ÿæ è¿‡ã€‚åšé‡çš„ç‹—å«å£°ã€‚æˆ‘é—»åˆ°ä¸€ç§ä½¿æˆ‘æ„Ÿåˆ°æ‚²æˆšçš„æ°”å‘³ï¼Œæˆ–è®¸æ˜¯å’Œè¿‡å»æŸç§æ‚²æˆšçš„å›å¿†è”ç³»èµ·æ¥ã€‚å…·ä½“å›å¿†å†…å®¹å€’ä¹Ÿè®°ä¸æ¸…ã€‚æ˜å¤©æˆ‘è¦èµ°äº†ï¼Œæˆ‘è¯´è¦åƒç‚¹è¾£çš„ï¼Œå»äº†å—è¾¹åˆæ²¡æœ‰äº†ã€‚ç»“æœè¿˜æ˜¯å»åƒäº†æ½®æ±•ç‰›è‚‰ã€‚å¦ˆå¦ˆä»Šå¤©æ¯”è¾ƒæ˜“æ€’ï¼Œæˆ‘çœ‹å¾—åˆ°å¥¹çš„ä¼¤æ„Ÿã€‚</p><p>å›å›½ï¼Œè¿™ä¸ªè¯å¯¹æˆ‘æ¥è¯´ä¸ç®—ä»€ä¹ˆï¼Œèº«å¤„å›½å†…çš„æ—¶å€™ï¼Œè‡ªå·±æ˜¯æ²¡æœ‰æ¦‚å¿µçš„ã€‚åªæœ‰åœ¨å›½å¤–çš„æ—¶å€™ï¼Œæ‰å¯¹å›½å†…æœ‰æ¦‚å¿µã€‚è¿™ä¸€ç‚¹çœŸæ˜¯è®½åˆºã€‚å›½å†…çš„æ—¶é—´åŒ†åŒ†è€Œè¿‡ï¼Œä»¥å‰ä¼šä¸ºäº†åœ¨å›½å¤–å¤šå¾…å‡ å¹´è€Œäº‰å–ï¼Œäº‰å–åˆ°äº†åˆé›€è·ƒã€‚ç°åœ¨çœŸè¦å¾…é‚£ä¹ˆå¤šå¹´äº†ï¼Œæ‰æ„Ÿåˆ°è‡ªå·±æ­£ååœ¨ä»€ä¹ˆé£é©°è€Œè¿œå»çš„åˆ—è½¦ä¸Šã€‚</p><p>äººç”Ÿåœ¨ä¸–ï¼Œå¥”å¤´è¿™ä¸ªè¯ï¼Œæˆ‘æ˜¯æƒ³è§£æ„å®ƒçš„ã€‚äººæ´»ç€ä¸ä¸ºäº†ä»€ä¹ˆï¼Œæ´»å°±æ´»äº†ã€‚å¯æ˜¯æˆ‘åˆ†æ˜åœ¨åŠªåŠ›ä»€ä¹ˆï¼Œåœ¨æŠ“ä½äº›ä»€ä¹ˆã€‚å˜´ç¡¬ç½¢äº†ï¼Œè°èƒ½è¶…å‡¡è„±ä¿—ï¼Œæ²¡ç‚¹æƒ¦å¿µçš„ä¸œè¥¿ï¼Ÿäº²äººï¼Œå‘å¾€çš„æŸç§ç”Ÿæ´»ï¼Œæˆå°±æ„Ÿï¼Œè¿™å°±æ˜¯æˆ‘çš„å¥”å¤´ã€‚åªæ˜¯ï¼šåœåœ¨åŸåœ°åŸæ¥æ˜¯ä¸€ç§å¹¸è¿ï¼Œä¹Ÿæ˜¯ä¸€ç§ç‰¹æƒã€‚</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ä»Šå¤©ï¼Œæ©æ–½çš„å‚æ™šä¼¼ä¹æ²¡æœ‰å¤•é˜³ã€‚å¤©ç©ºæ˜¯æ·±è“è‰²ï¼Œæ·±å±±ä¸Šæœ‰å‡ åªé»‘é¸Ÿæ è¿‡ã€‚åšé‡çš„ç‹—å«å£°ã€‚æˆ‘é—»åˆ°ä¸€ç§ä½¿æˆ‘æ„Ÿåˆ°æ‚²æˆšçš„æ°”å‘³ï¼Œæˆ–è®¸æ˜¯å’Œè¿‡å»æŸç§æ‚²æˆšçš„å›å¿†è”ç³»èµ·æ¥ã€‚å…·ä½“å›å¿†å†…å®¹å€’ä¹Ÿè®°ä¸æ¸…ã€‚æ˜å¤©æˆ‘è¦èµ°äº†ï¼Œæˆ‘è¯´è¦åƒç‚¹è¾£çš„ï¼Œå»äº†å—è¾¹åˆæ²¡æœ‰äº†ã€‚ç»“æœè¿˜æ˜¯å»åƒäº†æ½®æ±•ç‰›è‚‰ã€‚å¦ˆå¦ˆä»Šå¤©æ¯”è¾ƒæ˜“æ€’ï¼Œæˆ‘çœ‹å¾—åˆ°å¥¹çš„ä¼¤</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>å¥¶å¥¶</title>
    <link href="https://jyzhu.top/blog/Grandma/"/>
    <id>https://jyzhu.top/blog/Grandma/</id>
    <published>2023-07-03T18:08:23.000Z</published>
    <updated>2023-07-03T18:11:44.610Z</updated>
    
    <content type="html"><![CDATA[<p>å¥¶å¥¶èº«ä½“è¿˜å¥½ï¼Œä½†å¿ƒæ€ä¸å¥½ã€‚å¥å¿˜ã€å›ºæ‰§ï¼Œè‡ªé—­ï¼Œæ‚²è§‚ã€‚å¥¹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆç›¼å¤´äº†ã€‚å¥¹è§‰å¾—æ´»å¾—å¤±è´¥ï¼Œæ´»å¾—ä¸å¥½ã€‚å¥¹è§‰å¾—å¦‚ä»Šè„¸ä¸Šæ— å…‰ã€‚å¥¹è¯´ï¼š</p><p>æˆ‘æœ€ç—›æ¨åˆ«äººï¼ŒåŠç†Ÿä¸ç†Ÿçš„äººè§é¢ï¼Œè·Ÿä½ æ‰“æ‹›å‘¼ï¼Œé—®ä½ å®¶é‡Œè¿‘å†µæ€ä¹ˆæ ·ã€‚æˆ‘å¾ˆç”Ÿæ°”ï¼Œç®€ç›´æƒ³éª‚å›å»ã€‚å¯æ˜¯ä½ éª‚äº†å§ï¼Œäººå®¶è§‰å¾—ä½ æ˜¯ç¥ç»ç—…ã€‚å¯æ˜¯æˆ‘è¦æ€ä¹ˆå›ç­”å‘¢ï¼Ÿæˆ‘è¿™ä¸¤ä¸ªå„¿å­ï¼Œä¸€ä¸ªå·¥ä½œéƒ½ä¸¢äº†ï¼Œä¸€ä¸ªèº«ä½“åˆé‚£æ ·ä¸å¥½ã€‚</p><p>æˆ‘å‘½è‹¦å•Šã€‚æˆ‘ä»å°å®¶é‡Œç©·ï¼Œä¹Ÿæ²¡äººç®¡æˆ‘ã€‚13å²å°±å‡ºå»å·¥ä½œå…»æ´»è‡ªå·±äº†ã€‚è·Ÿäº†ä½ çˆ·çˆ·ï¼Œè¿‡çš„éƒ½æ˜¯è‹¦æ—¥å­ã€‚ä»–ä¸€ä¸ªæœˆä¸‰åå››å—äº”è§’é’±ï¼ŒäºŒåå—é’±ç»™ä»–çˆ¸å¦ˆï¼Œé›·æ‰“ä¸åŠ¨çš„ã€‚å®¶é‡Œé¥­éƒ½åƒä¸èµ·äº†ï¼Œä¹Ÿè¦ç»™ã€‚å‰©ä¸‹åå››å—äº”ï¼Œåå—é’±ç»™æˆ‘ï¼Œä»–ç•™å››å—äº”ã€‚æŠ½æœ€å·®çš„çƒŸï¼Œèµ°å¾—é‚£ä¹ˆæ—©ã€‚</p><p>æˆ‘è¦ç®¡å®¶é‡Œæ‰€æœ‰äº‹ã€‚ä»–ä»€ä¹ˆä¹Ÿä¸ç®¡å•Šï¼Œä¸€æ—¥ä¸‰é¤ï¼Œä»–å¦ˆï¼Œä¸¤ä¸ªå„¿å­ï¼Œå»æ²³è¾¹æ´—è¡£æœã€‚æˆ‘è¿˜è¦ä¸Šç­ã€‚æˆ‘æ¯å¤©éƒ½å¥½ç´¯ã€‚</p><p>å¥½ä¸å®¹æ˜“ï¼Œæ—¥å­ç¨å¾®å¥½ä¸€ç‚¹äº†ã€‚ä»–åˆèµ°äº†ï¼æˆ‘é‚£æ—¶ç®€ç›´æ¨ä»–ã€‚</p><p>åˆ«çœ‹æˆ‘ç°åœ¨è¿™æ ·ï¼Œç°åœ¨æ˜¯æˆ‘æœ€è½»æ¾çš„æ—¥å­ã€‚å…±äº§å…šç»™æˆ‘å‘é’±ã€‚æ‰€ä»¥æˆ‘è¯´ï¼Œæ„Ÿè°¢å…±äº§å…šã€‚æˆ‘æ‹¿è‡ªå·±çš„é’±ï¼Œè¿‡è‡ªå·±çš„æ—¥å­ï¼å°±æ˜¯è®°æ€§ä¸å¥½ã€‚äººæ´»åˆ°è¿è‡ªç†èƒ½åŠ›ä¹Ÿæ²¡æœ‰äº†ï¼Œè¿˜æœ‰ä»€ä¹ˆæ„æ€ã€‚è·Ÿä½ çˆ¸è¯´å¥½äº†ï¼Œåˆ°æ—¶å€™æˆ‘æ­»åœ¨è¿™é—´å±‹å­äº†ï¼Œå°±ç«åŒ–ã€‚æˆ‘éƒ½çœ‹å¾—å¼€ã€‚</p><p>æ­»ï¼æˆ‘å¬åˆ°å°±çœ¼æ³ªç›´æ¹æ¹åœ°æµã€‚å¥¶å¥¶è¯´ï¼Œä¸è¯´è¿™ä¸ªäº†ã€‚å¥¹ä¹ŸæŠ¹çœ¼æ³ªã€‚</p><p>å¥¶å¥¶ä¸€ä¸ªæœˆé¢†ä¸‰åƒå¤šé€€ä¼‘å·¥èµ„ï¼Œè‡ªå·±çœåƒä¿­ç”¨ï¼ŒåªèŠ±å¾—äº†ä¸€åƒå¤šã€‚å‰©çš„ï¼Œå­˜åœ¨é‚£ï¼Œæ¯å¹´å¯’æš‘å‡æˆ‘å’Œå¦¹å¦¹å»çœ‹å¥¹ï¼Œå¥¹å‘ç»™æˆ‘ä»¬ã€‚</p><p>æˆ‘è¯´çˆ¸çˆ¸ï¼Œä½ å¸®æˆ‘æŠŠé’±é€€ç»™å¥¹ã€‚</p><p>çˆ¸çˆ¸è¯´ä½ æ‹¿ç€ã€‚å¥¹è‡ªå·±èŠ±ä¸å®Œï¼Œè¿™é’±ä¸ç»™ä½ ä»¬ï¼Œå¥¹æ‹¿ç€æœ‰ä»€ä¹ˆç”¨äº†ï¼Ÿç»™äº†å¥¹å®‰å¿ƒã€‚</p><p>æˆ‘èµ°çš„æ—¶å€™ï¼Œå¥¹é€å‡ºå±‹å­ï¼Œé€åˆ°æ¥¼æ¢¯å£ã€‚</p><p>æˆ‘èµ°è¿œäº†ï¼Œçˆ¸çˆ¸è¯´ï¼Œä½ å›å¤´å†ç»™å¥¶å¥¶æŒ¥æŒ¥æ‰‹ï¼Œä½ çœ‹å¥¹åœ¨é˜³å°ä¸Šçœ‹ä½ å‘¢ã€‚</p><p>æˆ‘è½¬å¤´çœ‹åˆ°ä¸€ç°‡èŠ±ç™½çš„å¤´å‘ï¼Œè¿œè¿œçš„ï¼Œå¾ˆå°ã€‚</p><p>èƒ½ä¸èƒ½ä¸è¦æœ‰ç¦»åˆ«å‘¢ï¼Ÿ</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;å¥¶å¥¶èº«ä½“è¿˜å¥½ï¼Œä½†å¿ƒæ€ä¸å¥½ã€‚å¥å¿˜ã€å›ºæ‰§ï¼Œè‡ªé—­ï¼Œæ‚²è§‚ã€‚å¥¹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆç›¼å¤´äº†ã€‚å¥¹è§‰å¾—æ´»å¾—å¤±è´¥ï¼Œæ´»å¾—ä¸å¥½ã€‚å¥¹è§‰å¾—å¦‚ä»Šè„¸ä¸Šæ— å…‰ã€‚å¥¹è¯´ï¼š&lt;/p&gt;
&lt;p&gt;æˆ‘æœ€ç—›æ¨åˆ«äººï¼ŒåŠç†Ÿä¸ç†Ÿçš„äººè§é¢ï¼Œè·Ÿä½ æ‰“æ‹›å‘¼ï¼Œé—®ä½ å®¶é‡Œè¿‘å†µæ€ä¹ˆæ ·ã€‚æˆ‘å¾ˆç”Ÿæ°”ï¼Œç®€ç›´æƒ³éª‚å›å»ã€‚å¯æ˜¯ä½ éª‚äº†å§ï¼Œäººå®¶è§‰å¾—ä½ æ˜¯ç¥ç»ç—…ã€‚å¯æ˜¯æˆ‘è¦æ€ä¹ˆå›ç­”å‘¢ï¼Ÿæˆ‘è¿™ä¸¤ä¸ªå„¿å­ï¼Œä¸€ä¸ªå·¥ä½œéƒ½ä¸¢äº†ï¼Œä¸€ä¸ªèº«ä½“åˆé‚£æ ·ä¸å¥½ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å‘½è‹¦å•Šã€‚æˆ‘ä»å°å®¶é‡Œç©·ï¼Œä¹Ÿæ²¡äººç®¡æˆ‘ã€‚13å²å°±å‡ºå»å·¥ä½œå…»æ´»è‡ªå·±äº†ã€‚è·Ÿäº†ä½ çˆ·çˆ·ï¼Œè¿‡çš„éƒ½æ˜¯è‹¦æ—¥å­ã€‚ä»–ä¸€ä¸ªæœˆä¸‰åå››å—äº”è§’é’±ï¼ŒäºŒåå—é’±ç»™ä»–çˆ¸å¦ˆï¼Œé›·æ‰“ä¸åŠ¨çš„ã€‚å®¶é‡Œé¥­éƒ½åƒä¸èµ·äº†ï¼Œä¹Ÿè¦ç»™ã€‚å‰©ä¸‹åå››å—äº”ï¼Œåå—é’±ç»™æˆ‘ï¼Œä»–ç•™å››å—äº”ã€‚æŠ½æœ€å·®çš„çƒŸï¼Œèµ°å¾—é‚£ä¹ˆæ—©ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘è¦ç®¡å®¶é‡Œæ‰€æœ‰äº‹ã€‚ä»–ä»€ä¹ˆä¹Ÿä¸ç®¡å•Šï¼Œä¸€æ—¥ä¸‰é¤ï¼Œä»–å¦ˆï¼Œä¸¤ä¸ªå„¿å­ï¼Œå»æ²³è¾¹æ´—è¡£æœã€‚æˆ‘è¿˜è¦ä¸Šç­ã€‚æˆ‘æ¯å¤©éƒ½å¥½ç´¯ã€‚&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>Solutions to Common Problems in Pytorch3D Rendering</title>
    <link href="https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/"/>
    <id>https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/</id>
    <published>2023-04-23T06:22:24.000Z</published>
    <updated>2023-04-23T06:42:33.231Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch3d-rendering-çš„ä¸€äº›ç–‘éš¾æ‚ç—‡">Pytorch3D Rendering çš„ä¸€äº›ç–‘éš¾æ‚ç—‡</h1><p>æœ‰å“ªäº›ï¼Ÿ</p><ol type="1"><li>æœ‰äº†ç›¸æœºå†…å‚ Kï¼Œè€Œrenderåˆéœ€è¦NDCåæ ‡ç³»ï¼Œé‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœºï¼Ÿ</li><li>å›¾åƒçš„é»„è“è‰²åäº†ï¼Ÿ</li><li>render å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡ï¼Ÿæ€ä¹ˆæŠ—é”¯é½¿ï¼ˆAntialiasingï¼‰ï¼Ÿ</li><li>çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºï¼Œå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·ï¼Œæ€æ ·æ›´è‡ªç„¶ï¼Ÿ</li><li>æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªï¼Œæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†ï¼Ÿ</li><li>æ²¡è§£å†³çš„é—®é¢˜ï¼šPBRï¼ˆphysical based renderingï¼‰</li></ol><span id="more"></span><h3 id="æœ‰äº†ç›¸æœºå†…å‚-kè€Œrenderåˆéœ€è¦ndcåæ ‡ç³»é‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœº">1. æœ‰äº†ç›¸æœºå†…å‚ Kï¼Œè€Œrenderåˆéœ€è¦NDCåæ ‡ç³»ï¼Œé‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœºï¼Ÿ</h3><p>è¿™é‡Œçš„å‘åœ¨äºï¼Œcameraæœ¬èº«æ”¯æŒä»»æ„åæ ‡ç³»ï¼Œæ¯”å¦‚Freihandæä¾›çš„æ˜¯screenæ˜¯224*224çš„ç›¸æœºåæ ‡ç³»ã€‚ä½†æ˜¯ï¼Œrenderæ˜¯é»˜è®¤NDCåæ ‡ç³»çš„ï¼ä¹Ÿå°±æ˜¯normalized coordinate systemï¼Œxå’Œyæ˜¯normalizedåˆ°[-1,1]çš„ã€‚</p><p>ä¸€å¼€å§‹æˆ‘ç›´æ¥æŠŠç›¸æœºå†…å‚ä¼ ç»™<code>PerspectiveCameras</code>ï¼Œå¹¶ä¸”å®šä¹‰æˆ‘çš„ç›¸æœºscreenæ˜¯224*224ï¼Œåƒè¿™æ ·ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>å®Œå…¨ä¸æŠ¥é”™ï¼Œå°±æ˜¯æœ‰é—®é¢˜ï¼šrender è¿‡åæ²¡ä¸œè¥¿åœ¨ç”»é¢ä¸Šã€‚</p><h4 id="è§£å†³">è§£å†³ï¼š</h4><p>æˆ‘æœ€ååœ¨<a href="https://pytorch3d.org/docs/cameras">å®˜æ–¹æ–‡æ¡£</a>æ‰¾åˆ°ä¸èµ·çœ¼çš„ä¸€å¥ï¼š</p><blockquote><p>The PyTorch3D renderer for both meshes and point clouds assumes that the camera transformed points, meaning the points passed as input to the rasterizer, are in PyTorch3D's NDC space.</p></blockquote><figure><img src="https://user-images.githubusercontent.com/669761/145090051-67b506d7-6d73-4826-a677-5873b7cb92ba.png" alt="ï¼ˆä¸–ç•Œåæ ‡ç³» -&gt; ç›¸æœºåæ ‡ç³» -&gt; ndcåæ ‡ç³» -&gt; å›¾åƒåæ ‡ç³»ï¼‰" /><figcaption>ï¼ˆä¸–ç•Œåæ ‡ç³» -&gt; ç›¸æœºåæ ‡ç³» -&gt; ndcåæ ‡ç³» -&gt; å›¾åƒåæ ‡ç³»ï¼‰</figcaption></figure><p>æˆ‘ä¸€çœ‹ï¼ŒåŸæ¥é»˜è®¤PerspectiveCamerasæ˜¯ndcåæ ‡ç³»çš„ï¼Œ<code>in_ndc = False</code> by defaultï¼</p><p>æ‰€ä»¥è§£å†³æ–¹æ³•å°±æ˜¯ï¼š</p><blockquote><p>Screen space camera parameters are common and for that case the user needs to set <code>in_ndc</code> to <code>False</code> and also provide the <code>image_size=(height, width)</code> of the screen, aka the image.</p></blockquote><p>é‚£ä¹ˆåŠ ä¸€ä¸ªå‚æ•°å°±å¥½äº†ï¼Œå¯æ˜¯è°çŸ¥é“è¿™é—®é¢˜å›°æ‰°äº†æˆ‘æ•´æ•´ä¸¤ä¸‰å¤©ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, in_ndc=<span class="literal">False</span>, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>å¦å¤–ï¼Œæˆ‘è¿˜æ‰¾åˆ°äº†å¦‚ä¸‹è¿™ä¸ªç­‰ä»·æ–¹æ³•ï¼Œæ˜¯å…ˆæŠŠå†…å‚è½¬åˆ°NDCåæ ‡ç³»ï¼Œå†ä¼ ç»™<code>PerspectiveCameras</code>ã€‚ï¼ˆè‡³äºä¸ºä»€ä¹ˆæ¢ç´¢åˆ°è¿™ä¸ªæ–¹æ³•ï¼Œåœ¨åé¢é—®é¢˜ 3 é‡Œå¯ä»¥æ‰¾åˆ°åŸå› â€¦ï¼‰</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ndc_fcl_prp</span>(<span class="params">Ks</span>):</span></span><br><span class="line">        ndc_fx = Ks[:, <span class="number">0</span>, <span class="number">0</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_fy = Ks[:, <span class="number">1</span>, <span class="number">1</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_px = - (Ks[:, <span class="number">0</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_py = - (Ks[:, <span class="number">1</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        focal_length = torch.stack([ndc_fx, ndc_fy], dim=-<span class="number">1</span>)</span><br><span class="line">        principal_point = torch.stack([ndc_px, ndc_py], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> focal_length, principal_point</span><br><span class="line"></span><br><span class="line">fcl, prp = get_ndc_fcl_prp(Ks)</span><br><span class="line">cameras = PerspectiveCameras(focal_length=-fcl, principal_point=prp)</span><br></pre></td></tr></table></figure><p>æ³¨æ„<code>focal_length=-fcl</code>ï¼Œè¿™ä¸ªè´Ÿå·æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯å¦ä¸€ä¸ªå‘äº†å“ˆå“ˆå“ˆå“ˆã€‚</p><p>ç­”æ¡ˆæ˜¯ï¼špytorch3dåæ ‡ç³»çš„conventionå’Œæˆ‘çš„ç›¸æœºä¸ä¸€æ ·ï¼Œå®ƒæ˜¯+XæŒ‡å‘å·¦ï¼Œ+YæŒ‡å‘ä¸Šï¼Œ+ZæŒ‡å‘å›¾åƒå¹³é¢å¤–ã€‚è¿™å…¶ä¸­æœ‰ä¸ªä¸Šä¸‹å·¦å³é•œåƒçš„å…³ç³»ã€‚</p><h3 id="å›¾åƒçš„é»„è“è‰²åäº†">2. å›¾åƒçš„é»„è“è‰²åäº†ï¼Ÿ</h3><p>cv2çš„å›¾åƒæ˜¯BGRï¼ˆè€ç”Ÿå¸¸è°ˆäº†ï¼‰ï¼Œpytorch3dçš„æ˜¯RGBã€‚å¦‚æœå›¾åƒçš„é»„è“è‰²ç›¸åäº†ï¼ŒåŸºæœ¬å°±æ˜¯è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦ç¿»è½¬ä¸€ä¸‹ï¼Œå¯ä»¥ç”¨torchçš„<code>clip(dim=(2,))</code></p><h3 id="render-å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡æ€ä¹ˆæŠ—é”¯é½¿antialiasing">3. render å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡ï¼Ÿæ€ä¹ˆæŠ—é”¯é½¿ï¼ˆAntialiasingï¼‰ï¼Ÿ</h3><p>é”¯é½¿å°±æ˜¯è¯´åƒä¸‹å›¾è¿™æ ·ï¼Œç‰©ä½“çš„è¾¹ç¼˜å¾ˆå°–é”ï¼Œåƒç´ ç‚¹ç²’ç²’åˆ†æ˜ï¼</p><figure><img src="https://s2.loli.net/2023/04/23/6gx5DEXJK9uMGwS.png" alt="rand_4_skin_rendered_bad" /><figcaption>rand_4_skin_rendered_bad</figcaption></figure><p>ä¸‹é¢æ˜¯æˆ‘æŠ—é”¯é½¿å¤„ç†åçš„æ•ˆæœï¼Œå¯ä»¥çœ‹è§è¾¹ç¼˜æŸ”å’Œäº†å¾ˆå¤šï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/LAchPHVfImRtkDs.png" alt="rand_4_skin_rendered" /><figcaption>rand_4_skin_rendered</figcaption></figure><p>ï¼ˆæˆ‘çœŸçš„æäº†ä¸€å‘¨è¿™ä¸ªé—®é¢˜â€¦â€¦çœ‹çœ‹æˆ‘çš„å¿ƒè·¯å†ç¨‹ï¼š</p><ol type="1"><li>æ˜¯ä¸æ˜¯ camera æ²¡æœ‰ç”¨ NDCï¼Œè€Œæ˜¯ç›´æ¥ç”¨224x224çš„åæ ‡ç³»ï¼Œå¯¼è‡´æŠ•å½±è¿‡ç¨‹æœ‰æŸå¤±ï¼Ÿæ‰€ä»¥æˆ‘è¯•äº†å…ˆè½¬æ¢æˆ NDC åæ ‡ç³»çš„ç›¸æœºï¼Œå†renderã€‚ç­”æ¡ˆæ˜¯ï¼Œæ²¡æœ‰å½±å“ã€‚</li><li>æ˜¯ä¸æ˜¯ Shader çš„å‚æ•°è®¾ç½®å¾—ä¸å¯¹ï¼Œæ¯”å¦‚ <code>blur_radius</code> å’Œ <code>faces_per_pixel</code> åº”è¯¥è°ƒå¤§ä¸€äº›ï¼Ÿè¿™å…¶å®æ˜¯ä¸€ä¸ªå¾ˆç›´è§‚çš„æƒ³æ³•äº†ï¼Œç”šè‡³ä¸€ä¸ªæœ‰ç»éªŒçš„å­¦é•¿çœ‹äº†ä¹‹åéƒ½å‘Šè¯‰æˆ‘åº”è¯¥æ˜¯è¿™ä¸ªé—®é¢˜ã€‚å¯æ˜¯å½“æˆ‘ç–¯ç‹‚è°ƒå¤§è¿™ä¸¤ä¸ªå‚æ•°ï¼Œå‘ç°å¹¶æ²¡æœ‰æ”¹å˜è¿™ä¸ªé—®é¢˜ã€‚blur_radius åªä¼šè®©ç‰©ä½“å†…éƒ¨çš„æè´¨æ›´æ¨¡ç³Šï¼Œä½†æ˜¯è¾¹ç¼˜çš„é”¯é½¿å®Œå…¨æ²¡æ”¹å˜ã€‚faces_per_pixelæ›´æ˜¯æ— ç›Šï¼Œå‡ ä¹ä¸å½±å“æ•ˆæœã€‚</li><li>æ˜¯ä¸æ˜¯å›¾åƒå°ºå¯¸å¤ªå°äº†ï¼ˆ224x224ï¼‰ï¼Œåªèƒ½è¾¾åˆ°è¿™ä¹ˆä¸ªæ•ˆæœï¼Ÿæˆ‘é¦–å…ˆæµ‹è¯•äº†è°ƒå¤§å›¾åƒå°ºå¯¸ï¼Œåˆ°<code>1024x1024</code>ï¼Œå‘ç°é”¯é½¿è¾¹ç¼˜çš„ç¡®æ˜¯ä¸æ˜æ˜¾äº†ï¼å¯æ˜¯æˆ‘åˆçœ‹äº†ç›¸æœºæ‹æ‘„çš„åŸå§‹å›¾åƒï¼Œè™½ç„¶æ˜¯æœ‰ç‚¹æ¨¡ç³Šï¼Œä½†æ˜¯ä¸è‡³äºè¿™ä¹ˆå¤§çš„é”¯é½¿å‘€ï¼Œè‚¯å®šè¿˜æœ‰åˆ«çš„é—®é¢˜ã€‚ï¼‰</li></ol><h4 id="è§£å†³-1">è§£å†³ï¼š</h4><p>ç»ˆäºï¼Œåœ¨è¿™ä¸ªissueé‡Œæ‰¾åˆ°åŒæ ·çš„é—®é¢˜ï¼šhttps://github.com/facebookresearch/pytorch3d/issues/399</p><p>è§£å†³æ–¹æ¡ˆæ˜¯ï¼š</p><blockquote><p>render at a higher resolution and then use average pooling to reduce back to the target resolution</p></blockquote><p>å±…ç„¶è¿™ä¹ˆæš´åŠ›â€¦â€¦ä¸è¿‡issueé‡Œé¢æœ‰å¾ˆè¯¦ç»†çš„è§£é‡Šï¼Œä¹Ÿèƒ½ç†è§£ï¼Œè¿™å°±æ˜¯renderåŸç†ä¹‹å¤–éœ€è¦è€ƒè™‘çš„äº‹æƒ…ï¼Œç”šè‡³ç®—ä¸ä¸Šä»€ä¹ˆbugã€‚</p><p>ä»£ç å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">aa_factor = <span class="number">3</span> <span class="comment"># Anti-aliasing factor</span></span><br><span class="line">raster_settings_soft = RasterizationSettings(</span><br><span class="line">        image_size=<span class="number">224</span> * aa_factor, </span><br><span class="line">    )</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">images = renderer(mesh)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># NHWC -&gt; NCHW</span></span><br><span class="line">images = F.avg_pool2d(images, kernel_size=aa_factor, stride=aa_factor)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; NHWC</span></span><br></pre></td></tr></table></figure><h3 id="çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·æ€æ ·æ›´è‡ªç„¶">4. çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºï¼Œå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·ï¼Œæ€æ ·æ›´è‡ªç„¶ï¼Ÿ</h3><p>ä¸€å¼€å§‹ï¼Œçš®è‚¤ render å‡ºæ¥åƒè¿™æ ·ï¼Œè·Ÿé™¶ç“·ä¼¼çš„ï¼Œåƒè¯å—ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/8NWzr7txYKyqRk6.png" alt="rand_4_skin_rendered_bad2" /><figcaption>rand_4_skin_rendered_bad2</figcaption></figure><p>æ”¹è¿›åï¼Œæ•ˆæœè¿™æ ·ï¼Œè‡ªç„¶å¤šäº†ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/4qHNOs58ZhEI6Ry.png" alt="rand_4_skin_rendered_big" /><figcaption>rand_4_skin_rendered_big</figcaption></figure><h4 id="è§£å†³-2">è§£å†³ï¼š</h4><p>å…¶å®ææ¸…æ¥šæè´¨ç›¸å…³çš„ä¸€äº›å‚æ•°å°±å¥½äº†ã€‚ä¸»è¦æ¥è¯´ï¼Œè¿™ä¸ªåå…‰æ˜¯ç”±è¿™ä¸¤ä¸ªé‡å†³å®šçš„ï¼š</p><ol type="1"><li><code>specular_color</code>: specular reflectivity of the materialï¼ŒæŒ‡å®šé•œé¢åå°„é¢œè‰²ï¼Œåœ¨è¡¨é¢æœ‰å…‰æ³½å’Œé•œé¢èˆ¬çš„åœ°æ–¹çœ‹åˆ°çš„é¢œè‰²ã€‚</li><li><code>shininess</code>ï¼šå®šä¹‰æè´¨ä¸­é•œé¢åå°„é«˜å…‰çš„ç„¦ç‚¹ã€‚ å€¼é€šå¸¸ä»‹äº 0 åˆ° 1000 ä¹‹é—´ï¼Œè¾ƒé«˜çš„å€¼ä¼šäº§ç”Ÿç´§å¯†ã€é›†ä¸­çš„é«˜å…‰ã€‚</li></ol><p>æ³¨æ„è¿™é‡Œæ˜¯æ”¹ç‰©ä½“materialçš„è¿™äº›å‚æ•°ã€‚è™½ç„¶lightingä¹Ÿæœ‰è¿™äº›å‚æ•°å®šä¹‰ï¼Œä½†è¿™æ˜¯å…³äºå…‰æºçš„ï¼Œå’Œè¿™ä¸ªåå…‰æ²¡æœ‰å…³ç³»ã€‚</p><p>æ‰€ä»¥ä¿®æ”¹å¾ˆç®€å•ï¼šå®šä¹‰materialsç±»ï¼Œè°ƒæ•´<code>specular_color</code>ã€‚é»˜è®¤æ˜¯<code>1,1,1</code>ï¼Œå°±æ˜¯çº¯ç™½è‰²ï¼›è°ƒæˆ<code>0.2,0.2,0.2</code>æ¯”è¾ƒé€‚åˆäººçš„çš®è‚¤ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch3d.renderer <span class="keyword">import</span> Materials</span><br><span class="line"></span><br><span class="line">materials = Materials(</span><br><span class="line">    specular_color=((<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),), <span class="comment"># é»˜è®¤æ˜¯1,1,1ï¼Œå°±æ˜¯çº¯ç™½è‰²ï¼›æµ‹è¯•å‘ç°è°ƒæˆ0.2,0.2,0.2æ¯”è¾ƒé€‚åˆäººçš„çš®è‚¤ã€‚</span></span><br><span class="line">    shininess=<span class="number">30</span>, <span class="comment"># é»˜è®¤å€¼æ˜¯ 64ï¼Œçœ‹ä¸Šå»é«˜å…‰ç¨å¾®æœ‰ç‚¹èšé›†äº†ï¼Œæ”¹æˆ30çš„è¯ç•¥è‡ªç„¶ï¼Œå·®åˆ«ä¸å¤ªæ˜æ˜¾</span></span><br><span class="line">)</span><br><span class="line"> renderer_p3d = MeshRenderer(</span><br><span class="line">    rasterizer=MeshRasterizer(),</span><br><span class="line">    shader=HardPhongShader(</span><br><span class="line">        materials=materials,</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†">5. æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªï¼Œæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†ï¼Ÿ</h3><p>è¿˜æ˜¯ä¸€åªæ‰‹çš„æ¨¡å‹ï¼Œrender å‡ºæ¥å±…ç„¶åªæœ‰åŠä¸ªæ‰‹èƒŒï¼Œè·ç¦»ç›¸æœºæ›´è¿œçš„éƒ¨åˆ†åƒæ˜¯è¢«æˆªæ–­äº†ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/imzqovVyWfnK3Fs.png" alt="rand_1_skin_rendered_half" /><figcaption>rand_1_skin_rendered_half</figcaption></figure><p>æ”¹è¿›åï¼Œæ­£å¸¸çš„æ•ˆæœåº”è¯¥æ˜¯è¿™æ ·æ‰å¯¹ï¼š</p><figure><img src="https://s2.loli.net/2023/04/23/gSJ5wm74RUjHODY.png" alt="rand_1_skin_rendered_full" /><figcaption>rand_1_skin_rendered_full</figcaption></figure><p>æ‰€ä»¥é—®é¢˜å‡ºåœ¨å“ªå‘¢ï¼Ÿçš„ç¡®æ˜¯â€œæ›´è¿œçš„éƒ¨åˆ†è¢«æˆªæ‰äº†â€ã€‚æˆ‘æ‰¾åˆ°äº†<code>RasterizationSettings</code>é‡Œæœ‰è¿™ä¹ˆä¸€ä¸ªç›¸å…³çš„å‚æ•°ï¼š</p><ul><li>z_clip_value: if not None, then triangles will be clipped (and possibly subdivided into smaller triangles) such that z &gt;= z_clip_value. This avoids camera projections that go to infinity as z-&gt;0. Default is None as clipping affects rasterization speed and should only be turned on if explicitly needed. See clip.py for all the extra computation that is required.</li></ul><p>å¯æ˜¯é—®é¢˜ä¸åœ¨è¿™ä¸ªå‚æ•°ä¸Šï¼Œå› ä¸ºå®ƒçš„é»˜è®¤å€¼å°±æ˜¯Noneï¼Œåº”è¯¥åœ¨åç»­éƒ½æ²¡æœ‰å½±å“ã€‚</p><h4 id="è§£å†³-3">è§£å†³ï¼š</h4><p>ç»è¿‡ä»”ç»†çœ‹æºç ï¼Œæˆ‘å‘ç°é—®é¢˜å‡ºåœ¨<code>SoftPhongShader</code>â€¦â€¦å…·ä½“æ¥è¯´ï¼Œåœ¨<code>shader.py</code> ç¬¬138-139è¡Œï¼Œ<code>SoftPhongShader</code>çš„<code>forward</code>å‡½æ•°é‡Œï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">znear = kwargs.get(<span class="string">&quot;znear&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;znear&quot;</span>, <span class="number">1.0</span>))</span><br><span class="line">zfar = kwargs.get(<span class="string">&quot;zfar&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;zfar&quot;</span>, <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p>å±…ç„¶æœ‰ä¸€ä¸ªé»˜è®¤çš„zèŒƒå›´[1,100]â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦æ‰€ä»¥å…¶å®æ˜¯æˆ‘çš„meshçš„scaleå¤ªå¤§äº†ï¼Œå†åŠ ä¸Šç›¸æœºçš„distæ¯”è¾ƒå¤§ï¼Œæ•´ä¸ªæ·±åº¦å°±è¶…è¿‡zfaräº†ã€‚æ‰€ä»¥æœ‰ä¸¤ç§æ–¹æ³•ï¼Œè¦ä¹ˆç¼©å°ä¸€ä¸‹meshçš„å°ºåº¦ï¼›è¦ä¹ˆä¸æƒ³æ”¹å˜åŸæ•°æ®çš„è¯ï¼Œåœ¨renderçš„æ—¶å€™ï¼ŒæŠŠ<code>znear</code> <code>zfar</code>å‚æ•°é¢å¤–ä¼ å…¥ï¼Œå¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">images = renderer(mesh, ..., znear=-<span class="number">2.0</span>, zfar=<span class="number">1000.0</span>)</span><br></pre></td></tr></table></figure><h3 id="æ²¡è§£å†³çš„é—®é¢˜pbrphysical-based-rendering">6. æ²¡è§£å†³çš„é—®é¢˜ï¼šPBRï¼ˆphysical based renderingï¼‰</h3><p>æˆ‘çš„æ•°æ®ä¸­3D meshçš„æè´¨ç”¨äº†PBRï¼ˆphysical based renderingï¼‰ã€‚å®ƒæä¾›ä¸‰å¼ è´´å›¾å›¾åƒï¼šdiffuse mapï¼Œspecular mapå’Œnormal mapã€‚</p><p>ä½†æ˜¯pytorch3dç›®å‰å¹¶ä¸æ”¯æŒPBR inspired shadingï¼ˆsee <a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>ï¼‰ã€‚</p><p>æ‰€ä»¥ç›®å‰æˆ‘åªèƒ½æŠŠdiffuse mapä½œä¸ºä¸€èˆ¬æ„ä¹‰ä¸Šçš„texture mapï¼Œè€Œå¿½ç•¥äº†specular mapå’Œnormal mapè¿™ä¸¤å¼ å›¾ã€‚</p><p>æˆ‘ä¸ç¡®å®šèƒ½ä¸èƒ½è‡ªå·±å®ç°è¿™éƒ¨åˆ†åŠŸèƒ½ï¼Œæ¯”å¦‚è‡ªå®šä¹‰ <code>phong_shading</code>å‡½æ•°ï¼ˆå‚è€ƒ<a href="https://github.com/facebookresearch/pytorch3d/issues/865">issue</a>ï¼‰ã€‚ä½†è¿™æœ‰ç‚¹è¶…å‡ºæˆ‘çš„èƒ½åŠ›èŒƒå›´å’Œç²¾åŠ›èŒƒå›´ï¼Œæ‰€ä»¥æš‚æ—¶æç½®äº†ã€‚å¦‚æœèƒ½å®ç°çš„è¯ï¼ŒPyTorch3D ä¼¼ä¹æ˜¯æ¬¢è¿contributionçš„ï¼ˆ<a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>ï¼‰</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;pytorch3d-rendering-çš„ä¸€äº›ç–‘éš¾æ‚ç—‡&quot;&gt;Pytorch3D Rendering çš„ä¸€äº›ç–‘éš¾æ‚ç—‡&lt;/h1&gt;
&lt;p&gt;æœ‰å“ªäº›ï¼Ÿ&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;æœ‰äº†ç›¸æœºå†…å‚ Kï¼Œè€Œrenderåˆéœ€è¦NDCåæ ‡ç³»ï¼Œé‚£è¦æ€ä¹ˆå®šä¹‰ç›¸æœºï¼Ÿ&lt;/li&gt;
&lt;li&gt;å›¾åƒçš„é»„è“è‰²åäº†ï¼Ÿ&lt;/li&gt;
&lt;li&gt;render å®Œçš„å›¾åƒé”¯é½¿å¾ˆä¸¥é‡ï¼Ÿæ€ä¹ˆæŠ—é”¯é½¿ï¼ˆAntialiasingï¼‰ï¼Ÿ&lt;/li&gt;
&lt;li&gt;çš®è‚¤è¡¨é¢åå…‰å¤ªå¼ºï¼Œå…‰æ»‘å¾—åƒé•œé¢ä¸€æ ·ï¼Œæ€æ ·æ›´è‡ªç„¶ï¼Ÿ&lt;/li&gt;
&lt;li&gt;æ€ä¹ˆç‰©ä½“åªå‰©åŠæˆªï¼Œæ›´è¿œçš„éƒ¨åˆ†ä¼¼ä¹è¢«æˆªæ‰äº†ï¼Ÿ&lt;/li&gt;
&lt;li&gt;æ²¡è§£å†³çš„é—®é¢˜ï¼šPBRï¼ˆphysical based renderingï¼‰&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="Pytorch3D" scheme="https://jyzhu.top/blog/tags/Pytorch3D/"/>
    
  </entry>
  
  <entry>
    <title>Camera projection with the pinhole model</title>
    <link href="https://jyzhu.top/blog/Camera-Model-Notes/"/>
    <id>https://jyzhu.top/blog/Camera-Model-Notes/</id>
    <published>2023-04-02T13:44:43.000Z</published>
    <updated>2023-04-02T15:47:48.447Z</updated>
    
    <content type="html"><![CDATA[<p>A camera is a mapping between the 3D world (object space) and a 2D image.</p><p>In general, the camera projection matrix P has 11 degrees of freedom: <span class="math display">\[P=K[R\ \ \ t]\]</span></p><table><thead><tr class="header"><th>Component</th><th># DOF</th><th>Elements</th><th>Known As</th></tr></thead><tbody><tr class="odd"><td>K</td><td>5</td><td><span class="math inline">\(f_x, f_y, s,p_x, p_y\)</span></td><td>Intrinsic Parameters; camera calibration matrix</td></tr><tr class="even"><td>R</td><td>3</td><td><span class="math inline">\(\alpha,\beta,\gamma\)</span></td><td>Extrinsic Parameters</td></tr><tr class="odd"><td>t (or <span class="math inline">\(\tilde{C}\)</span>)</td><td>3</td><td><span class="math inline">\((t_x,t_y,t_z)\)</span></td><td>Extrinsic Parameters</td></tr></tbody></table><p>3D world frame ----- R, t ----&gt; 3D camera frame ------ K -----&gt; 2D image</p><p>Explanation:</p><ul><li><p>P: Projective camera, maps 3D world points to 2D image points.</p></li><li><p>K: Camera calibration matrix, 3 x 3, <span class="math inline">\(x=K[I|0]X_{cam}\)</span>, given 3D points in camera coordinate frame <span class="math inline">\(X_{cam}\)</span>, we can project it into 2D points on image <span class="math inline">\(x\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/FEe9lM5t3TJKgyL.png" alt="K" style="zoom:50%;" /></p></li><li><p>R and t: Camera Rotation and Translation, rigid transformation. <span class="math inline">\(X_{cam}=( X,Y,Z,1)^T\)</span> is expressed in the camera coordinate frame. In general, 3D points are expressed in a different Euclidean coordinate frame, known as the <strong>world coordinate frame</strong>. The two frames are related via a rigid transformation (R, t).</p></li></ul><h3 id="some-other-terms-you-may-see">Some other terms you may see</h3><ul><li><p><strong>P</strong>: 3x4, homogeneous, camera projection matrix, <span class="math inline">\(P=diag(f,f,1)[I|0]\)</span>. P is K without considering <span class="math inline">\((x_{cam},y_{cam})\)</span> in the image. (In other words, it simplify <span class="math inline">\((p_x, p_y)=(0,0)\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/TlcW7QFhOAXK6sY.png" alt="P" style="zoom:50%;" /></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;A camera is a mapping between the 3D world (object space) and a 2D image.&lt;/p&gt;
&lt;p&gt;In general, the camera projection matrix P has 11 degrees of freedom: &lt;span class=&quot;math display&quot;&gt;\[
P=K[R\ \ \ t]
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;# DOF&lt;/th&gt;
&lt;th&gt;Elements&lt;/th&gt;
&lt;th&gt;Known As&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;K&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\(f_x, f_y, s,p_x, p_y\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Intrinsic Parameters; camera calibration matrix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\(\alpha,\beta,\gamma\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;t (or &lt;span class=&quot;math inline&quot;&gt;\(\tilde{C}\)&lt;/span&gt;)&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\((t_x,t_y,t_z)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3D world frame ----- R, t ----&amp;gt; 3D camera frame ------ K -----&amp;gt; 2D image&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Configure Academic Page in Jekyll + Blog in Hexo together</title>
    <link href="https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/"/>
    <id>https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/</id>
    <published>2023-03-14T06:24:17.000Z</published>
    <updated>2023-04-02T16:12:37.901Z</updated>
    
    <content type="html"><![CDATA[<h2 id="difficult-situations">Difficult situations:</h2><ol type="1"><li>The Academic page is powered by <a href="http://jekyllrb.com/">Jekyll</a>, while the blog website is powered by Hexo.</li><li>And they are maintained in two separated repositories on Github.</li><li>Besides <code>[username].github.io</code>, I have a domain <code>jyzhu.top</code>, and want to use my custom domain.</li><li>All in all, I hope to visit the academic page is at <a href="jyzhu.top" class="uri">jyzhu.top</a>, while visit the blog is at <a href="jyzhu.top/blog" class="uri">jyzhu.top/blog</a>.</li></ol><h2 id="now-lets-configure.">Now let's configure.</h2><ol type="1"><li><p>Rename the blog repo as <code>blog</code>; rename the academic page repo as <code>[username].github.io</code>.</p></li><li><p>Edit the blog's Hexo config file:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="string">https://jyzhu.top/blog</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/blog/</span></span><br></pre></td></tr></table></figure><p><em>While no need to move all the files into a subfolder <code>blog</code> of your repo.</em></p><p>The Jekyll config is simple. Nothing needs to specify.</p></li><li><p>Edit the Github repo settings. Set the academic repo's <strong>custom domain</strong> as <code>jyzhu.top</code>. A <code>CNAME</code> file will be automatically added in the root. Now obviously, the <code>jyzhu.top</code> successfully refers to the academic page.</p><p>Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of <code>[username].github.io</code> by Github. Then coz <code>[username].github.io</code> is mapped to <code>[url]</code>, everything will be there, including <code>[url]/blog</code> for the <code>blog</code> repo.</p></li></ol><h2 id="todo">TODO</h2><p>The <code>hexo-douban</code> plugin cannot render styles now. Need to fix.</p><p><em>Update</em>:</p><ol type="1"><li>updated hexo-douban to the latest version</li><li>edit the file path of <code>loading.gif</code> in the <code>index.js</code> of this plugin</li></ol><p>then it seems ok now.</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;difficult-situations&quot;&gt;Difficult situations:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The Academic page is powered by &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;, while the blog website is powered by Hexo.&lt;/li&gt;
&lt;li&gt;And they are maintained in two separated repositories on Github.&lt;/li&gt;
&lt;li&gt;Besides &lt;code&gt;[username].github.io&lt;/code&gt;, I have a domain &lt;code&gt;jyzhu.top&lt;/code&gt;, and want to use my custom domain.&lt;/li&gt;
&lt;li&gt;All in all, I hope to visit the academic page is at &lt;a href=&quot;jyzhu.top&quot; class=&quot;uri&quot;&gt;jyzhu.top&lt;/a&gt;, while visit the blog is at &lt;a href=&quot;jyzhu.top/blog&quot; class=&quot;uri&quot;&gt;jyzhu.top/blog&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;now-lets-configure.&quot;&gt;Now let&#39;s configure.&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Rename the blog repo as &lt;code&gt;blog&lt;/code&gt;; rename the academic page repo as &lt;code&gt;[username].github.io&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the blog&#39;s Hexo config file:&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;url:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://jyzhu.top/blog&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;root:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/blog/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;While no need to move all the files into a subfolder &lt;code&gt;blog&lt;/code&gt; of your repo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Jekyll config is simple. Nothing needs to specify.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the Github repo settings. Set the academic repo&#39;s &lt;strong&gt;custom domain&lt;/strong&gt; as &lt;code&gt;jyzhu.top&lt;/code&gt;. A &lt;code&gt;CNAME&lt;/code&gt; file will be automatically added in the root. Now obviously, the &lt;code&gt;jyzhu.top&lt;/code&gt; successfully refers to the academic page.&lt;/p&gt;
&lt;p&gt;Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of &lt;code&gt;[username].github.io&lt;/code&gt; by Github. Then coz &lt;code&gt;[username].github.io&lt;/code&gt; is mapped to &lt;code&gt;[url]&lt;/code&gt;, everything will be there, including &lt;code&gt;[url]/blog&lt;/code&gt; for the &lt;code&gt;blog&lt;/code&gt; repo.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Blog" scheme="https://jyzhu.top/blog/tags/Blog/"/>
    
    <category term="Hexo" scheme="https://jyzhu.top/blog/tags/Hexo/"/>
    
    <category term="Jekyll" scheme="https://jyzhu.top/blog/tags/Jekyll/"/>
    
  </entry>
  
  <entry>
    <title>é½æ¬¡åæ ‡ç³»</title>
    <link href="https://jyzhu.top/blog/homogeneous-coordinates/"/>
    <id>https://jyzhu.top/blog/homogeneous-coordinates/</id>
    <published>2023-01-26T10:49:19.000Z</published>
    <updated>2023-01-26T10:58:06.245Z</updated>
    
    <content type="html"><![CDATA[<h1 id="é½æ¬¡åæ ‡ç³»">é½æ¬¡åæ ‡ç³»</h1><p>ä¹‹å‰ä¸ç†è§£ä¸ºä»€ä¹ˆè¦ç”¨ä¸€ä¸ªå’Œä»å°åˆ°å¤§å­¦çš„ç¬›å¡å°”åæ ‡ç³»ä¸åŒçš„é½æ¬¡åæ ‡ç³»æ¥è¡¨ç¤ºä¸œè¥¿ï¼Œå¹¶ä¸”å¼„å¾—å¾ˆå¤æ‚ï¼›å­¦äº†å„ç§å…¬å¼ä¹Ÿå¾ˆç³Šæ¶‚ã€‚ç°åœ¨ç»ˆäºæ˜ç™½äº†</p><h2 id="é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰">é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰</h2><p>å°±æ˜¯ç”¨æ¥è¡¨ç¤ºç°å®ä¸–ç•Œä¸­æˆ‘ä»¬çœ¼ç›çœ‹åˆ°çš„æ ·å­ï¼šä¸¤æ¡å¹³è¡Œçº¿åœ¨æ— é™è¿œå¤„èƒ½ç›¸äº¤ã€‚ <embed src="https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp" /></p><h2 id="é½æ¬¡åæ ‡ç³»çš„æœ¬è´¨">é½æ¬¡åæ ‡ç³»çš„æœ¬è´¨ï¼š</h2><p>å°±æ˜¯ç”¨N+1ç»´æ¥ä»£è¡¨Nç»´åæ ‡ã€‚</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸæœ¬äºŒç»´ç©ºé—´çš„ç‚¹<span class="math inline">\((X,Y)\)</span>ï¼Œå¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œç”¨<span class="math inline">\((x,y,w)\)</span>æ¥è¡¨ç¤ºã€‚æŠŠé½æ¬¡åæ ‡è½¬æ¢æˆç¬›å¡å°”åæ ‡æ˜¯å¾ˆç®€å•çš„ï¼Œå¯¹å‰ä¸¤ä¸ªç»´åº¦åˆ†åˆ«é™¤ä»¥æœ€åä¸€ä¸ªç»´åº¦çš„å€¼ï¼Œå°±å¥½äº†ï¼Œå³ <span class="math display">\[X=\frac x w,\  Y=\frac y w\\ (X,Y)=(\frac x w,\frac y w)\]</span> <embed src="https://pic2.zhimg.com/80/v2-da28eed57fda0fc6a06b7122be5f2a1d_1440w.webp" /></p><p>è¿™æ ·åšå°±å¯ä»¥è¡¨ç¤ºä¸¤æ¡å¹³è¡Œçº¿åœ¨è¿œå¤„èƒ½ç›¸äº¤äº†ï¼whyï¼Ÿ</p><p>è¦è§£é‡Šè¿™ä¸ªï¼Œéœ€è¦å…ˆè§£é‡Šä¸€ä¸ªé½æ¬¡åæ ‡ç³»çš„ç‰¹ç‚¹ï¼šè§„æ¨¡ä¸å˜æ€§ï¼ˆä¹Ÿæ˜¯å«homogeneousè¿™ä¸ªåå­—çš„åŸå› ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹ä»»æ„éé›¶çš„kï¼Œ<span class="math inline">\((x,y,w)\)</span>å’Œ<span class="math inline">\((kx,ky,kw)\)</span>éƒ½è¡¨ç¤ºäºŒç»´ç©ºé—´ä¸­åŒä¸€ä¸ªç‚¹<span class="math inline">\((\frac x w,\frac y w)\)</span>ã€‚ï¼ˆå› ä¸º<span class="math inline">\(\frac{kx}{kw}=\frac xw\)</span>å˜›ã€‚ï¼‰</p><p>é¦–å…ˆï¼Œç”¨åŸæœ¬ç¬›å¡å°”åæ ‡ç³»ä¸­çš„è¡¨ç¤ºæ–¹æ³•ï¼Œæ— é™è¿œå¤„çš„ç‚¹ä¼šè¢«è¡¨ç¤ºæˆ<span class="math inline">\((\infty,\infty)\)</span>ï¼Œä»è€Œå¤±å»æ„ä¹‰ã€‚ä½†æ˜¯æˆ‘ä»¬å‘ç°ç”¨é½æ¬¡åæ ‡ï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªæ–¹æ³•æ˜ç¡®è¡¨ç¤ºæ— é™è¿œå¤„çš„ä»»æ„ç‚¹ï¼Œå³ï¼Œ<span class="math inline">\((x,y,0)\)</span>ã€‚ï¼ˆä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæŠŠå®ƒè½¬æ¢å›ç¬›å¡å°”åæ ‡ï¼Œä¼šå¾—åˆ°<span class="math inline">\((\frac x 0,\frac y 0)=(\infty,\infty)\)</span>ï¼‰ã€‚</p><p>ç°åœ¨ï¼Œç”¨åˆä¸­æ‰€å­¦ï¼Œè”ç«‹ä¸¤æ¡ç›´çº¿çš„æ–¹ç¨‹ï¼Œå¾—åˆ°çš„è§£æ˜¯ä¸¤æ¡ç›´çº¿çš„äº¤ç‚¹ã€‚å‡å¦‚æœ‰ä¸¤æ¡å¹³è¡Œçº¿<span class="math inline">\(Ax+By+C=0\)</span>å’Œ<span class="math inline">\(Ax+By+D=0\)</span>ï¼Œæ±‚äº¤ç‚¹ï¼Œåˆ™ <span class="math display">\[\left\{\matrix{Ax+By+C=0 \\Ax+By+D=0}\right.\]</span> åœ¨ç¬›å¡å°”åæ ‡ç³»ä¸­ï¼Œå¯çŸ¥å”¯ä¸€è§£æ˜¯<span class="math inline">\(C=D\)</span>ï¼Œå³ä¸¤æ¡çº¿ä¸ºåŒä¸€æ¡ç›´çº¿ã€‚</p><p>ä½†æ˜¯ï¼Œå¦‚æœæŠŠå®ƒæ¢æˆé½æ¬¡åæ ‡ï¼Œå¾—åˆ° <span class="math display">\[\left\{\matrix{A\frac x w+B\frac y w+C=0\\ A\frac x w + B\frac y w +D=0}\right.\]</span></p><p><span class="math display">\[\left\{\matrix{Ax+By+Cw=0\\Ax+By+Dw=0}\right.\]</span></p><p>å½“<span class="math inline">\(w=0\)</span>ï¼Œä¸Šå¼å˜æˆ<span class="math inline">\(Ax+By=0\)</span>ï¼Œå¾—åˆ°è§£<span class="math inline">\((x,-\frac {A}Bx,0)\)</span>ã€‚å…¶å®è¿™é‡Œçš„xå’Œyæ˜¯ä»€ä¹ˆä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯w=0ï¼Œæ„å‘³ç€è¿™æ˜¯ä¸ªæ— é™è¿œå¤„çš„ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸¤æ¡å¹³è¡Œçº¿åœ¨æ— é™è¿œå¤„ç›¸äº¤äº†ï¼ç”šè‡³èƒ½æ˜ç¡®æ±‚å‡ºäº¤ç‚¹ï¼</p><blockquote><p>Reference:</p><p>http://www.songho.ca/math/homogeneous/homogeneous.html</p><p>https://zhuanlan.zhihu.com/p/373969867</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;é½æ¬¡åæ ‡ç³»&quot;&gt;é½æ¬¡åæ ‡ç³»&lt;/h1&gt;
&lt;p&gt;ä¹‹å‰ä¸ç†è§£ä¸ºä»€ä¹ˆè¦ç”¨ä¸€ä¸ªå’Œä»å°åˆ°å¤§å­¦çš„ç¬›å¡å°”åæ ‡ç³»ä¸åŒçš„é½æ¬¡åæ ‡ç³»æ¥è¡¨ç¤ºä¸œè¥¿ï¼Œå¹¶ä¸”å¼„å¾—å¾ˆå¤æ‚ï¼›å­¦äº†å„ç§å…¬å¼ä¹Ÿå¾ˆç³Šæ¶‚ã€‚ç°åœ¨ç»ˆäºæ˜ç™½äº†&lt;/p&gt;
&lt;h2 id=&quot;é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰&quot;&gt;é½æ¬¡åæ ‡ç³»çš„ç°å®æ„ä¹‰&lt;/h2&gt;
&lt;p&gt;å°±æ˜¯ç”¨æ¥è¡¨ç¤ºç°å®ä¸–ç•Œä¸­æˆ‘ä»¬çœ¼ç›çœ‹åˆ°çš„æ ·å­ï¼šä¸¤æ¡å¹³è¡Œçº¿åœ¨æ— é™è¿œå¤„èƒ½ç›¸äº¤ã€‚ &lt;embed src=&quot;https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to NeRF</title>
    <link href="https://jyzhu.top/blog/Introduction-to-NeRF/"/>
    <id>https://jyzhu.top/blog/Introduction-to-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:21.514Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h1 id="introduction-to-nerf">1. Introduction to NeRF</h1><h2 id="what-is-nerf">What is NeRF</h2><blockquote><p>Reference: Original NeRF paper; an online ariticle</p></blockquote><p>åœ¨å·²çŸ¥è§†è§’ä¸‹å¯¹åœºæ™¯è¿›è¡Œä¸€ç³»åˆ—çš„æ•è· (åŒ…æ‹¬æ‹æ‘„åˆ°çš„å›¾åƒï¼Œä»¥åŠæ¯å¼ å›¾åƒå¯¹åº”çš„å†…å¤–å‚)ï¼Œåˆæˆæ–°è§†è§’ä¸‹çš„å›¾åƒã€‚</p><p>NeRF æƒ³åšè¿™æ ·ä¸€ä»¶äº‹ï¼Œä¸éœ€è¦ä¸­é—´ä¸‰ç»´é‡å»ºçš„è¿‡ç¨‹ï¼Œä»…æ ¹æ®ä½å§¿å†…å‚å’Œå›¾åƒï¼Œç›´æ¥åˆæˆæ–°è§†è§’ä¸‹çš„å›¾åƒã€‚ä¸ºæ­¤ NeRF å¼•å…¥äº†è¾å°„åœºçš„æ¦‚å¿µï¼Œè¿™åœ¨å›¾å½¢å­¦ä¸­æ˜¯éå¸¸é‡è¦çš„æ¦‚å¿µï¼Œåœ¨æ­¤æˆ‘ä»¬ç»™å‡ºæ¸²æŸ“æ–¹ç¨‹çš„å®šä¹‰ï¼š</p><p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p><p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p><p>é‚£ä¹ˆè¾å°„å’Œé¢œè‰²æ˜¯ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿç®€å•è®²å°±æ˜¯ï¼Œå…‰å°±æ˜¯ç”µç£è¾å°„ï¼Œæˆ–è€…è¯´æ˜¯æŒ¯è¡çš„ç”µç£åœºï¼Œå…‰åˆæœ‰æ³¢é•¿å’Œé¢‘ç‡ï¼Œ<span class="math inline">\(æ³¢é•¿\times é¢‘ç‡=å…‰é€Ÿ\)</span>ï¼Œå…‰çš„é¢œè‰²æ˜¯ç”±é¢‘ç‡å†³å®šçš„ï¼Œå¤§å¤šæ•°å…‰æ˜¯ä¸å¯è§çš„ï¼Œäººçœ¼å¯è§çš„å…‰è°±ç§°ä¸ºå¯è§å…‰è°±ï¼Œå¯¹åº”çš„é¢‘ç‡å°±æ˜¯æˆ‘ä»¬è®¤ä¸ºçš„é¢œè‰²ï¼š</p><p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p><p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p><h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3><p>SDFæ˜¯ä¸€ç§è®¡ç®—å›¾å½¢å­¦ä¸­å®šä¹‰è·ç¦»çš„å‡½æ•°ã€‚SDFå®šä¹‰äº†ç©ºé—´ä¸­çš„ç‚¹åˆ°éšå¼æ›²é¢çš„è·ç¦»ï¼Œè¯¥ç‚¹åœ¨æ›²é¢å†…å¤–å†³å®šäº†å…¶SDFçš„æ­£è´Ÿæ€§ã€‚</p><p>ç›¸è¾ƒäºå…¶ä»–åƒç‚¹äº‘ï¼ˆpoint cloudï¼‰ã€ä½“ç´ ï¼ˆvoxelï¼‰ã€é¢äº‘ï¼ˆmeshï¼‰é‚£æ ·çš„ç»å…¸3Dæ¨¡å‹è¡¨ç¤ºæ–¹æ³•ï¼ŒSDFæœ‰å›ºå®šçš„æ•°å­¦æ–¹ç¨‹ï¼Œæ›´å…³æ³¨ç‰©ä½“çš„è¡¨é¢ä¿¡æ¯ï¼Œå…·æœ‰å¯æ§çš„è®¡ç®—æˆæœ¬ã€‚</p><h2 id="features-of-nerf">Features of NeRF</h2><ul><li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.<ul><li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li></ul></li><li>Neural Field has advantages:<ol type="1"><li>Compactness ç´§è‡´:</li><li>Regularization: nn itself as inductive bias makes it easy to learn</li><li>Domain Agonostic: cheap to add a dimension</li></ol></li><li>also problems<ul><li>Editability / Manipulability</li><li>Computational Complexity</li><li>Spectral Bias</li></ul></li></ul><h2 id="problem-formulation">Problem Formulation</h2><ul><li>Input: multiview images</li><li>Output: 3D Geometry and appearance</li><li>Objective:</li></ul><p><span class="math display">\[\arg \min_x\|y-F(x)\|+\lambda P(x)\]</span></p><p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p><p>F can be differentiable, then you can supervise this.</p><ul><li>nnæœ¬èº«å°±æ˜¯æŸç§constraintsï¼Œä½ å°±ä¸éœ€è¦åŠ å¤ªå¤šhandicraft constraints</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;introduction-to-nerf&quot;&gt;1. Introduction to NeRF&lt;/h1&gt;
&lt;h2 id=&quot;what-is-nerf&quot;&gt;What is NeRF&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: Original NeRF paper; an online ariticle&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Learning NeRF</title>
    <link href="https://jyzhu.top/blog/Learning-NeRF/"/>
    <id>https://jyzhu.top/blog/Learning-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:37.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-nerf">Learning NeRF</h1><p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="reading-list">Reading List</h2><h3 id="classical">Classical</h3><ul><li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p><p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li><li><p><a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li></ul><h3 id="survey">Survey</h3><ul><li><a href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li></ul><h3 id="cvpr">2021CVPR</h3><p>2021å¹´CVPRè¿˜æœ‰è®¸å¤šç›¸å…³çš„ç²¾å½©å·¥ä½œå‘è¡¨ã€‚ä¾‹å¦‚ï¼Œæå‡ç½‘ç»œçš„æ³›åŒ–æ€§ï¼š</p><ul><li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>ï¼šå°†æ¯ä¸ªåƒç´ çš„ç‰¹å¾å‘é‡è€Œéåƒç´ æœ¬èº«ä½œä¸ºè¾“å…¥ï¼Œå…è®¸ç½‘ç»œåœ¨ä¸åŒåœºæ™¯çš„å¤šè§†å›¾å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ åœºæ™¯å…ˆéªŒï¼Œç„¶åæµ‹è¯•æ—¶ç›´æ¥æ¥æ”¶ä¸€ä¸ªæˆ–å‡ ä¸ªè§†å›¾ä¸ºè¾“å…¥åˆæˆæ–°è§†å›¾ã€‚</li><li><a href="https://ibrnet.github.io/">IBRNet</a>ï¼šå­¦ä¹ ä¸€ä¸ªé€‚ç”¨äºå¤šç§åœºæ™¯çš„é€šç”¨è§†å›¾æ’å€¼å‡½æ•°ï¼Œä»è€Œä¸ç”¨ä¸ºæ¯ä¸ªæ–°çš„åœºæ™¯éƒ½æ–°å­¦ä¹ ä¸€ä¸ªæ¨¡å‹æ‰èƒ½æ¸²æŸ“ï¼›ä¸”ç½‘ç»œç»“æ„ä¸Šç”¨äº†å¦ä¸€ä¸ªæ—¶é«¦çš„ä¸œè¥¿ Transformerã€‚</li><li><a href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>ï¼šè®­ç»ƒä¸€ä¸ªå…·æœ‰æ³›åŒ–æ€§èƒ½çš„å…ˆéªŒç½‘ç»œï¼Œåœ¨æ¨ç†çš„æ—¶å€™åªç”¨3å¼ è¾“å…¥å›¾ç‰‡å°±é‡å»ºä¸€ä¸ªæ–°çš„åœºæ™¯ã€‚</li></ul><p>é’ˆå¯¹åŠ¨æ€åœºæ™¯çš„NeRF:</p><ul><li><a href="https://nerfies.github.io/">Nerfies</a>ï¼šå¤šä½¿ç”¨äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¥æ‹Ÿåˆå½¢å˜çš„SE(3) fieldï¼Œä»è€Œå»ºæ¨¡å¸§é—´åœºæ™¯å½¢å˜ã€‚Nerfies: Deformable Neural Radiance Fields</li><li><a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>ï¼šå¤šä½¿ç”¨äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¥æ‹Ÿåˆåœºæ™¯å½¢å˜çš„displacementã€‚</li><li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>ï¼šå¤šæå‡ºäº†ä¸€ä¸ªscene flow fieldsæ¥æè¿°æ—¶åºçš„åœºæ™¯å½¢å˜ã€‚</li></ul><p>å…¶ä»–åˆ›æ–°ç‚¹ï¼š</p><ul><li><a href="https://kai-46.github.io/PhySG-website/">PhySG</a>ï¼šç”¨çƒçŠ¶é«˜æ–¯å‡½æ•°æ¨¡æ‹ŸBRDFï¼ˆé«˜çº§ç€è‰²çš„ä¸Šå¤ç¥å™¨ï¼‰å’Œç¯å¢ƒå…‰ç…§ï¼Œé’ˆå¯¹æ›´å¤æ‚çš„å…‰ç…§ç¯å¢ƒï¼Œèƒ½å¤„ç†éæœ—ä¼¯è¡¨é¢çš„åå°„ã€‚</li><li><a href="https://nex-mpi.github.io/">NeX</a>ï¼šç”¨MPIï¼ˆMulti-Plane Image ï¼‰ä»£æ›¿NeRFçš„RGBÏƒä½œä¸ºç½‘ç»œçš„è¾“å‡ºã€‚</li></ul><h3 id="cvpr-1">2022 CVPR</h3><p><a href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p><h2 id="useful-references"><strong>Useful References:</strong></h2><blockquote><p><a href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p><p><a href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p><p><a href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p><p><a href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p></blockquote><p>Bigger to learn:</p><ul><li>[ ] Above NeRF: neural rendering</li><li>[ ] Related theories in graphics and computer vision</li><li>[ ] NeRFçš„ä¸€ä½œBen Mildenhallåœ¨SIGGRAPH 2021 Course <a href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>ä¸­ä»æ¦‚ç‡çš„è§’åº¦æ¨å¯¼äº†NeRFçš„ä½“æ¸²æŸ“å…¬å¼ã€‚</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;learning-nerf&quot;&gt;Learning NeRF&lt;/h1&gt;
&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;reading-list&quot;&gt;Reading List&lt;/h2&gt;
&lt;h3 id=&quot;classical&quot;&gt;Classical&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Manipulate Neural Fields</title>
    <link href="https://jyzhu.top/blog/Manipulate-Neural-Fields/"/>
    <id>https://jyzhu.top/blog/Manipulate-Neural-Fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:11:38.197Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2><p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p><figure><img src="https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption></figure><p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p><p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p><figure><img src="https://s2.loli.net/2023/01/12/S7HWcQPh1FtdJaw.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption></figure><p>The following examples è½åœ¨ä¸åŒçš„è±¡é™ã€‚</p><h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3><ul><li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li><li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li><li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p><figure><img src="https://s2.loli.net/2023/01/12/y4bGulHpfOwWkqN.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption></figure><ul><li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p><figure><img src="https://s2.loli.net/2023/01/12/SBzGy3rnUaqLFI8.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption></figure></li></ul></li><li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p><figure><img src="https://s2.loli.net/2023/01/12/UbFu74iCQ15mK3B.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption></figure></li></ul><h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/zxFElDIuSnPioJ7.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption></figure><p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p><p>We need to define the consistency here, so that the network can learn through forward and backward:</p><figure><img src="https://s2.loli.net/2023/01/12/VS1K3rQxHXYPRIg.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption></figure><h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3><p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p><figure><img src="https://s2.loli.net/2023/01/12/w2XEyYn5qbh41OB.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption></figure><ul><li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li></ul><figure><img src="https://s2.loli.net/2023/01/12/ruER6MJPpljSZiX.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption></figure><ul><li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/HL61itcqsIaEThX.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/y7eCcKDmdUY4VOu.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption></figure><ul><li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/2lTvenQixfRm8Po.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption></figure><ul><li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li></ul><h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/5Ohb7ExW4osc1n2.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption></figure><ul><li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li><li>Beyond geometry, we can also manipulate <strong>color</strong></li></ul><figure><img src="https://s2.loli.net/2023/01/12/vM1QwkT4BJqGNIR.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption></figure><p>It decomposes the network into shape and color networks, and we can edit each independently.</p><figure><img src="https://s2.loli.net/2023/01/12/38HNsE9GnF1pydQ.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption></figure><ul><li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;manipulate-neural-fields&quot;&gt;2.5. Manipulate Neural Fields&lt;/h2&gt;
&lt;p&gt;Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png&quot; alt=&quot;image-20221212211525928&quot;&gt;&lt;figcaption&gt;image-20221212211525928&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Differentiable Forward Maps</title>
    <link href="https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/"/>
    <id>https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:34.505Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2><figure><img src="https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption></figure><h3 id="differentiable-rendering">Differentiable rendering</h3><figure><img src="https://s2.loli.net/2023/01/12/1Ng8wz2KP4oTiVH.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption></figure><p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p><h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3><ul><li>BRDF Shading? details later.</li></ul><h3 id="differentiation-itself">Differentiation itself</h3><p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p><figure><img src="https://s2.loli.net/2023/01/12/Gi6IaAkhvlBxpoe.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption></figure><p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p><p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p><h3 id="special-identity-operator">Special: Identity Operator</h3><p><span class="math display">\[\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\\text{Reconstruction} == \text{Sensor domain}\]</span></p><p>Q&amp;A:</p><ul><li>Can we obtain a neural network in just one forward, without optimization?</li><li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;differentiable-forward-maps&quot;&gt;2.3. Differentiable Forward Maps&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png&quot; alt=&quot;image-20221208175453557&quot;&gt;&lt;figcaption&gt;image-20221208175453557&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;differentiable-rendering&quot;&gt;Differentiable rendering&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Hybrid representations</title>
    <link href="https://jyzhu.top/blog/NeRF-Hybrid-representations/"/>
    <id>https://jyzhu.top/blog/NeRF-Hybrid-representations/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:55.420Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="hybrid-representations">2.2. Hybrid representations</h2><h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3><figure><img src="https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/sDg8FHQcjrGRW1b.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption></figure><p>You may choose one proper representation depending on your own application</p><h3 id="grid">1. Grid</h3><figure><img src="https://s2.loli.net/2023/01/12/ZpYEMbXvRdeOqiy.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption></figure><p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p><figure><img src="https://s2.loli.net/2023/01/12/lMi3tK9NPgcWUaI.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption></figure><p>NeRFusion CVPR22: online!</p><h3 id="point-cloud">2. point cloud</h3><figure><img src="https://s2.loli.net/2023/01/12/zhYHQnFsgxBLCfM.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption></figure><p>Cons:</p><ol type="1"><li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li><li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li></ol><p><img src="https://s2.loli.net/2023/01/12/37EbFsANOCc9voX.png" alt="image-20221208163050867" style="zoom:50%;" /></p><h3 id="mesh">3. Mesh</h3><p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p><figure><img src="https://s2.loli.net/2023/01/12/Dwir9hsm3VgZjQk.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/nPMw7T3hqRAU2iv.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption></figure><h3 id="multiplanar-images">4. Multiplanar Images</h3><p>Something like project a 3D grid into an axis to get levels of planes.</p><figure><img src="https://s2.loli.net/2023/01/12/CmhFDT5NoiMAOnv.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption></figure><p>Pros:</p><ol type="1"><li>Compact</li><li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li></ol><p>Cons:</p><ol type="1"><li>Resolution bias on plane axis: coz it is discrete betweens planes.</li></ol><p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p><p><img src="https://s2.loli.net/2023/01/12/UDO6HlWAp3y7qF1.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p><h3 id="multiresolution-grids">5. Multiresolution grids</h3><figure><img src="https://s2.loli.net/2023/01/12/TlmkK1NDj2dAtUp.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption></figure><p>Pros:</p><ol start="2" type="1"><li>Stable coz you indeed need both low and high resolution info</li></ol><h3 id="hash-grids">6. Hash grids</h3><p><img src="https://s2.loli.net/2023/01/12/NZtH7wfxpSPVkGe.png" alt="image-20221208170131069" /> <span class="math display">\[[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}\]</span> Pros:</p><ol type="1"><li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li><li>Can be online!</li></ol><p>Cons:</p><ol type="1"><li>May still need large codebooks</li><li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li></ol><h3 id="codebook-grids">7. Codebook grids</h3><figure><img src="https://s2.loli.net/2023/01/12/pnBJGxM6jNHD2v5.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption></figure><p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p><p>cons:</p><ol type="1"><li>To make the indexing operation differentiable, the computing complexity rises here.</li><li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li></ol><h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3><figure><img src="https://s2.loli.net/2023/01/12/LQ6fzh21OltTJxq.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption></figure><p>Commonly used method in computer graphics</p><h3 id="others-voxel">9. Others (voxel)</h3><figure><img src="https://s2.loli.net/2023/01/12/5sE1MYpOLB4mPCF.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption></figure><ul><li>For dynamic nerfs, is there any better hybrid representation? Sure.</li><li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;hybrid-representations&quot;&gt;2.2. Hybrid representations&lt;/h2&gt;
&lt;h3 id=&quot;tradeoffs-of-choosing-a-proper-representation&quot;&gt;Tradeoffs of choosing a proper representation&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png&quot; alt=&quot;image-20221208172055153&quot;&gt;&lt;figcaption&gt;image-20221208172055153&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Network Architecture</title>
    <link href="https://jyzhu.top/blog/NeRF-Network-Architecture/"/>
    <id>https://jyzhu.top/blog/NeRF-Network-Architecture/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:07.993Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="network-architecture">2.1. Network Architecture</h2><h3 id="input-encoding">1. Input Encoding</h3><p>Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the å…‰çº¿çš„æ•£å°„</p><h3 id="activation-functions">2. Activation functions</h3><p>ReLU is not perfect for this task. Because it ä¸èƒ½è§£å†³å¯¹é«˜é˜¶å¯¼æœ‰constraintsçš„å‡½æ•°ã€‚</p><p>SIREN is a replacement.</p><h3 id="symmetry-invariance-equivariance">3. Symmetry, Invariance &amp; Equivariance</h3><figure><img src="https://s2.loli.net/2023/01/12/tynBhWCulMINrU3.png" alt="image-20221205193423558" /><figcaption>image-20221205193423558</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/Rjx5h2kfY34JZct.png" alt="image-20221205193614341" /><figcaption>image-20221205193614341</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;network-architecture&quot;&gt;2.1. Network Architecture&lt;/h2&gt;
&lt;h3 id=&quot;input-encoding&quot;&gt;1. Input Encoding&lt;/h3&gt;
&lt;p&gt;Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the å…‰çº¿çš„æ•£å°„&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Notebook</title>
    <link href="https://jyzhu.top/blog/NeRF-Notebook/"/>
    <id>https://jyzhu.top/blog/NeRF-Notebook/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:36:59.994Z</updated>
    
    <content type="html"><![CDATA[<p>I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.</p><h1 id="contents">Contents</h1><p><a href="https://jyzhu.top/Learning-NeRF/">Learning NeRF</a>: Reading list, learning references, and plans</p><p><strong>Notes of CVPR22' Tutorial:</strong></p><p><a href="https://jyzhu.top/Introduction-to-NeRF/">1. Introduction to NeRF</a>: What is NeRF and its features</p><p>2. Techniques</p><p>â€‹ <a href="https://jyzhu.top/NeRF-Network-Architecture/">2.1. Network Architecture</a></p><p>â€‹ <a href="https://jyzhu.top/NeRF-Hybrid-representations/">2.2. Hybrid representations</a></p><p>â€‹ <a href="https://jyzhu.top/NeRF-Differentiable-Forward-Maps/">2.3. Differentiable Forward Maps</a></p><p>â€‹ <a href="https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/">2.4. Prior-based reconstruction of neural fields</a></p><p>â€‹ <a href="https://jyzhu.top/Manipulate-Neural-Fields/">2.5. Manipulate Neural Fields</a></p><p>3. Applications</p><p><em>TBC</em></p><p><strong>Notes of paper reading</strong>:</p><p><a href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/">Reading NeuMan: Neural Human Radiance Field from a Single Video</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.&lt;/p&gt;
&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://jyzhu.top/Learning-NeRF/&quot;&gt;Learning NeRF&lt;/a&gt;: Reading list, learning references, and plans&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notes of CVPR22&#39; Tutorial:&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Prior based reconstruction of neural fields</title>
    <link href="https://jyzhu.top/blog/Prior-based-reconstruction-of-neural-fields/"/>
    <id>https://jyzhu.top/blog/Prior-based-reconstruction-of-neural-fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:12.867Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="prior-based-reconstruction-of-neural-fields">2.4. Prior-based reconstruction of neural fields</h2><p>Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let's learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.</p><figure><img src="https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png" alt="image-20221211234430727" /><figcaption>image-20221211234430727</figcaption></figure><h3 id="how-does-the-latent-code-look-like">How does the latent code look like?</h3><figure><img src="https://s2.loli.net/2023/01/12/A41EOlYCBXrUxto.png" alt="image-20221211234923290" /><figcaption>image-20221211234923290</figcaption></figure><ul><li>Global: not local. A small latent code represents a neural field<ul><li>main limitation: can only represent very simple (single) object. coz if you have multiple objects in a scene, the degree of freedom grows non-linearly.</li><li><strong>How about giving the natural language descriptions as conditions???</strong></li></ul></li><li>Local: you get different latent codes considering the locality where you are. So, you have a prior 3D data structure to store the latent codes.<ul><li>3D point clouds -&gt; grids -&gt; triplanes interpolation</li></ul></li></ul><blockquote><p>Convolutional Occupancy Networks</p></blockquote><h3 id="autodecoder-instead-of-encoder-decoder">Autodecoder instead of Encoder-decoder</h3><figure><img src="https://s2.loli.net/2023/01/12/zr2GnpBXeZDJUqa.png" alt="image-20221212005012783" /><figcaption>image-20221212005012783</figcaption></figure><ul><li><p>Encoder is a 2D CNN structure.</p></li><li><p>But while using autodecoder, the backpropogate through the forward map (i.e., the neural renderer) will give the 3D structural information to the latent codes directly. <span class="math display">\[\text{latent code }\hat z=\arg \min_z \|\text{Render(}\Phi)-g.t.\|\]</span> <img src="https://s2.loli.net/2023/01/12/FQjMgibmNtIP4ca.png" alt="image-20221212004946040" /></p></li></ul><p><strong>Instead of trying to build the encoder, sometimes just use the backpropogation through the forward map is helpful.</strong></p><h3 id="light-field-networks----dont-need-to-render-anymore">Light field networks -- Don't need to render anymore</h3><figure><img src="https://s2.loli.net/2023/01/12/FVP15fmBYz8rEoe.png" alt="image-20221212005908926" /><figcaption>image-20221212005908926</figcaption></figure><p>Instead of learning a NeRF that you use a neural renderer to generate all points along a ray; you can learn a network to directly give you a color along a ray. So you do not use a 3d coordinate as the query, instead, use a ray.</p><p>But this do not work in complicated task yet.</p><figure><img src="https://s2.loli.net/2023/01/12/mAFx6pzqaIuUMin.png" alt="image-20221212010316991" /><figcaption>image-20221212010316991</figcaption></figure><h3 id="outlook">Outlook</h3><ul><li>You don't need to use 600 images of a scene to reconstruct it. Synthesis images?</li><li>Open minds: other ways to skip the expensive forward map? (e.g., the light field)</li><li>Understanding the scene like humans do: disentangle different objects</li><li>Local conditioning methods? Regular grids are easy to tackle with, but it's harder for point clouds / factorized representations</li><li>Transformers: seems like local conditioning</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;prior-based-reconstruction-of-neural-fields&quot;&gt;2.4. Prior-based reconstruction of neural fields&lt;/h2&gt;
&lt;p&gt;Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let&#39;s learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png&quot; alt=&quot;image-20221211234430727&quot;&gt;&lt;figcaption&gt;image-20221211234430727&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>The difference between RNN&#39;s output and h_n</title>
    <link href="https://jyzhu.top/blog/The-difference-between-RNN-s-output-and-h-n/"/>
    <id>https://jyzhu.top/blog/The-difference-between-RNN-s-output-and-h-n/</id>
    <published>2022-10-22T08:56:44.000Z</published>
    <updated>2022-10-22T09:12:01.123Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Reference: <a href="https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm" class="uri">https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm</a></p></blockquote><p>I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(s_{t-1}\)</span> as input, and outputs <span class="math inline">\(s_t\)</span>, i.e., <span class="math inline">\(s_t = RNN(y_{t-1}, s_{t-1})\)</span>.</p><p>But the pytorch implementation of RNN is: <span class="math inline">\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)</span>, which takes in a sequence of elements, computes in serials, and outputs a sequence also.</p><p>I was confused about what is the <span class="math inline">\(s_t\)</span>. Is it the <span class="math inline">\(outputs\)</span>, or the <span class="math inline">\(hidden\_states\)</span>?</p><p>This is the very helpful picture:</p><p><img src="https://i.stack.imgur.com/SjnTl.png" /></p><p>The <span class="math inline">\(output\)</span> here is the <span class="math inline">\(hidden\_states\)</span> of the last layer among all elements in the sequence (time steps), while the <span class="math inline">\(h_n,c_n = hidden\_last\)</span> is the <span class="math inline">\(hidden\_states\)</span> of the last time step among all layers.</p><p>The former is the <span class="math inline">\(H\)</span>, hidden state collection, which can be used in subsequent calculations, like attentions or scores; and the latter is the hidden state that can be directly used in the next iteration.</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm&quot; class=&quot;uri&quot;&gt;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes &lt;span class=&quot;math inline&quot;&gt;\(y_{t-1}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(s_{t-1}\)&lt;/span&gt; as input, and outputs &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt;, i.e., &lt;span class=&quot;math inline&quot;&gt;\(s_t = RNN(y_{t-1}, s_{t-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But the pytorch implementation of RNN is: &lt;span class=&quot;math inline&quot;&gt;\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)&lt;/span&gt;, which takes in a sequence of elements, computes in serials, and outputs a sequence also.&lt;/p&gt;
&lt;p&gt;I was confused about what is the &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt;. Is it the &lt;span class=&quot;math inline&quot;&gt;\(outputs\)&lt;/span&gt;, or the &lt;span class=&quot;math inline&quot;&gt;\(hidden\_states\)&lt;/span&gt;?&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Deep Learning" scheme="https://jyzhu.top/blog/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://jyzhu.top/blog/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Reading 3D Photography using Context-aware Layered Depth Inpainting</title>
    <link href="https://jyzhu.top/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/"/>
    <id>https://jyzhu.top/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/</id>
    <published>2022-10-07T00:35:43.000Z</published>
    <updated>2022-10-22T09:15:26.167Z</updated>
    
    <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼š<a href="https://shihmengli.github.io/3D-Photo-Inpainting" class="uri">https://shihmengli.github.io/3D-Photo-Inpainting</a></p><p>ä½œè€…ï¼š<a href="https://shihmengli.github.io/">Meng-Li Shih</a>, <a href="https://lemonatsu.github.io/">Shih-Yang Su</a>, <a href="https://johanneskopf.de/">Johannes Kopf</a>, <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></p><p>å‘è¡¨ï¼š CVPR2020</p><p>é“¾æ¥ï¼š <a href="https://github.com/vt-vl-lab/3d-photo-inpainting" class="uri">https://github.com/vt-vl-lab/3d-photo-inpainting</a></p><hr /><h2 id="why">Whyï¼š</h2><p>ä¹‹å‰çš„ç…§ç‰‡3DåŒ–çš„æ–¹æ³•ï¼Œä¼šåœ¨è§†è§’å˜åŠ¨åå‡ºç°çš„æ–°åƒç´ åŒºåŸŸ å¡«å……å¾ˆæ¨¡ç³Šçš„èƒŒæ™¯ï¼›è¿™ä¸ªæ–¹æ³•ä¸»è¦æ˜¯ç”¨inpaintingçš„æ–¹æ³•æé«˜æ–°èƒŒæ™¯ç”Ÿæˆçš„æ•ˆæœ</p><figure><img src="https://s2.loli.net/2022/10/22/dfjuy9vNrBak1oP.png" alt="image-20221007084129382" /><figcaption>image-20221007084129382</figcaption></figure><h2 id="what">Whatï¼š</h2><ol type="1"><li>ä»»åŠ¡æ˜¯3D photographyï¼Œå›¾åƒ3DåŒ–ï¼ŒæŠŠä¸€å¼ 2D+æ·±åº¦ä¿¡æ¯çš„RGB-Då›¾åƒè½¬åŒ–æˆ3Dé£æ ¼çš„å›¾åƒã€‚</li><li>ç°åœ¨çš„å¤šé•œå¤´æ™ºèƒ½æ‰‹æœºæ‹çš„ç…§ç‰‡éƒ½èƒ½æä¾›æ·±åº¦ä¿¡æ¯ã€‚æ²¡æœ‰çš„è¯ï¼Œä¹Ÿèƒ½ç”¨å…¶ä»–æ¨¡å‹é¢„æµ‹æ·±åº¦ã€‚</li><li>ç”¨åˆ†å±‚æ·±åº¦å›¾åƒï¼ˆLayered Depth Imageï¼‰æ¥è¡¨ç¤ºå›¾åƒï¼šèƒ½æ˜¾å¼åœ°è¡¨ç¤ºåƒç´ ç‚¹ä¹‹é—´çš„è¿é€šæ€§ã€‚å’Œæ™®é€šçš„2Då›¾åƒç›¸æ¯”ï¼Œå¯ä»¥æŠŠåƒç´ ç‚¹åˆ†æˆå¤šå±‚æ¥è¡¨ç¤ºï¼ŒåŒä¸€ä¸ªåæ ‡å¤„å¯ä»¥æœ‰é‡åˆçš„ä¸åŒå±‚æ¬¡çš„åƒç´ ç‚¹ã€‚</li><li>æå‡ºä¸€ä¸ªåŸºäºå­¦ä¹ çš„ inpainting æ–¹æ³•å¡«å……é‡å åŒºåŸŸçš„åƒç´ ï¼Œè®©3Då›¾åƒè§†è§’å˜åŒ–çš„æ—¶å€™å‡ºç°çš„æ–°èƒŒæ™¯æ•ˆæœå¾ˆå¥½ã€‚</li></ol><h2 id="how">Howï¼š</h2><p>æ˜¯ä¸€ä¸ªå¾ˆæ¸…æ™°çš„æµç¨‹ï¼š</p><ol type="1"><li><p>è¾“å…¥ä¸ºå•å¼ RGB-Då›¾åƒã€‚Dä¸ºdepthï¼Œä¸€èˆ¬å¤šé•œå¤´æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡éƒ½èƒ½æä¾›æ·±åº¦ä¿¡æ¯ï¼›æ²¡æœ‰çš„è¯å°±ç”¨å…¶ä»–æ¨¡å‹é¢„æµ‹æ·±åº¦ï¼Œæ¯”å¦‚MegaDepth, MiDas, and Kinect depth sensor</p></li><li><p>å°†è¾“å…¥å›¾åƒè½¬åŒ–æˆåˆ†å±‚æ·±åº¦å›¾åƒï¼ˆLayered Depth Imageï¼‰ã€‚LDIä¸­çš„æ¯ä¸ªåƒç´ ç‚¹ä¿å­˜é¢œè‰²å’Œæ·±åº¦ä¿¡æ¯ï¼Œä»¥åŠä¸Šä¸‹å·¦å³å››ä¸ªæ–¹å‘çš„é‚»å±…åƒç´ ç‚¹ã€‚åŒä¸€ä¸ªåæ ‡å¤„å¯ä»¥æœ‰é‡åˆçš„ä¸åŒæ·±åº¦çš„åƒç´ ç‚¹ã€‚</p></li><li><p>å›¾åƒé¢„å¤„ç†ï¼šæ£€æµ‹æ·±åº¦ä¸è¿è´¯çš„è¾¹ç¼˜</p><figure><img src="https://s2.loli.net/2022/10/22/6tjCgUHTpVyIAFN.png" alt="image-20221007090910332" /><figcaption>image-20221007090910332</figcaption></figure><p>ç”¨filteræŠŠæ·±åº¦è¾¹ç¼˜è¿‡æ»¤å¾—æ›´é”åˆ©ï¼Œç„¶åæ¸…ç†ä¸€äº›ä¸è¿è´¯çš„è¾¹ç¼˜ï¼Œæœ€åæ ¹æ®è¿é€šæ€§åˆ’åˆ†ä¸åŒçš„æ·±åº¦è¾¹ï¼ˆå¦‚å›¾2 ï¼ˆfï¼‰ä¸­ï¼Œä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒæ·±åº¦è¾¹ï¼‰ã€‚</p></li><li><p>å¯¹äºæ¯ä¸€ä¸ªæ·±åº¦è¾¹ï¼ŒæŠŠLDIå›¾ä¸­çš„åƒç´ ç‚¹åˆ‡å‰²å¼€ï¼Œå¹¶åœ¨èƒŒæ™¯å±‚æ‰©å±•ä¸€äº›åƒç´ ç‚¹ï¼Œå¯¹æ‰©å±•åŒºåŸŸè¿›è¡Œç”Ÿæˆ</p><figure><img src="https://s2.loli.net/2022/10/22/Ap62PxdYEDKzBkJ.png" alt="image-20221007091217942" /><figcaption>image-20221007091217942</figcaption></figure><ol type="1"><li><p>æ‰¾åˆ°ä¸€ä¸ªæ·±åº¦è¾¹ï¼ŒæŠŠä¸¤å±‚çš„åƒç´ ç‚¹åˆ‡å‰²å¼€</p></li><li><p>å¯¹äºèƒŒæ™¯å±‚ï¼Œç”¨flood-fill likeç®—æ³•è¿­ä»£åœ°é€‰å–ä¸€å®šçš„å·²çŸ¥åŒºåŸŸä½œä¸ºcontext regionï¼Œä»¥åŠæ‰©å±•ä¸€å®šçš„æœªçŸ¥åŒºåŸŸä½œä¸ºsynthesis region</p></li><li><p>åˆ©ç”¨å·²çŸ¥context region ç”ŸæˆæœªçŸ¥synthesis region çš„æ·±åº¦å’Œé¢œè‰²ï¼šé‡‡ç”¨åŸºäºå­¦ä¹ çš„inpaintingæ–¹æ³•</p><figure><img src="https://s2.loli.net/2022/10/22/h8uGZm4XAEcL5SP.png" alt="image-20221007091716266" /><figcaption>image-20221007091716266</figcaption></figure><p>è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œæœ€å…³é”®çš„å°±æ˜¯åœ¨é¢„æµ‹colorå’Œdepthä¹‹å‰ï¼Œå…ˆé¢„æµ‹äº†ä¸€ä¸‹depth edgesï¼Œç„¶åæŠŠè¿™ä¸ªedgesä¿¡æ¯åŠ è¿›å»ï¼Œå¯ä»¥æ›´å¥½åœ°é¢„æµ‹colorå’Œdepthã€‚</p></li><li><p>å°†ç”Ÿæˆå®Œæ¯•çš„åƒç´ èåˆå›LDIå›¾åƒ</p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;è®ºæ–‡åœ°å€ï¼š&lt;a href=&quot;https://shihmengli.github.io/3D-Photo-Inpainting&quot; class=&quot;uri&quot;&gt;https://shihmengli.github.io/3D-Photo-Inpainting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…ï¼š&lt;a href=&quot;https://shihmengli.github.io/&quot;&gt;Meng-Li Shih&lt;/a&gt;, &lt;a href=&quot;https://lemonatsu.github.io/&quot;&gt;Shih-Yang Su&lt;/a&gt;, &lt;a href=&quot;https://johanneskopf.de/&quot;&gt;Johannes Kopf&lt;/a&gt;, &lt;a href=&quot;https://filebox.ece.vt.edu/~jbhuang/&quot;&gt;Jia-Bin Huang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;å‘è¡¨ï¼š CVPR2020&lt;/p&gt;
&lt;p&gt;é“¾æ¥ï¼š &lt;a href=&quot;https://github.com/vt-vl-lab/3d-photo-inpainting&quot; class=&quot;uri&quot;&gt;https://github.com/vt-vl-lab/3d-photo-inpainting&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3DCV" scheme="https://jyzhu.top/blog/tags/3DCV/"/>
    
  </entry>
  
  <entry>
    <title>Reading SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos</title>
    <link href="https://jyzhu.top/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/"/>
    <id>https://jyzhu.top/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/</id>
    <published>2022-09-09T06:25:03.000Z</published>
    <updated>2022-10-22T09:16:13.032Z</updated>
    
    <content type="html"><![CDATA[<p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2112.13715</p><p>ä½œè€…ï¼š<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng%2C+A">Ailing Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ju%2C+X">Xuan Ju</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J">Jiefeng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+Q">Qiang Xu</a></p><p>å‘è¡¨ï¼š ECCV 2022</p><p>é“¾æ¥ï¼š <a href="https://github.com/cure-lab/SmoothNet" class="uri">https://github.com/cure-lab/SmoothNet</a></p><hr /><blockquote><p>å¦‚æœä½ å»åšè¿™ä¸ªä»»åŠ¡ï¼Œä¼šæ€ä¹ˆåšï¼Ÿä½œè€…åšçš„æ–¹æ³•å’Œä½ æƒ³çš„æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ</p></blockquote><h2 id="why">Whyï¼š</h2><ol type="1"><li>ä»è§†é¢‘ä¼°è®¡äººä½“å§¿åŠ¿æ—¶ï¼ŒæŠ–åŠ¨æ˜¯ä¸ªé—®é¢˜</li><li>é™¤äº†è½»å¾®æŠ–åŠ¨ä»¥å¤–ï¼Œæœ‰ä¸€äº›long- termæŠ–åŠ¨ï¼Œè¿˜æœ‰å› ä¸ºé‡å ã€å§¿åŠ¿å°‘è§ç­‰åŸå› é€ æˆçš„ä¼°æµ‹å›°éš¾</li></ol><h2 id="what">Whatï¼š</h2><figure><img src="https://s2.loli.net/2022/10/22/fRIB2t9hw8na6my.png" alt="image-20220909143919787" /><figcaption>image-20220909143919787</figcaption></figure><ol type="1"><li>ä¸€ä¸ªä»…åŸºäºæ—¶åºçš„ç²¾ç‚¼ç½‘ç»œï¼Œä»¥å…¶ä»–ç½‘ç»œçš„å§¿åŠ¿ä¼°è®¡ç»“æœä½œä¸ºè¾“å…¥ã€‚</li><li>æœ‰ç›‘ç£çš„</li><li>é‡‡ç”¨æ»‘åŠ¨çª—å£ï¼ŒåŸºäºTCN</li><li>å¹¶ä¸æ˜¯å¸¸è§çš„é‚£ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œå³é‡‡ç”¨æ—¶é—´-ç©ºé—´æ¨¡å‹æ¥åŒæ—¶ä¼˜åŒ–é€å¸§çš„å‡†ç¡®ç‡å’Œæ—¶åºçš„å¹³æ»‘æ€§ã€‚è¿™ä¸ªæ–¹æ³•é€šè¿‡å­¦ä¹ æ¯ä¸€ä¸ªå…³èŠ‚åœ¨é•¿æ—¶é—´èŒƒå›´çš„è¿åŠ¨ç‰¹å¾ï¼ˆè€Œä¸æ˜¯å…³èŠ‚ä¹‹é—´çš„å…³ç³»ï¼‰ï¼Œæ¥è‡ªç„¶åœ°å»ºæ¨¡èº«ä½“è¿åŠ¨ä¸­çš„å¹³æ»‘ç‰¹å¾ã€‚</li><li>ç”±äºå®ƒä»…ä»…éœ€è¦æ—¶åºä¿¡æ¯ï¼Œæ‰€ä»¥å¯ä»¥æ³›åŒ–åˆ°å¾ˆå¤šç§ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬2Då’Œ3Dçš„å§¿åŠ¿ä¼°è®¡ã€body recoveryç­‰</li></ol><h2 id="how">Howï¼š</h2><ol type="1"><li><p>æ ¹æ®æŒç»­æ—¶é•¿ï¼Œå°†æŠ–åŠ¨å½’ç±»ä¸ºsudden jitterå’Œlong- term jitterä¸¤ç§ã€‚ä¸ºäº†è§£å†³long- termçš„æŠ–åŠ¨é—®é¢˜ï¼Œç°æœ‰é‚£äº›æ–¹æ³•éƒ½ä¸å¤§è¡Œã€‚</p><p>æ ¹æ®ç¨‹åº¦ï¼Œåˆå¯ä»¥å°†æŠ–åŠ¨åˆ†ä¸ºå°æŠ–åŠ¨å’Œå¤§æŠ–åŠ¨ã€‚å°æŠ–åŠ¨ä¸€èˆ¬ç”±äºä¸å¯é¿å…çš„è¯¯å·®ï¼Œæˆ–è€…æ ‡æ³¨ä¸Šçš„è¯¯å·®ï¼›å¤§æŠ–åŠ¨åˆ™æ˜¯ç”±äºå›¾åƒè´¨é‡å·®ã€å§¿åŠ¿å°‘ã€é‡å ä¸¥é‡ç­‰ã€‚</p><figure><img src="https://s2.loli.net/2022/10/22/pNa7bcVqzi9lGZw.png" alt="image-20220909143902334" /><figcaption>image-20220909143902334</figcaption></figure></li><li><p>å°†è¯¯å·®å½’ç±»ä¸ºç›¸é‚»å¸§ä¹‹é—´çš„æŠ–åŠ¨é€ æˆçš„è¯¯å·®ï¼ˆjitter errorï¼‰å’Œæ¨¡å‹ä¼°è®¡ç»“æœä¸çœŸå®ç»“æœä¹‹é—´çš„åå·®ï¼ˆbias errorï¼‰è¿™ä¸¤ç§ã€‚ç°æœ‰é‚£äº›æ–¹æ³•å¹¶æ²¡æœ‰å°†è¿™ä¸¤ç±»è¯¯å·®è§£è€¦</p></li><li><p>æå‡ºäº†basic smoothnetå’Œæ­£ç»smoothnetã€‚</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/10/22/Zf6lpahbHrRG7kF.png" alt="image-20220909154626864" /><figcaption>image-20220909154626864</figcaption></figure><p>Basic smoothnetï¼ŒFCNæ˜¯backboneã€‚é€šè¿‡é•¿åº¦ä¸ºTçš„æ»‘çª—ï¼Œæ¯æ¬¡ä¼ å…¥Tå¸§å›¾åƒï¼ŒåŒ…å«Cä¸ªchannelsã€‚</p><figure><img src="https://s2.loli.net/2022/10/22/hRcX2MCx8SnHZfz.png" alt="image-20220909155504588" /><figcaption>image-20220909155504588</figcaption></figure><p>æƒé‡<span class="math inline">\(w_t^l\)</span>å’Œåå·®<span class="math inline">\(b^l\)</span>æ˜¯ç¬¬<span class="math inline">\(t_{th}\)</span>å¸§çš„ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„channelä¹‹é—´æ˜¯å…±äº«çš„ã€‚</p></li><li><figure><img src="https://s2.loli.net/2022/10/22/uCJgF8M6tG5S1PB.png" alt="image-20220909155701901" /><figcaption>image-20220909155701901</figcaption></figure><p>å®Œæ•´çš„motion- aware smoothnetå°±æ˜¯åŠ ä¸Šäº†é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ä¸¤ä¸ªæ¨¡å—ã€‚</p><p>å› ä¸ºjitterçš„ä¸€ä¸ªè¡¡é‡æ–¹å¼å°±æ˜¯åŠ é€Ÿåº¦ï¼Œæ‰€ä»¥æŠŠåŠ é€Ÿåº¦ç›´è§‚åœ°æ˜¾ç¤ºåœ¨æ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªå¾ˆæ˜¾ç„¶çš„æ–¹å¼ã€‚ç»™å®šé¢„æµ‹å‡ºçš„å§¿åŠ¿<span class="math inline">\(\hat Y\)</span>ï¼Œé€Ÿåº¦å°±æ˜¯ä¸¤å¸§ä¹‹é—´ç›¸å‡ï¼Œå¾—åˆ° <span class="math display">\[\hat V_{i,t} = \hat Y_{i,t} âˆ’ \hat Y_{i,tâˆ’1}\]</span> åŠ é€Ÿåº¦å°±æ˜¯é€Ÿåº¦ä¹‹é—´çš„å·®ï¼š <span class="math display">\[\hat A_{i,t} = \hat V_{i,t} âˆ’ \hat V_{i,tâˆ’1}\]</span></p></li></ol></li><li><p>losså°±æ˜¯ä¸¤ä¸ªï¼š</p><ol type="1"><li><p>ground truth poseå’Œä¼°è®¡poseä¹‹é—´çš„è¯¯å·®ï¼š <span class="math display">\[L_{pose} = \frac{1}{T\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G_{i,t} âˆ’ Y_{i,t}|,\]</span></p></li><li><p>ground truth åŠ é€Ÿåº¦å’Œä¼°è®¡åŠ é€Ÿåº¦ä¹‹é—´çš„è¯¯å·®ï¼š <span class="math display">\[L_{acc} = \frac{1}{(T-2)\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G&#39;&#39;_{i,t} âˆ’ A_{i,t}|,\]</span></p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2112.13715&lt;/p&gt;
&lt;p&gt;ä½œè€…ï¼š&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zeng%2C+A&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang%2C+L&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ju%2C+X&quot;&gt;Xuan Ju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li%2C+J&quot;&gt;Jiefeng Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang%2C+J&quot;&gt;Jianyi Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu%2C+Q&quot;&gt;Qiang Xu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;å‘è¡¨ï¼š ECCV 2022&lt;/p&gt;
&lt;p&gt;é“¾æ¥ï¼š &lt;a href=&quot;https://github.com/cure-lab/SmoothNet&quot; class=&quot;uri&quot;&gt;https://github.com/cure-lab/SmoothNet&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>To Midnight</title>
    <link href="https://jyzhu.top/blog/To-Midnight/"/>
    <id>https://jyzhu.top/blog/To-Midnight/</id>
    <published>2022-09-08T18:06:51.000Z</published>
    <updated>2023-07-16T18:23:52.850Z</updated>
    
    <content type="html"><![CDATA[<p>å¤±çœ çš„è¯</p><p>å¤œæ™šå°±å˜å¾—ç»µé•¿</p><p>ç¬¬å‡ èŠ‚è„Šæ¤ä¸Šäº†å‘æ¡ï¼Œæ‹§åŠ¨æ—¶</p><p>å¥‡å¼‚çš„å¯¹å¶çš„è¯—æµå‘æ–°çš„å°èŠ‚</p><p>è€Œç»µé•¿çš„å¤œä¸è¯¥æœ‰è¯—</p><p>ä¹Ÿä¸è¯¥æœ‰ç³Ÿç³•çš„æ¯”å–»å’Œæ¯”å–»ä¸€æ ·çš„é£</p><p>è¯¥ç»™æ¯ä¸€é˜µæµ·æµªå‘½ä»€ä¹ˆåå‘¢</p><p>è¿™æ˜¯å½“ä¸‹æœ€è¦ç´§çš„å¹½æš—é—®é¢˜</p><p>åˆ«æ— è¦ç´§äº‹</p><p>ä¸€ä¸ªäººæœ‰å¤šæƒ³å‚ä¸ç”Ÿæ´»</p><p>åˆè·Ÿç”Ÿæ´»å¯¹ååœ¨é•¿æ¡Œä¸¤ç«¯</p><p>é£å…‰é£å…‰ï¼ŒåŠå°”ä¸€æ¯é…’</p><p>å¤©äº®ååˆåˆ°äº†é…’ç¥çš„æ¢¦</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;å¤±çœ çš„è¯&lt;/p&gt;
&lt;p&gt;å¤œæ™šå°±å˜å¾—ç»µé•¿&lt;/p&gt;
&lt;p&gt;ç¬¬å‡ èŠ‚è„Šæ¤ä¸Šäº†å‘æ¡ï¼Œæ‹§åŠ¨æ—¶&lt;/p&gt;
&lt;p&gt;å¥‡å¼‚çš„å¯¹å¶çš„è¯—æµå‘æ–°çš„å°èŠ‚&lt;/p&gt;</summary>
    
    
    
    <category term="poems" scheme="https://jyzhu.top/blog/categories/poems/"/>
    
    
  </entry>
  
</feed>
